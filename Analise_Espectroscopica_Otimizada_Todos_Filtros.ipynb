{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Análise Espectroscópica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este notebook segue as melhores práticas da literatura para análise espectroscópica robusta, incluindo:\n",
        "- Divisão dos dados por Kennard-Stone (70-30)\n",
        "- Remoção de outliers espectrais (PCA, T²/Q)\n",
        "- Remoção de outliers dos atributos (boxplot + seaborn)\n",
        "- Teste de todos os filtros (dependentes e independentes de y)\n",
        "- Modelagem com PLSR, PCR, RFR, SVMR (hiperparâmetros da literatura)\n",
        "- Plotagens robustas e salvamento de métricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports necessários\n",
        "import os\n",
        "import pywt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "import joblib\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.signal import savgol_filter, detrend\n",
        "import warnings\n",
        "from sklearn.pipeline import Pipeline\n",
        "import time\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "# Cria diretórios para organizar todas as imagens salvas\n",
        "print(\"Criando diretórios para salvar os gráficos...\")\n",
        "os.makedirs('graficos_outliers_pca', exist_ok=True)\n",
        "os.makedirs('graficos_outliers_boxplot', exist_ok=True)\n",
        "os.makedirs('modelos_salvos', exist_ok=True)\n",
        "# A pasta para os modelos será criada depois, mas podemos garantir aqui\n",
        "# Nome da pasta para o notebook geral\n",
        "os.makedirs('graficos_modelos', exist_ok=True) \n",
        "# Nome da pasta para o notebook MLPR\n",
        "os.makedirs('graficos_mlpr', exist_ok=True) \n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurações de plot\n",
        "plt.style.use('default')\n",
        "sns.set_palette('husl')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Carregamento dos Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O primeiro passo em qualquer projeto de ciência de dados é carregar e entender os dados. Nesta seção, realizamos as seguintes ações:\n",
        "- **Carregamento:** Utilizamos a biblioteca `pandas` para ler o arquivo `Avaliacao_Maturacao_Palmer_e_Tommy_Fieldspec.xlsx`, que contém os dados espectrais e os valores de referência (atributos físico-químicos).\n",
        "- **Separação de Dados:** A função `load_data` inteligentemente separa o arquivo em duas partes:\n",
        "    - `metadata`: Um DataFrame contendo as informações de referência (ex: Firmness, Dry Mass, TSS, TA, AA, Weight, Width, Lenght), que serão nossas variáveis-alvo (y).\n",
        "    - `wavelengths`: Um DataFrame contendo a resposta espectral (absorbância ou reflectância) em cada comprimento de onda, que serão nossas variáveis preditoras (X).\n",
        "- **Definição de Variáveis:** Convertemos os dados espectrais para um array NumPy (`X`) para otimizar os cálculos e definimos a lista de `atributos` que desejamos modelar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Função para carregar dados\n",
        "def load_data(filepath):\n",
        "    \"\"\"Carrega dados espectrais e separa metadados de espectros.\"\"\"\n",
        "    df = pd.read_excel(filepath, engine='openpyxl')\n",
        "    \n",
        "    # Identificar colunas que são comprimentos de onda (numéricas)\n",
        "    numeric_cols = []\n",
        "    for col in df.columns:\n",
        "        try:\n",
        "            float(col)\n",
        "            numeric_cols.append(col)\n",
        "        except ValueError:\n",
        "            continue\n",
        "    \n",
        "    # Separar metadados e comprimentos de onda\n",
        "    metadata = df.drop(columns=numeric_cols)\n",
        "    wavelengths = df[numeric_cols]\n",
        "    \n",
        "    return metadata, wavelengths\n",
        "\n",
        "# Carregar dados\n",
        "filepath = 'Data/raw/Fruto/Avaliacao_Maturacao_Palmer_e_Tommy_Fieldspec.xlsx'\n",
        "metadata, wavelengths = load_data(filepath)\n",
        "X = wavelengths.values\n",
        "wavelength_values = wavelengths.columns.astype(float)\n",
        "# 8 VARIÁVEIS DA SARAH?\n",
        "atributos = ['Firmness (N)', 'Dry Mass (%)', 'TSS (Brix)', 'TA (g/mL)', 'AA (mg/100g)','Weight (g)','Width (mm)','Length (mm)']\n",
        "\n",
        "print(f'Dados carregados: {X.shape[0]} amostras, {X.shape[1]} comprimentos de onda')\n",
        "print(f'Faixa espectral: {wavelength_values.min():.1f} - {wavelength_values.max():.1f} nm')\n",
        "print(f'Atributos disponíveis: {list(metadata.columns)}')\n",
        "print(f'Atributos a analisar: {atributos}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Validação Cruzada K-Fold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Divisão dos Dados com Kennard-Stone (70-30)\n",
        "\"\"\"\n",
        "def kennard_stone(X, train_size=0.7):\n",
        "    # Implementa o algoritmo Kennard-Stone para divisão de dados.\n",
        "    n_samples = X.shape[0]\n",
        "    n_train = int(n_samples * train_size)\n",
        "    \n",
        "    if n_train < 2:\n",
        "        raise ValueError(\"O conjunto de treinamento precisa ter pelo menos duas amostras!\")\n",
        "    \n",
        "    distances = cdist(X, X, metric='euclidean')\n",
        "    \n",
        "    mean_sample = np.mean(X, axis=0)\n",
        "    first_sample = np.argmax(np.linalg.norm(X - mean_sample, axis=1))\n",
        "    selected = [first_sample]\n",
        "    \n",
        "    remaining = list(range(n_samples))\n",
        "    remaining.remove(first_sample)\n",
        "    \n",
        "    for _ in range(1, n_train):\n",
        "        dist_to_selected = distances[selected, :]\n",
        "        min_dist_to_selected = np.min(dist_to_selected[:, remaining], axis=0)\n",
        "        \n",
        "        next_sample_idx_in_remaining = np.argmax(min_dist_to_selected)\n",
        "        next_sample = remaining.pop(next_sample_idx_in_remaining)\n",
        "        \n",
        "        selected.append(next_sample)\n",
        "        \n",
        "    return np.array(selected), np.array(remaining)\n",
        "\n",
        "# Divisão para cada atributo\n",
        "dados_divididos = {}\n",
        "\n",
        "for atributo in atributos:\n",
        "    y = metadata[atributo].values\n",
        "    original_indices = np.arange(len(y)) # Guarda os índices originais\n",
        "    \n",
        "    mask = ~np.isnan(y)\n",
        "    X_clean = X[mask]\n",
        "    y_clean = y[mask]\n",
        "    original_indices_clean = original_indices[mask] # Filtra os índices também\n",
        "    \n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_clean)\n",
        "    \n",
        "    idx_cal_split, idx_val_split = kennard_stone(X_scaled, train_size=0.7)\n",
        "    \n",
        "    X_cal, X_val = X_clean[idx_cal_split], X_clean[idx_val_split]\n",
        "    y_cal, y_val = y_clean[idx_cal_split], y_clean[idx_val_split]\n",
        "    \n",
        "    # Guarda os índices originais de cada conjunto\n",
        "    indices_orig_cal = original_indices_clean[idx_cal_split]\n",
        "    indices_orig_val = original_indices_clean[idx_val_split]\n",
        "    \n",
        "    dados_divididos[atributo] = {\n",
        "        'X_cal': X_cal, 'X_val': X_val,\n",
        "        'y_cal': y_cal, 'y_val': y_val,\n",
        "        'indices_orig_cal': indices_orig_cal,\n",
        "        'indices_orig_val': indices_orig_val\n",
        "    }\n",
        "    \n",
        "    print(f'{atributo}: {X_cal.shape[0]} calibração, {X_val.shape[0]} validação')\n",
        "\n",
        "print(f'\\nDivisão Kennard-Stone concluída para {len(atributos)} atributos!')\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Remoção de Outliers Espectrais (PCA, T²/Q) nos Dados de Calibração"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Outliers espectrais são amostras cujo espectro é muito diferente do resto da população, podendo ser causados por erros de medição, contaminação ou características únicas da amostra. Eles podem prejudicar significativamente a capacidade de generalização do modelo.\n",
        "- **Método PCA (T²/Q):** Usamos a Análise de Componentes Principais (PCA) para identificar esses outliers.\n",
        "    - **Estatística T² (Hotelling's T²):** Mede a variação de uma amostra *dentro* do modelo PCA. Valores altos indicam que a amostra é um outlier na combinação das variáveis principais.\n",
        "    - **Estatística Q (Resíduos):** Mede a variação da amostra *fora* do modelo PCA (o que o modelo não conseguiu capturar). Valores altos indicam que a estrutura do espectro da amostra é anormal.\n",
        "- **Lógica:** Uma amostra é considerada outlier se seu valor de T² ou Q ultrapassa um limite de confiança (geralmente 3 desvios padrão da média).\n",
        "- **Importante:** A remoção de outliers é feita **apenas no conjunto de calibração**. O conjunto de validação deve permanecer intacto para simular dados \"reais\" que o modelo encontrará no futuro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pca_outlier_removal(X, n_components=10, threshold=3):\n",
        "    \"\"\"Remove outliers usando PCA com estatísticas T² e Q.\"\"\"\n",
        "    # Padronizar dados\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    \n",
        "    # Aplicar PCA\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "    \n",
        "    # Calcular estatística T² (Hotelling's T²)\n",
        "    T2 = np.sum((X_pca / np.std(X_pca, axis=0)) ** 2, axis=1)\n",
        "    \n",
        "    # Calcular estatística Q (resíduos)\n",
        "    X_reconstructed = pca.inverse_transform(X_pca)\n",
        "    Q = np.sum((X_scaled - X_reconstructed) ** 2, axis=1)\n",
        "    \n",
        "    # Definir limites (média + threshold * desvio padrão)\n",
        "    T2_limit = np.mean(T2) + threshold * np.std(T2)\n",
        "    Q_limit = np.mean(Q) + threshold * np.std(Q)\n",
        "    \n",
        "    # Identificar outliers\n",
        "    outliers_mask = (T2 > T2_limit) | (Q > Q_limit)\n",
        "    \n",
        "    # Retornamos a máscara de quem NÃO é outlier e o modelo PCA treinado\n",
        "    return ~outliers_mask, pca, T2, Q, T2_limit, Q_limit\n",
        "\n",
        "# Remover outliers espectrais dos dados de calibração\n",
        "dados_pca = {}\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" for atributo in atributos:\n",
        "    X_cal = dados_divididos[atributo]['X_cal']\n",
        "    y_cal = dados_divididos[atributo]['y_cal']\n",
        "    indices_orig_cal = dados_divididos[atributo]['indices_orig_cal']\n",
        "    \n",
        "    # Chama a função de remoção de outliers e armazena todos os resultados\n",
        "    keep_mask, pca_model, T2, Q, T2_limit, Q_limit = pca_outlier_removal(X_cal)\n",
        "    \n",
        "    # Identifica e informa os outliers removidos\n",
        "    outliers_indices = indices_orig_cal[~keep_mask]\n",
        "    print(f'{atributo}: {len(outliers_indices)} outliers espectrais removidos da calibração.')\n",
        "    if len(outliers_indices) > 0:\n",
        "        print(f'  Índices Originais Removidos: {outliers_indices}')\n",
        "    \n",
        "    # Filtra os dados de calibração\n",
        "    X_cal_clean = X_cal[keep_mask]\n",
        "    y_cal_clean = y_cal[keep_mask]\n",
        "    indices_orig_cal_clean = indices_orig_cal[keep_mask]\n",
        "    \n",
        "    dados_pca[atributo] = {\n",
        "        'X_cal': X_cal_clean,\n",
        "        'y_cal': y_cal_clean,\n",
        "        'indices_orig_cal': indices_orig_cal_clean,\n",
        "        'pca_model': pca_model,\n",
        "        'T2': T2,\n",
        "        'Q': Q,\n",
        "        'T2_limit': T2_limit,\n",
        "        'Q_limit': Q_limit,\n",
        "        'keep_mask': keep_mask\n",
        "    }\n",
        "    \n",
        "    print(f'  Calibração: {X_cal_clean.shape[0]} amostras')\n",
        "    print(f'  Validação: {dados_divididos[atributo][\"X_val\"].shape[0]} amostras')\n",
        "    print()\n",
        "\"\"\" "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Verificação de Outliers Espectrais no Conjunto de Validação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Após construir o modelo de PCA com os dados de calibração limpos, podemos usá-lo para avaliar o conjunto de validação. O objetivo aqui **não é remover** amostras da validação, mas sim entender se ele contém amostras que seriam consideradas \"estranhas\" pelo modelo.\n",
        "- **Metodologia:**\n",
        "    1. Padronizamos os dados de validação usando o `scaler` treinado na calibração.\n",
        "    2. Projetamos esses dados no espaço PCA do modelo de calibração.\n",
        "    3. Calculamos as estatísticas T² e Q para as amostras de validação.\n",
        "    4. Comparamos esses valores com os **limites (T²_limit e Q_limit) definidos na calibração**.\n",
        "- **Interpretação:** Se amostras de validação excederem os limites, isso indica que o modelo pode ter dificuldade em predizê-las, pois elas representam uma variabilidade não vista (ou removida) no treino. Isso é uma informação valiosa sobre a robustez do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificação de Outliers na Validação\n",
        "print(\"--- Verificação de Outliers no Conjunto de Validação ---\")\n",
        "for atributo in atributos:\n",
        "    # Carrega o modelo PCA e os dados de validação\n",
        "    pca_model = dados_pca[atributo]['pca_model']\n",
        "    X_val = dados_divididos[atributo]['X_val']\n",
        "    indices_orig_val = dados_divididos[atributo]['indices_orig_val']\n",
        "    \n",
        "    # Recalcula os limites usando o modelo salvo, se necessário\n",
        "    # (Ou usa os limites já calculados e salvos anteriormente)\n",
        "    \n",
        "    # Padroniza e projeta os dados de validação\n",
        "    scaler_val = StandardScaler().fit(dados_divididos[atributo]['X_cal']) # Usa o scaler da calibração\n",
        "    X_val_scaled = scaler_val.transform(X_val)\n",
        "    X_val_pca = pca_model.transform(X_val_scaled)\n",
        "\n",
        "    # Calcula T² e Q para a validação\n",
        "    T2_val = np.sum((X_val_pca / np.std(X_val_pca, axis=0)) ** 2, axis=1)\n",
        "    X_val_reconstructed = pca_model.inverse_transform(X_val_pca)\n",
        "    Q_val = np.sum((X_val_scaled - X_val_reconstructed) ** 2, axis=1)\n",
        "\n",
        "    # Identifica outliers da validação usando os limites da CALIBRAÇÃO\n",
        "    outliers_val_mask = (T2_val > T2_limit) | (Q_val > Q_limit)\n",
        "    outliers_val_indices = indices_orig_val[outliers_val_mask]\n",
        "    \n",
        "    print(f'\\nAtributo: {atributo}')\n",
        "    if len(outliers_val_indices) > 0:\n",
        "        print(f'  {len(outliers_val_indices)} amostras de VALIDAÇÃO excedem os limites da calibração.')\n",
        "        print(f'  Índices Originais: {outliers_val_indices}')\n",
        "    else:\n",
        "        print('  Nenhuma amostra de validação excede os limites da calibração.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Gráficos de Outliers T² e Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for atributo in atributos:\n",
        "    d = dados_pca[atributo]\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Gráfico 1: Variância explicada cumulativa\n",
        "    # Acessa o modelo PCA com a chave correta 'pca_model'\n",
        "    cumulative_variance = np.cumsum(d['pca_model'].explained_variance_ratio_)\n",
        "    axes[0].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'o-', linewidth=2, markersize=6)\n",
        "    axes[0].set_xlabel('Número de Componentes Principais')\n",
        "    axes[0].set_ylabel('Variância Explicada Cumulativa')\n",
        "    axes[0].set_title(f'Variância Explicada - {atributo}')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[0].axhline(y=0.95, color='r', linestyle='--', alpha=0.7, label='95%')\n",
        "    axes[0].legend()\n",
        "    \n",
        "    # Gráfico 2: T² vs Q (detecção de outliers)\n",
        "    # Acessa as variáveis T2, Q, T2_limit, Q_limit e keep_mask com as chaves corretas\n",
        "    scatter = axes[1].scatter(d['T2'], d['Q'], c=d['keep_mask'], cmap='coolwarm', \n",
        "                              edgecolor='k', alpha=0.7, s=50)\n",
        "    axes[1].axhline(d['Q_limit'], color='r', linestyle='--', linewidth=2, label=f'Q Limit: {d[\"Q_limit\"]:.2f}')\n",
        "    axes[1].axvline(d['T2_limit'], color='g', linestyle='--', linewidth=2, label=f'T² Limit: {d[\"T2_limit\"]:.2f}')\n",
        "    axes[1].set_xlabel(\"Hotelling's T²\")\n",
        "    axes[1].set_ylabel('Q Residual')\n",
        "    axes[1].set_title(f'Detecção de Outliers - {atributo}')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Adicionar legenda de cores (esta parte também foi ajustada para a chave 'keep_mask')\n",
        "    legend1 = axes[1].legend(*scatter.legend_elements(), title=\"Status\")\n",
        "    axes[1].add_artist(legend1)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f'Gráficos de outliers para {atributo} exibidos.')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Remoção de Outliers dos Atributos (Boxplot + Seaborn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Além dos outliers espectrais (em X), podemos ter outliers nos valores de referência (em y). Por exemplo, um valor de pH ou SST que é analiticamente improvável ou resultado de um erro de anotação.\n",
        "- **Método (IQR):** O método do Intervalo Interquartil (IQR) é uma forma estatística robusta de identificar esses pontos.\n",
        "    - `IQR = Q3 (percentil 75) - Q1 (percentil 25)`\n",
        "    - Um valor é considerado outlier se estiver abaixo de `Q1 - 1.5 * IQR` ou acima de `Q3 + 1.5 * IQR`.\n",
        "- **Visualização:** O `boxplot` é a ferramenta visual perfeita para essa análise, pois ele desenha os \"bigodes\" exatamente nesses limites de 1.5 * IQR, mostrando os outliers como pontos individuais.\n",
        "- **Ação:** Novamente, a remoção é feita apenas no conjunto de calibração."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remover outliers dos atributos usando boxplot e IQR\n",
        "dados_final = {}\n",
        "\n",
        "for atributo in atributos:\n",
        "    # Pega os dados já limpos de outliers espectrais\n",
        "    d = dados_pca[atributo]\n",
        "    X_cal = d['X_cal']\n",
        "    y_cal = d['y_cal']\n",
        "    indices_orig_cal = d['indices_orig_cal']\n",
        "    \n",
        "    # Boxplot (seu código de plotagem original pode ser mantido aqui)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(x=y_cal)\n",
        "    sns.stripplot(x=y_cal, color='red', alpha=0.6)\n",
        "    plt.title(f'Boxplot com Dispersão - {atributo}')\n",
        "    \n",
        "    nome_arquivo = f'graficos_outliers_boxplot/boxplot_outliers_{atributo}.png'\n",
        "    plt.savefig(nome_arquivo, dpi=300, bbox_inches='tight')\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "    # Lógica de remoção de outliers\n",
        "    Q1 = np.percentile(y_cal, 25)\n",
        "    Q3 = np.percentile(y_cal, 75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    \n",
        "    keep_mask_attr = (y_cal >= lower_bound) & (y_cal <= upper_bound)\n",
        "    \n",
        "    # Identifica e informa os outliers removidos\n",
        "    outliers_attr_indices = indices_orig_cal[~keep_mask_attr]\n",
        "    print(f'{atributo}: {len(outliers_attr_indices)} outliers de atributo removidos.')\n",
        "    if len(outliers_attr_indices) > 0:\n",
        "        print(f'  Índices Originais Removidos: {outliers_attr_indices}')\n",
        "    \n",
        "    # Armazena os dados finais e limpos para modelagem\n",
        "    dados_final[atributo] = {\n",
        "        'X_cal': X_cal[keep_mask_attr],\n",
        "        'y_cal': y_cal[keep_mask_attr],\n",
        "        'X_val': dados_divididos[atributo]['X_val'], # Validação original\n",
        "        'y_val': dados_divididos[atributo]['y_val']   # Validação original\n",
        "    }\n",
        "    \n",
        "    print(f'  Calibração final: {dados_final[atributo][\"X_cal\"].shape[0]} amostras')\n",
        "    print(f'  Validação: {dados_final[atributo][\"X_val\"].shape[0]} amostras')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Filtros de Pré-processamento (Independentes e Dependentes de y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Os dados espectrais brutos contêm diversas fontes de variação indesejada que podem mascarar a informação química relevante. Essas variações podem ser causadas por espalhamento de luz (efeitos físicos da amostra), mudanças na linha de base (drift do instrumento) e ruído. O pré-processing visa minimizar esses efeitos para que o modelo foque na correlação entre o espectro e o atributo de interesse.\n",
        "\n",
        "Neste notebook, testamos uma ampla gama de filtros, divididos em duas categorias:\n",
        "\n",
        "### Filtros Independentes de y\n",
        "\n",
        "São aplicados apenas aos espectros (`X`) e não utilizam a informação da variável alvo (`y`).\n",
        "\n",
        "-   **`Raw`**: Utiliza os dados espectrais brutos, sem nenhum tratamento. Serve como uma linha de base para comparar a eficácia dos outros filtros.\n",
        "-   **`MSC` (Multiplicative Scatter Correction)**: Corrige o espalhamento de luz (aditivo e multiplicativo) causado por variações no tamanho de partícula e compactação da amostra. Ele ajusta cada espectro para se parecer mais com um espectro \"ideal\" (geralmente a média de todos os espectros).\n",
        "-   **`SNV` (Standard Normal Variate)**: Alternativa ao MSC que também corrige o espalhamento de luz. A diferença é que o SNV padroniza cada espectro individualmente (subtrai a média e divide pelo desvio padrão daquele espectro), sem usar um espectro de referência.\n",
        "-   **`SG_D1` e `SG_D2` (Savitzky-Golay Derivatives)**: Calcula a primeira ou a segunda derivada do espectro. Derivadas são excelentes para remover desvios de linha de base (efeitos aditivos) e para resolver picos espectrais sobrepostos, realçando a informação de bandas de absorção específicas.\n",
        "-   **`Detrend`**: Remove tendências lineares ou polinomiais da linha de base do espectro. É muito eficaz para corrigir \"inclinações\" no espectro causadas por drift do instrumento.\n",
        "-   **`Normalize`**: Realiza uma normalização Min-Max, escalonando a intensidade de cada espectro para um intervalo fixo (geralmente [0, 1]). Ajuda a corrigir variações de intensidade causadas por diferenças na distância da amostra ou na potência da fonte de luz.\n",
        "-   **`EMSC` (Extended Multiplicative Signal Correction)**: Uma versão avançada do MSC. Além de corrigir os efeitos de espalhamento, o EMSC pode incluir termos polinomiais para modelar e remover efeitos de linha de base mais complexos e não-lineares.\n",
        "-   **`Continuum Removal`**: Técnica que normaliza os espectros para que as bandas de absorção possam ser comparadas em termos de sua profundidade, e não de sua intensidade absoluta. Ele ajusta um \"envelope\" (casco convexo) sobre o espectro e divide o espectro original por este envelope, realçando as características de absorção.\n",
        "-   **`Wavelet_Denoising`**: Utiliza a Transformada Wavelet para decompor o espectro em diferentes níveis de frequência. A técnica permite remover o ruído (geralmente presente em altas frequências) de forma muito eficaz, preservando as principais características do sinal espectral.\n",
        "\n",
        "### Filtros Dependentes de y e Combinações\n",
        "\n",
        "Estes filtros utilizam a variável alvo (`y`) para otimizar a remoção de variação não correlacionada em `X`, ou são combinações sequenciais de múltiplos filtros para um tratamento mais completo.\n",
        "\n",
        "-   **`OSC_1` e `OSC_2` (Orthogonal Signal Correction)**: Filtro que remove componentes (1 ou 2, neste caso) dos espectros `X` que são ortogonais (não correlacionados) à variável alvo `y`. O objetivo é limpar `X` da variação que não ajuda na predição, potencialmente melhorando o modelo subsequente. Esta técnica é frequentemente referida como um pré-processamento **OPLS** (Orthogonal Projections to Latent Structures).\n",
        "-   **`MSC_SG_OSC`**: Uma **cadeia de pré-processamentos** aplicada na seguinte ordem:\n",
        "    1.  `MSC` para corrigir o espalhamento.\n",
        "    2.  `Savitzky-Golay (1ª derivada)` para corrigir a linha de base.\n",
        "    3.  `OSC (1 componente)` para remover variação não correlacionada com `y`.\n",
        "-   **`OPLS1_SNV_SG_D1` e `OPLS2_SNV_SG_D1`**: Outra cadeia de processamento:\n",
        "    1.  `SNV` para correção de espalhamento.\n",
        "    2.  `Savitzky-Golay (1ª derivada)`.\n",
        "    3.  `OPLS/OSC` para remover 1 ou 2 componentes ortogonais a `y`.\n",
        "-   **`SNV_Detrend_SG_D1`**: Uma combinação de filtros independentes de `y`, mas que, por sua complexidade, é testada junto às outras cadeias:\n",
        "    1.  `SNV`.\n",
        "    2.  `Detrend` para remoção de tendência.\n",
        "    3.  `Savitzky-Golay (1ª derivada)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementação dos filtros de pré-processamento\n",
        "# Filtros independentes de y\n",
        "def msc(X):\n",
        "    \"\"\"Multiplicative Scatter Correction.\"\"\"\n",
        "    X = np.asarray(X)\n",
        "    mean_spectrum = np.mean(X, axis=0)\n",
        "    corrected_spectra = np.zeros_like(X)\n",
        "    for i in range(X.shape[0]):\n",
        "        slope, intercept = np.polyfit(mean_spectrum, X[i, :], 1)\n",
        "        corrected_spectra[i, :] = (X[i, :] - intercept) / slope\n",
        "    return corrected_spectra\n",
        "\n",
        "def snv(X):\n",
        "    \"\"\"Standard Normal Variate.\"\"\"\n",
        "    X = np.asarray(X)\n",
        "    return (X - np.mean(X, axis=1, keepdims=True)) / np.std(X, axis=1, keepdims=True)\n",
        "\n",
        "def savitzky_golay(X, window_size=11, poly_order=2, deriv_order=1):\n",
        "    \"\"\"Savitzky-Golay filter.\"\"\"\n",
        "    return savgol_filter(X, window_length=window_size, polyorder=poly_order, deriv=deriv_order, axis=1)\n",
        "\n",
        "def detrend_filter(X):\n",
        "    \"\"\"Detrending filter.\"\"\"\n",
        "    return detrend(X, axis=1)\n",
        "\n",
        "def normalize(X):\n",
        "    \"\"\"Normalização Min-Max.\"\"\"\n",
        "    return (X - np.min(X, axis=1, keepdims=True)) / (np.max(X, axis=1, keepdims=True) - np.min(X, axis=1, keepdims=True))\n",
        "\n",
        "def emsc(X, reference=None):\n",
        "    \"\"\"Extended Multiplicative Signal Correction.\"\"\"\n",
        "    X = np.asarray(X)\n",
        "    if reference is None:\n",
        "        reference = np.mean(X, axis=0)  # Usa o espectro médio como referência\n",
        "    \n",
        "    X_corr = np.zeros_like(X)\n",
        "    for i in range(X.shape[0]):\n",
        "        # Modelo: X[i] ≈ a + b*reference\n",
        "        model = np.vstack([np.ones_like(reference), reference]).T\n",
        "        params, _, _, _ = np.linalg.lstsq(model, X[i, :], rcond=None)\n",
        "        a, b = params[0], params[1]\n",
        "        X_corr[i,:] = (X[i, :] - a) / b\n",
        "    return X_corr\n",
        "\n",
        "def continuum_removal(X, wavelengths):\n",
        "    \"\"\"Continuum Removal.\"\"\"\n",
        "    X = np.asarray(X)\n",
        "    X_cr = np.zeros_like(X)\n",
        "    for i in range(X.shape[0]):\n",
        "        spectrum = X[i, :]\n",
        "        # Encontra os pontos do casco convexo superior\n",
        "        q_u = [0]\n",
        "        for k in range(1, len(wavelengths) - 1):\n",
        "            s_k = (spectrum[len(wavelengths)-1] - spectrum[0]) / (wavelengths[-1] - wavelengths[0])\n",
        "            s_q = (spectrum[k] - spectrum[q_u[-1]]) / (wavelengths[k] - wavelengths[q_u[-1]])\n",
        "            if s_q > s_k:\n",
        "                q_u.append(k)\n",
        "        q_u.append(len(wavelengths)-1)\n",
        "        \n",
        "        # Interpolação linear entre os pontos do casco\n",
        "        continuum = np.interp(wavelengths, wavelengths[q_u], spectrum[q_u])\n",
        "        X_cr[i, :] = spectrum / continuum\n",
        "    return X_cr\n",
        "\n",
        "def wavelet_denoising(X, wavelet='db4', level=4):\n",
        "    \"\"\"Wavelet Transform para Denoising.\"\"\"\n",
        "    X = np.asarray(X)\n",
        "    original_length = X.shape[1]\n",
        "    denoised_list = []\n",
        "\n",
        "    for i in range(X.shape[0]):\n",
        "        # 1. Decomposição Wavelet\n",
        "        coeffs = pywt.wavedec(X[i, :], wavelet, level=level)\n",
        "\n",
        "        # 2. Cálculo do limiar (threshold)\n",
        "        sigma = np.median(np.abs(coeffs[-1])) / 0.6745\n",
        "        threshold = sigma * np.sqrt(2 * np.log(original_length))\n",
        "\n",
        "        # 3. Aplicação do filtro (soft thresholding) nos coeficientes de detalhe\n",
        "        coeffs[1:] = [pywt.threshold(c, value=threshold, mode='soft') for c in coeffs[1:]]\n",
        "\n",
        "        # 4. Reconstrução do sinal\n",
        "        reconstructed_signal = pywt.waverec(coeffs, wavelet)\n",
        "\n",
        "        # 5. Ajuste do tamanho\n",
        "        denoised_list.append(reconstructed_signal[:original_length])\n",
        "\n",
        "    return np.asarray(denoised_list)\n",
        "\n",
        "# Filtros dependentes de y (Orthogonal Signal Correction)\n",
        "class OrthogonalCorrection:\n",
        "    \"\"\"Orthogonal Signal Correction (OSC).\"\"\"\n",
        "    def __init__(self, n_components=1):\n",
        "        self.n_components = n_components\n",
        "    \n",
        "    def fit_transform(self, X, y):\n",
        "        X, y = np.asarray(X), np.asarray(y).ravel()\n",
        "        self.w_ortho_ = []\n",
        "        self.p_ortho_ = []\n",
        "        self.X_corr_ = X.copy()\n",
        "        \n",
        "        for _ in range(self.n_components):\n",
        "            pls = PLSRegression(n_components=1)\n",
        "            pls.fit(self.X_corr_, y)\n",
        "            t = pls.x_scores_\n",
        "            w = pls.x_weights_\n",
        "            p = pls.x_loadings_\n",
        "            \n",
        "            # Componente Ortogonal\n",
        "            w_ortho = p - (np.dot(w.T, p) / np.dot(w.T, w)) * w\n",
        "            t_ortho = np.dot(self.X_corr_, w_ortho)\n",
        "            p_ortho = np.dot(t_ortho.T, self.X_corr_) / np.dot(t_ortho.T, t_ortho)\n",
        "            \n",
        "            # Remover variação ortogonal\n",
        "            self.X_corr_ -= np.dot(t_ortho, p_ortho)\n",
        "            self.w_ortho_.append(w_ortho)\n",
        "            self.p_ortho_.append(p_ortho)\n",
        "        \n",
        "        return self.X_corr_\n",
        "    \n",
        "    def transform(self, X):\n",
        "        X_res = np.asarray(X).copy()\n",
        "        for i in range(self.n_components):\n",
        "            t_ortho = np.dot(X_res, self.w_ortho_[i])\n",
        "            X_res -= np.dot(t_ortho, self.p_ortho_[i])\n",
        "        return X_res\n",
        "\n",
        "# Dicionário de filtros independentes de y\n",
        "filtros_independentes = {\n",
        "    'Raw': lambda X: X,\n",
        "    'MSC': msc,\n",
        "    'SNV': snv,\n",
        "    'SG_D1': lambda X: savitzky_golay(X, window_size=11, poly_order=2, deriv_order=1),\n",
        "    'SG_D2': lambda X: savitzky_golay(X, window_size=11, poly_order=2, deriv_order=2),\n",
        "    'Detrend': detrend_filter,\n",
        "    'Normalize': normalize,\n",
        "    'EMSC': emsc,\n",
        "    'Continuum_Removal': lambda X: continuum_removal(X, wavelength_values),\n",
        "    'Wavelet_Denoising': wavelet_denoising\n",
        "}\n",
        "\n",
        "# Dicionário de filtros dependentes de y\n",
        "filtros_dependentes = {\n",
        "    'OSC_1': lambda X, y: OrthogonalCorrection(n_components=1).fit_transform(X, y),\n",
        "    'OSC_2': lambda X, y: OrthogonalCorrection(n_components=2).fit_transform(X, y),\n",
        "    'MSC_SG_OSC': lambda X, y: OrthogonalCorrection(n_components=1).fit_transform(\n",
        "        savitzky_golay(msc(X), window_size=11, poly_order=2, deriv_order=1), y),\n",
        "    'OPLS1_SNV_SG_D1': lambda X, y: OrthogonalCorrection(n_components=1).fit_transform(\n",
        "        savitzky_golay(snv(X), window_size=11, poly_order=2, deriv_order=1), y),\n",
        "    'OPLS2_SNV_SG_D1': lambda X, y: OrthogonalCorrection(n_components=2).fit_transform(\n",
        "        savitzky_golay(snv(X), window_size=11, poly_order=2, deriv_order=1), y),\n",
        "    'SNV_Detrend_SG_D1': lambda X, y: savitzky_golay(detrend_filter(snv(X)), window_size=11, poly_order=2, deriv_order=1)\n",
        "}\n",
        "\n",
        "print(f'Filtros independentes de y: {list(filtros_independentes.keys())}')\n",
        "print(f'Filtros dependentes de y: {list(filtros_dependentes.keys())}')\n",
        "print(f'Total de filtros: {len(filtros_independentes) + len(filtros_dependentes)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Avaliação dos Filtros e Seleção dos Melhores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nesta etapa crucial, construímos e avaliamos os modelos de regressão. Em vez de testar cada modelo separadamente, criamos um fluxo automatizado e exaustivo para garantir que encontremos a melhor performance possível.\n",
        "- **Estratégia:**\n",
        "    1. **Iteração por Atributo:** Um loop externo passa por cada variável alvo (AT, PH, etc.).\n",
        "    2. **Iteração por Modelo:** Para cada atributo, testamos uma lista de modelos quimiométricos:\n",
        "        - `PLSR`: Padrão da área, lida bem com multicolinearidade.\n",
        "        - `PCR`: Alternativa que primeiro reduz a dimensionalidade.\n",
        "        - `RFR` (Random Forest): Modelo não-linear, robusto e poderoso.\n",
        "        - `SVMR` (Support Vector Machine): Eficaz em espaços de alta dimensão.\n",
        "    3. **Iteração por Filtro:** Para cada modelo, aplicamos **todos os 16 filtros** de pré-processamento definidos na seção anterior.\n",
        "    4. **Otimização (GridSearchCV):** Para cada combinação `(atributo, modelo, filtro)`, usamos `GridSearchCV` com validação cruzada (10-fold) para encontrar os melhores hiperparâmetros do modelo (ex: número de componentes para PLS, parâmetros C e gamma para SVM).\n",
        "- **Avaliação:** O desempenho de cada combinação é medido pelo R² da validação cruzada. O melhor modelo é então treinado com todos os dados de calibração e sua performance final é avaliada no conjunto de validação, que foi mantido separado durante todo o processo.\n",
        "- **Saídas:** Ao final, teremos os resultados de centenas de modelos, permitindo uma comparação justa e a seleção do campeão para cada atributo. Todos os gráficos de predição e as métricas são salvos automaticamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Função para o Gráfico de CALIBRAÇÃO (Ref vs Predito vs CV) ---\n",
        "def save_calibration_plot(y_cal, y_pred_cal, y_pred_cv, atributo, filtro, modelo, file_path):\n",
        "    \"\"\"\n",
        "    Gera e salva um gráfico comparando predições de treino e de validação cruzada\n",
        "    no conjunto de calibração.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Garante que o diretório exista\n",
        "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "        # Métricas para Predição no Treino Completo\n",
        "        slope_pred, offset_pred = np.polyfit(y_cal, y_pred_cal, 1)\n",
        "        rmse_pred = np.sqrt(mean_squared_error(y_cal, y_pred_cal))\n",
        "        r2_pred = r2_score(y_cal, y_pred_cal)\n",
        "\n",
        "        # Métricas para Predição da Validação Cruzada\n",
        "        slope_cv, offset_cv = np.polyfit(y_cal, y_pred_cv, 1)\n",
        "        rmse_cv = np.sqrt(mean_squared_error(y_cal, y_pred_cv))\n",
        "        r2_cv = r2_score(y_cal, y_pred_cv)\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.scatter(y_cal, y_pred_cal, color='blue', marker='o', alpha=0.6, label=f'Predição no Treino (R²={r2_pred:.3f})')\n",
        "        plt.scatter(y_cal, y_pred_cv, color='red', marker='x', alpha=0.7, label=f'Predição CV (R²={r2_cv:.3f})')\n",
        "        \n",
        "        # Linha ideal 1:1\n",
        "        min_val = min(min(y_cal), min(y_pred_cal), min(y_pred_cv))\n",
        "        max_val = max(max(y_cal), max(y_pred_cal), max(y_pred_cv))\n",
        "        plt.plot([min_val, max_val], [min_val, max_val], 'k--', label='Linha Ideal (1:1)')\n",
        "\n",
        "        plt.xlabel(\"Valores Reais (Calibração)\")\n",
        "        plt.ylabel(\"Valores Preditos\")\n",
        "        plt.title(f'Desempenho na Calibração: {atributo} | {modelo} | {filtro}')\n",
        "        plt.grid(True, linestyle='--', alpha=0.6)\n",
        "        plt.legend(loc='lower right')\n",
        "        \n",
        "        # Adiciona texto com métricas\n",
        "        stats_text = (\n",
        "            f'Treino - RMSE: {rmse_pred:.3f}, Slope: {slope_pred:.3f}, Offset: {offset_pred:.3f}\\n'\n",
        "            f'CV     - RMSE: {rmse_cv:.3f}, Slope: {slope_cv:.3f}, Offset: {offset_cv:.3f}'\n",
        "        )\n",
        "        plt.text(0.05, 0.95, stats_text, transform=plt.gca().transAxes, fontsize=10,\n",
        "                 verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5))\n",
        "\n",
        "        plt.savefig(file_path, format='png', dpi=200, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao gerar gráfico de calibração para {modelo} com {filtro}: {e}\")\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "# --- Função para o Gráfico de VALIDAÇÃO (Predito vs Real) ---\n",
        "def save_validation_plot(y_val, y_pred_val, atributo, filtro, modelo, file_path):\n",
        "    \"\"\"\n",
        "    Gera e salva o gráfico de predições no conjunto de validação (teste).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Garante que o diretório exista\n",
        "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "        \n",
        "        # Métricas\n",
        "        r2 = r2_score(y_val, y_pred_val)\n",
        "        rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
        "        slope, offset = np.polyfit(y_val, y_pred_val, 1)\n",
        "\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.scatter(y_val, y_pred_val, alpha=0.7, edgecolors='k', label='Dados de Validação')\n",
        "        \n",
        "        # Linha ideal 1:1\n",
        "        min_val = min(min(y_val), min(y_pred_val))\n",
        "        max_val = max(max(y_val), max(y_pred_val))\n",
        "        plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Linha Ideal (1:1)')\n",
        "\n",
        "        plt.xlabel('Valores Reais (Validação)')\n",
        "        plt.ylabel('Valores Preditos')\n",
        "        plt.title(f'Desempenho na Validação: {atributo} | {modelo} | {filtro}')\n",
        "        plt.grid(True, linestyle='--', alpha=0.6)\n",
        "        \n",
        "        stats_text = f'R² = {r2:.4f}\\nRMSE = {rmse:.4f}\\nSlope = {slope:.4f}'\n",
        "        plt.text(0.05, 0.95, stats_text, transform=plt.gca().transAxes, fontsize=12,\n",
        "                 verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', fc='lightblue', alpha=0.5))\n",
        "        \n",
        "        plt.legend()\n",
        "        plt.savefig(file_path, format='png', dpi=200, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao gerar gráfico de validação para {modelo} com {filtro}: {e}\")\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Definir modelos e seus grids de parâmetros\n",
        "modelos = {\n",
        "    'PLSR': {\n",
        "        'estimador': PLSRegression(),\n",
        "        'params': {'n_components': [5, 10, 15, 20]}\n",
        "    },\n",
        "    'PCR': {\n",
        "        'estimador': Pipeline([('pca', PCA()), ('regressor', LinearRegression())]),\n",
        "        'params': {'pca__n_components': [5, 10, 15, 20]}\n",
        "    },\n",
        "    'RFR': {\n",
        "        'estimador': RandomForestRegressor(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [100, 150, 200, 250, 300],\n",
        "            'max_depth': [10, 20, 30, None],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4],\n",
        "            'warm_start': [True, False],\n",
        "            'bootstrap': [True, False]\n",
        "        }\n",
        "    },\n",
        "    'SVMR': {\n",
        "        'estimador': SVR(),\n",
        "        'params': {\n",
        "            'C': [0.1, 1, 10, 100],\n",
        "            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
        "            'kernel': ['rbf', 'linear'],\n",
        "            'epsilon': [0.1, 0.2, 0.5, 1.0]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Combina todos os filtros\n",
        "todos_filtros = {**filtros_independentes, **filtros_dependentes}\n",
        "\n",
        "# Lista para armazenar todos os resultados\n",
        "lista_resultados_finais = []\n",
        "os.makedirs('graficos', exist_ok=True)  # Cria pasta para salvar gráficos\n",
        "os.makedirs('modelos_salvos', exist_ok=True) # Cria pasta para salvar os modelos\n",
        "os.makedirs('modelos_salvos_top5', exist_ok=True) # Nova pasta para os top 5\n",
        "\n",
        "# 2. Loop principal de modelagem\n",
        "for atributo in atributos:\n",
        "    print(f'\\n{\"=\"*30}')\n",
        "    print(f'INICIANDO MODELAGEM PARA: {atributo}')\n",
        "    print(f'{\"=\"*30}')\n",
        "\n",
        "    melhor_r2_atributo = -999  # Inicia com um valor muito baixo\n",
        "    melhor_combinacao_info = {}  # Dicionário para guardar as infos do melhor\n",
        "    \n",
        "    # Dicionário para armazenar as 5 melhores combinações por modelo\n",
        "    melhores_por_modelo = {nome: [] for nome in modelos.keys()}\n",
        "\n",
        "    dados = dados_final[atributo]\n",
        "    X_cal, y_cal = dados['X_cal'], dados['y_cal']\n",
        "    X_val, y_val = dados['X_val'], dados['y_val']\n",
        "\n",
        "    for nome_modelo, info_modelo in modelos.items():\n",
        "        print(f'\\n--- Modelo: {nome_modelo} ---')\n",
        "        \n",
        "        combinacoes_deste_modelo = [] # Lista temporária para este modelo\n",
        "\n",
        "        for nome_filtro, funcao_filtro in todos_filtros.items():\n",
        "            start_time = time.time()\n",
        "            print(f'  Testando filtro: {nome_filtro}...', end='')\n",
        "\n",
        "            # Aplicar filtro\n",
        "            if nome_filtro in filtros_independentes:\n",
        "                X_cal_f = funcao_filtro(X_cal)\n",
        "                X_val_f = funcao_filtro(X_val)\n",
        "                tipo_filtro = 'independente'\n",
        "            else:  # Filtro dependente de Y\n",
        "                osc_model = OrthogonalCorrection(n_components=1)\n",
        "                X_cal_f = osc_model.fit_transform(X_cal, y_cal)\n",
        "                X_val_f = osc_model.transform(X_val)\n",
        "                tipo_filtro = 'dependente'\n",
        "            \n",
        "            # GridSearchCV\n",
        "            grid = GridSearchCV(info_modelo['estimador'], info_modelo['params'], cv=10, scoring='r2', n_jobs=-1)\n",
        "            grid.fit(X_cal_f, y_cal)\n",
        "            \n",
        "            melhor_modelo = grid.best_estimator_\n",
        "            \n",
        "            # Avaliação final no conjunto de validação\n",
        "            y_pred = melhor_modelo.predict(X_val_f)\n",
        "            r2_val = r2_score(y_val, y_pred)\n",
        "            rmse_val = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "            \n",
        "            end_time = time.time()\n",
        "            print(f' R² Val: {r2_val:.4f} | Concluído em {end_time - start_time:.2f}s')\n",
        "\n",
        "            # Salvar o resultado da combinação atual\n",
        "            resultados_atuais = {\n",
        "                'Atributo': atributo,\n",
        "                'Modelo': nome_modelo,\n",
        "                'Filtro': nome_filtro,\n",
        "                'Tipo_Filtro': tipo_filtro,\n",
        "                'CV_R2': grid.best_score_,\n",
        "                'Val_R2': r2_val,\n",
        "                'Val_RMSE': rmse_val,\n",
        "                'Melhores_Params': str(grid.best_params_),\n",
        "                'Tempo_s': end_time - start_time,\n",
        "                'objeto_modelo': melhor_modelo # Adiciona o objeto do modelo\n",
        "            }\n",
        "            \n",
        "            lista_resultados_finais.append(resultados_atuais)\n",
        "            combinacoes_deste_modelo.append(resultados_atuais)\n",
        "\n",
        "            # Lógica para salvar o melhor modelo geral\n",
        "            if r2_val > melhor_r2_atributo:\n",
        "                melhor_r2_atributo = r2_val\n",
        "                melhor_combinacao_info = {\n",
        "                    'objeto_modelo': melhor_modelo,\n",
        "                    'nome_modelo': nome_modelo,\n",
        "                    'nome_filtro': nome_filtro,\n",
        "                    'parametros': grid.best_params_\n",
        "                }\n",
        "            \n",
        "            # --- Lógica de plotagem nova e aprimorada ---\n",
        "            y_pred_cal = melhor_modelo.predict(X_cal_f)\n",
        "            y_pred_cv = cross_val_predict(info_modelo['estimador'], X_cal_f, y_cal, cv=10)\n",
        "\n",
        "            path_cal = f'graficos_modelos/calibracao/{atributo}_{nome_modelo}_{nome_filtro}.png'\n",
        "            save_calibration_plot(y_cal, y_pred_cal, y_pred_cv, atributo, nome_filtro, nome_modelo, path_cal)\n",
        "\n",
        "            path_val = f'graficos_modelos/validacao/{atributo}_{nome_modelo}_{nome_filtro}.png'\n",
        "            save_validation_plot(y_val, y_pred, atributo, nome_filtro, nome_modelo, path_val)\n",
        "\n",
        "        # Ordenar e salvar as 5 melhores para este modelo\n",
        "        combinacoes_deste_modelo.sort(key=lambda x: x['Val_R2'], reverse=True)\n",
        "        melhores_por_modelo[nome_modelo] = combinacoes_deste_modelo[:5]\n",
        "\n",
        "    if melhor_combinacao_info:  # Garante que encontrou pelo menos um modelo\n",
        "        melhor_obj = melhor_combinacao_info['objeto_modelo']\n",
        "        nome_arquivo_modelo = f\"modelos_salvos/melhor_modelo_{atributo.replace(' ', '_')}.joblib\"\n",
        "        \n",
        "        joblib.dump(melhor_obj, nome_arquivo_modelo)\n",
        "        \n",
        "        print(f\"\\n🏆 MELHOR COMBINAÇÃO GERAL PARA '{atributo}' FOI SALVA!\")\n",
        "        print(f\"   - Arquivo: {nome_arquivo_modelo}\")\n",
        "        print(f\"   - Modelo: {melhor_combinacao_info['nome_modelo']}\")\n",
        "        print(f\"   - Filtro: {melhor_combinacao_info['nome_filtro']}\")\n",
        "        print(f\"   - Melhor R² de Validação: {melhor_r2_atributo:.4f}\")\n",
        "        print(f\"   - Melhores Parâmetros: {melhor_combinacao_info['parametros']}\")\n",
        "    \n",
        "    # Adicione esta parte no final do loop de atributo\n",
        "    # Salvar os 5 melhores modelos para cada tipo\n",
        "    print(f\"\\n--- Salvando os 5 melhores modelos para {atributo} ---\")\n",
        "    for nome_modelo, lista_combinacoes in melhores_por_modelo.items():\n",
        "        for i, combinacao in enumerate(lista_combinacoes):\n",
        "            nome_arquivo = f\"modelos_salvos_top5/top_{i+1}_{atributo.replace(' ', '_')}_{combinacao['Modelo']}_{combinacao['Filtro']}.joblib\"\n",
        "            joblib.dump(combinacao['objeto_modelo'], nome_arquivo)\n",
        "            print(f\"  - Top {i+1} do modelo {nome_modelo} salvo em: {nome_arquivo}\")\n",
        "\n",
        "print('\\n\\n✅ Modelagem exaustiva concluída!')\n",
        "\n",
        "# 3. Criar DataFrame e exportar para Excel\n",
        "df_completo = pd.DataFrame(lista_resultados_finais)\n",
        "df_completo_ordenado = df_completo.sort_values(by=['Atributo', 'Val_R2'], ascending=[True, False])\n",
        "df_completo_ordenado.to_excel('analise_completa_resultados.xlsx', index=False)\n",
        "print('✅ Resultados salvos em \"analise_completa_resultados.xlsx\"')\n",
        "\n",
        "print(\"\\n--- Melhores Resultados por Atributo (Todos os Modelos) ---\")\n",
        "melhores_resultados = df_completo_ordenado.groupby('Atributo').first().reset_index()\n",
        "\n",
        "# Arredondar colunas numéricas para melhor visualização\n",
        "colunas_numericas_para_arredondar = ['CV_R2', 'Val_R2', 'Val_RMSE']\n",
        "melhores_resultados[colunas_numericas_para_arredondar] = melhores_resultados[colunas_numericas_para_arredondar].round(4)\n",
        "\n",
        "# Selecionar e exibir as colunas mais importantes\n",
        "colunas_para_exibir = ['Atributo', 'Modelo', 'Filtro', 'Val_R2', 'Val_RMSE', 'Melhores_Params']\n",
        "print(melhores_resultados[colunas_para_exibir].to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Análise Comparativa dos Resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Todos os resultados da modelagem exaustiva foram compilados, ordenados e salvos no arquivo `analise_completa_resultados.xlsx`. Agora, podemos carregar essa planilha para visualizar e discutir os melhores modelos para cada atributo.\n",
        "\n",
        "A tabela abaixo mostra o **melhor resultado para cada modelo** dentro de cada atributo, facilitando a comparação entre as diferentes abordagens (PLSR, PCR, RFR, SVMR)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregar os resultados salvos para análise\n",
        "df_analise = pd.read_excel('analise_completa_resultados.xlsx')\n",
        "\n",
        "# Encontrar o melhor resultado para cada combinação de Atributo e Modelo\n",
        "idx = df_analise.groupby(['Atributo', 'Modelo'])['Val_R2'].transform(max) == df_analise['Val_R2']\n",
        "df_melhores_por_modelo = df_analise[idx]\n",
        "\n",
        "print(\"--- Tabela Comparativa dos Melhores Resultados por Modelo ---\")\n",
        "print(df_melhores_por_modelo[['Atributo', 'Modelo', 'Filtro', 'CV_R2', 'Val_R2', 'Val_RMSE']].round(4))\n",
        "\n",
        "# Encontrar o melhor modelo geral para cada atributo\n",
        "print(\"\\\\n--- Melhores Modelos Gerais por Atributo (baseado no R² de Validação) ---\")\n",
        "idx_geral = df_melhores_por_modelo.groupby(['Atributo'])['Val_R2'].transform(max) == df_melhores_por_modelo['Val_R2']\n",
        "print(df_melhores_por_modelo[idx_geral][['Atributo', 'Modelo', 'Filtro', 'Val_R2', 'Val_RMSE']].round(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Discussão de Robustez e Referências"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este fluxo de análise implementa um protocolo robusto e exaustivo, alinhado às melhores práticas da literatura em quimiometria e espectroscopia, garantindo alta confiabilidade e reprodutibilidade.\n",
        "\n",
        "**1. Divisão de Dados e Rastreabilidade**\n",
        "- **Kennard-Stone (KS):** Utilizamos o método de Kennard-Stone para dividir os dados em conjuntos de calibração (70%) e validação (30%). Diferente de uma divisão aleatória, o KS garante que o conjunto de calibração cubra a máxima variabilidade espectral dos dados, o que é fundamental para construir modelos mais generalistas.\n",
        "- **Rastreamento de Amostras:** Durante a divisão, os índices originais de cada amostra são preservados. Isso permite total rastreabilidade, possibilitando a identificação exata de quais amostras são removidas nas etapas de limpeza de outliers.\n",
        "\n",
        "**2. Tratamento de Outliers (Apenas na Calibração)**\n",
        "- **Outliers Espectrais (PCA, T²/Q):** Amostras com espectros anômalos são identificadas no conjunto de calibração usando Análise de Componentes Principais (PCA). A decisão de remoção baseia-se nas estatísticas T² (variação dentro do modelo) e Q (resíduos), que medem o quão \"estranha\" uma amostra é em relação à população principal.\n",
        "- **Outliers de Atributos (IQR):** Valores de referência (y) analiticamente improváveis são detectados pelo método do Intervalo Interquartil (IQR) e removidos.\n",
        "- **Validação da Robustez:** Crucialmente, a remoção de outliers ocorre **apenas no conjunto de calibração**. O conjunto de validação é então **verificado** usando os limites de T²/Q do modelo de calibração para identificar amostras que o modelo pode ter dificuldade em prever, fornecendo uma medida real de sua robustez em dados novos e não vistos.\n",
        "\n",
        "**3. Teste Exaustivo de Pré-processamentos**\n",
        "- **Avaliação Completa:** Em vez de uma pré-seleção, o notebook testa um vasto arsenal de **16 filtros** de pré-processamento para cada modelo. Isso inclui:\n",
        "    - **Filtros Independentes de y:** `MSC`, `SNV`, Derivadas Savitzky-Golay, `Detrend`, etc., para corrigir efeitos físicos como espalhamento de luz e desvios de linha de base.\n",
        "    - **Filtros Dependentes de y e Cadeias:** `OSC/OPLS` e combinações (ex: `SNV_Detrend_SG_D1`) para remover dos espectros a variação que não é correlacionada com a variável alvo, focando o modelo na informação química relevante.\n",
        "\n",
        "**4. Modelagem e Otimização Automatizada**\n",
        "- **Abordagem Exaustiva:** O núcleo da análise é um fluxo automatizado que cruza todos os **5 atributos** com todos os **4 modelos** e todos os **16 filtros**.\n",
        "- **Modelos Testados:**\n",
        "    - **PLSR:** Método padrão para espectroscopia, ideal para dados com alta colinearidade.\n",
        "    - **PCR:** Alternativa que foca na redução de dimensionalidade para controle de ruído.\n",
        "    - **RFR:** Modelo de machine learning não-linear, capaz de capturar relações complexas entre espectro e atributo.\n",
        "    - **SVMR:** Robusto para dados de alta dimensão e eficaz na prevenção de overfitting.\n",
        "- **Otimização de Hiperparâmetros (GridSearchCV):** Para **cada combinação** de modelo e filtro, uma busca em grade (`GridSearchCV`) é realizada para encontrar os melhores hiperparâmetros (ex: número de componentes latentes no PLSR, parâmetros do Random Forest), garantindo que cada modelo seja avaliado em sua performance ótima.\n",
        "\n",
        "**5. Validação e Geração de Relatórios**\n",
        "- **Validação Cruzada (10 folds):** Usada durante a otimização de hiperparâmetros no conjunto de calibração para uma estimativa robusta do desempenho.\n",
        "- **Teste Independente:** O desempenho final de cada modelo otimizado é medido no conjunto de validação, que permaneceu \"intocado\" durante todo o treinamento.\n",
        "- **Exportação Automatizada:** Todos os resultados, incluindo as métricas de performance e os melhores parâmetros para cada combinação, são salvos em uma planilha Excel (`.xlsx`). Além disso, os gráficos de valores preditos vs. reais são salvos como imagens (`.png`), garantindo um registro completo e de fácil acesso da análise.\n",
        "\n",
        "**6. Métricas de Qualidade**\n",
        "- **R² (Coeficiente de Determinação):** Indica a proporção da variância da variável alvo que é explicada pelo modelo (quanto mais próximo de 1, melhor).\n",
        "- **RMSE (Raiz do Erro Quadrático Médio):** Mede a magnitude média dos erros de predição, na mesma unidade da variável original (quanto menor, melhor).\n",
        "- **MAE (Erro Absoluto Médio):** Similar ao RMSE, mas menos sensível a grandes erros pontuais."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Conclusões\n",
        "\n",
        "Este fluxo garante:\n",
        "- ✅ Reprodutibilidade dos resultados\n",
        "- ✅ Robustez dos modelos\n",
        "- ✅ Validação adequada\n",
        "- ✅ Seleção objetiva de parâmetros\n",
        "- ✅ Comparabilidade entre métodos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Referências Principais\n",
        "\n",
        "1. **Brereton, R. G. (2003).** Chemometrics: Data analysis for the laboratory and chemical plant. Wiley.\n",
        "\n",
        "2. **Rinnan, Å., van den Berg, F., & Engelsen, S. B. (2009).** Review of the most common pre-processing techniques for near-infrared spectra. TrAC Trends in Analytical Chemistry, 28(10), 1201-1222.\n",
        "\n",
        "3. **Cozzolino, D. (2021).** An overview of the use of chemometrics in NIR spectroscopy.\n",
        "\n",
        "4. **Kennard, R. W., & Stone, L. A. (1969).** Computer aided design of experiments. Technometrics, 11(1), 137-148.\n",
        "\n",
        "5. **Wold, S., et al. (2001).** PLS-regression: a basic tool of chemometrics. Chemometrics and intelligent laboratory systems, 58(2), 109-130."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

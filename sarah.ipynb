{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f27fb9a",
   "metadata": {},
   "source": [
    "# Análise Espectroscópica\n",
    "\n",
    "Este notebook segue as melhores práticas da literatura para análise espectroscópica robusta, incluindo:\n",
    "- Cross Fold\n",
    "- Remoção de outliers espectrais (PCA, T²/Q)\n",
    "- Remoção de outliers dos atributos (boxplot + seaborn)\n",
    "- Teste de todos os filtros (dependentes e independentes de y)\n",
    "- Modelagem com PLSR, PCR, RFR, SVMR (hiperparâmetros da literatura)\n",
    "- Plotagens robustas e salvamento de métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43087f04",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260fbaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criando diretórios para salvar os gráficos...\n"
     ]
    }
   ],
   "source": [
    "# Imports necessários\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_predict\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from matplotlib.mlab import detrend\n",
    "import pywt\n",
    "\n",
    "# Cria diretórios para organizar todas as imagens salvas\n",
    "print(\"Criando diretórios para salvar os gráficos...\")\n",
    "os.makedirs('cv_graficos_modelos', exist_ok=True)\n",
    "os.makedirs('cv_modelos_salvos', exist_ok=True)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db46c4e",
   "metadata": {},
   "source": [
    "## 2. Carregamento dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19985b8b",
   "metadata": {},
   "source": [
    "O primeiro passo em qualquer projeto de ciência de dados é carregar e entender os dados. Nesta seção, realizamos as seguintes ações:\n",
    "- **Carregamento:** Utilizamos a biblioteca `pandas` para ler o arquivo `Avaliacao_Maturacao_Palmer_e_Tommy_Fieldspec.xlsx`, que contém os dados espectrais e os valores de referência (atributos físico-químicos).\n",
    "- **Separação de Dados:** A função `load_data` inteligentemente separa o arquivo em duas partes:\n",
    "    - `metadata`: Um DataFrame contendo as informações de referência (ex: Firmness, Dry Mass, TSS, TA, AA, Weight, Width, Lenght), que serão nossas variáveis-alvo (y).\n",
    "    - `wavelengths`: Um DataFrame contendo a resposta espectral (absorbância ou reflectância) em cada comprimento de onda, que serão nossas variáveis preditoras (X).\n",
    "- **Definição de Variáveis:** Convertemos os dados espectrais para um array NumPy (`X`) para otimizar os cálculos e definimos a lista de `atributos` que desejamos modelar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd3a7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para carregar dados\n",
    "def load_data(filepath):\n",
    "    \"\"\"Carrega dados espectrais e separa metadados de espectros.\"\"\"\n",
    "    df = pd.read_excel(filepath, engine='openpyxl')\n",
    "    \n",
    "    # Identificar colunas que são comprimentos de onda (numéricas)\n",
    "    numeric_cols = []\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            float(col)\n",
    "            numeric_cols.append(col)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    # Separar metadados e comprimentos de onda\n",
    "    metadata = df.drop(columns=numeric_cols)\n",
    "    wavelengths = df[numeric_cols]\n",
    "    \n",
    "    return metadata, wavelengths\n",
    "\n",
    "# Carregar dados\n",
    "filepath = 'Data/raw/Fruto/Avaliacao_Maturacao_Palmer_e_Tommy_Fieldspec.xlsx'\n",
    "metadata, wavelengths = load_data(filepath)\n",
    "X = wavelengths.values\n",
    "wavelength_values = wavelengths.columns.astype(float)\n",
    "# 8 VARIÁVEIS DA SARAH?\n",
    "atributos = ['Firmness (N)', 'Dry Mass (%)', 'TSS (Brix)', 'TA (g/mL)', 'AA (mg/100g)','Weight (g)','Width (mm)','Length (mm)']\n",
    "\n",
    "print(f'Dados carregados: {X.shape[0]} amostras, {X.shape[1]} comprimentos de onda')\n",
    "print(f'Faixa espectral: {wavelength_values.min():.1f} - {wavelength_values.max():.1f} nm')\n",
    "print(f'Atributos disponíveis: {list(metadata.columns)}')\n",
    "print(f'Atributos a analisar: {atributos}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be6893f",
   "metadata": {},
   "source": [
    "## 3. Filtros de Pré-processamento (Independentes e Dependentes de y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a8977",
   "metadata": {},
   "source": [
    "Os dados espectrais brutos contêm diversas fontes de variação indesejada que podem mascarar a informação química relevante. Essas variações podem ser causadas por espalhamento de luz (efeitos físicos da amostra), mudanças na linha de base (drift do instrumento) e ruído. O pré-processing visa minimizar esses efeitos para que o modelo foque na correlação entre o espectro e o atributo de interesse.\n",
    "\n",
    "Neste notebook, testamos uma ampla gama de filtros, divididos em duas categorias:\n",
    "\n",
    "### Filtros Independentes de y\n",
    "\n",
    "São aplicados apenas aos espectros (`X`) e não utilizam a informação da variável alvo (`y`).\n",
    "\n",
    "-   **`Raw`**: Utiliza os dados espectrais brutos, sem nenhum tratamento. Serve como uma linha de base para comparar a eficácia dos outros filtros.\n",
    "-   **`MSC` (Multiplicative Scatter Correction)**: Corrige o espalhamento de luz (aditivo e multiplicativo) causado por variações no tamanho de partícula e compactação da amostra. Ele ajusta cada espectro para se parecer mais com um espectro \"ideal\" (geralmente a média de todos os espectros).\n",
    "-   **`SNV` (Standard Normal Variate)**: Alternativa ao MSC que também corrige o espalhamento de luz. A diferença é que o SNV padroniza cada espectro individualmente (subtrai a média e divide pelo desvio padrão daquele espectro), sem usar um espectro de referência.\n",
    "-   **`SG_D1` e `SG_D2` (Savitzky-Golay Derivatives)**: Calcula a primeira ou a segunda derivada do espectro. Derivadas são excelentes para remover desvios de linha de base (efeitos aditivos) e para resolver picos espectrais sobrepostos, realçando a informação de bandas de absorção específicas.\n",
    "-   **`Detrend`**: Remove tendências lineares ou polinomiais da linha de base do espectro. É muito eficaz para corrigir \"inclinações\" no espectro causadas por drift do instrumento.\n",
    "-   **`Normalize`**: Realiza uma normalização Min-Max, escalonando a intensidade de cada espectro para um intervalo fixo (geralmente [0, 1]). Ajuda a corrigir variações de intensidade causadas por diferenças na distância da amostra ou na potência da fonte de luz.\n",
    "-   **`EMSC` (Extended Multiplicative Signal Correction)**: Uma versão avançada do MSC. Além de corrigir os efeitos de espalhamento, o EMSC pode incluir termos polinomiais para modelar e remover efeitos de linha de base mais complexos e não-lineares.\n",
    "-   **`Continuum Removal`**: Técnica que normaliza os espectros para que as bandas de absorção possam ser comparadas em termos de sua profundidade, e não de sua intensidade absoluta. Ele ajusta um \"envelope\" (casco convexo) sobre o espectro e divide o espectro original por este envelope, realçando as características de absorção.\n",
    "-   **`Wavelet_Denoising`**: Utiliza a Transformada Wavelet para decompor o espectro em diferentes níveis de frequência. A técnica permite remover o ruído (geralmente presente em altas frequências) de forma muito eficaz, preservando as principais características do sinal espectral.\n",
    "\n",
    "### Filtros Dependentes de y e Combinações\n",
    "\n",
    "Estes filtros utilizam a variável alvo (`y`) para otimizar a remoção de variação não correlacionada em `X`, ou são combinações sequenciais de múltiplos filtros para um tratamento mais completo.\n",
    "\n",
    "-   **`OSC_1` e `OSC_2` (Orthogonal Signal Correction)**: Filtro que remove componentes (1 ou 2, neste caso) dos espectros `X` que são ortogonais (não correlacionados) à variável alvo `y`. O objetivo é limpar `X` da variação que não ajuda na predição, potencialmente melhorando o modelo subsequente. Esta técnica é frequentemente referida como um pré-processamento **OPLS** (Orthogonal Projections to Latent Structures).\n",
    "-   **`MSC_SG_OSC`**: Uma **cadeia de pré-processamentos** aplicada na seguinte ordem:\n",
    "    1.  `MSC` para corrigir o espalhamento.\n",
    "    2.  `Savitzky-Golay (1ª derivada)` para corrigir a linha de base.\n",
    "    3.  `OSC (1 componente)` para remover variação não correlacionada com `y`.\n",
    "-   **`OPLS1_SNV_SG_D1` e `OPLS2_SNV_SG_D1`**: Outra cadeia de processamento:\n",
    "    1.  `SNV` para correção de espalhamento.\n",
    "    2.  `Savitzky-Golay (1ª derivada)`.\n",
    "    3.  `OPLS/OSC` para remover 1 ou 2 componentes ortogonais a `y`.\n",
    "-   **`SNV_Detrend_SG_D1`**: Uma combinação de filtros independentes de `y`, mas que, por sua complexidade, é testada junto às outras cadeias:\n",
    "    1.  `SNV`.\n",
    "    2.  `Detrend` para remoção de tendência.\n",
    "    3.  `Savitzky-Golay (1ª derivada)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80ad97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação dos filtros de pré-processamento\n",
    "# Filtros independentes de y\n",
    "\n",
    "\n",
    "def msc(X):\n",
    "    \"\"\"Multiplicative Scatter Correction.\"\"\"\n",
    "    X = np.asarray(X)\n",
    "    mean_spectrum = np.mean(X, axis=0)\n",
    "    corrected_spectra = np.zeros_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        slope, intercept = np.polyfit(mean_spectrum, X[i, :], 1)\n",
    "        corrected_spectra[i, :] = (X[i, :] - intercept) / slope\n",
    "    return corrected_spectra\n",
    "\n",
    "def snv(X):\n",
    "    \"\"\"Standard Normal Variate.\"\"\"\n",
    "    X = np.asarray(X)\n",
    "    return (X - np.mean(X, axis=1, keepdims=True)) / np.std(X, axis=1, keepdims=True)\n",
    "\n",
    "def savitzky_golay(X, window_size=11, poly_order=2, deriv_order=1):\n",
    "    \"\"\"Savitzky-Golay filter.\"\"\"\n",
    "    return savgol_filter(X, window_length=window_size, polyorder=poly_order, deriv=deriv_order, axis=1)\n",
    "\n",
    "def detrend_filter(X):\n",
    "    \"\"\"Detrending filter.\"\"\"\n",
    "    return detrend(X, axis=1)\n",
    "\n",
    "def normalize(X):\n",
    "    \"\"\"Normalização Min-Max.\"\"\"\n",
    "    return (X - np.min(X, axis=1, keepdims=True)) / (np.max(X, axis=1, keepdims=True) - np.min(X, axis=1, keepdims=True))\n",
    "\n",
    "def emsc(X, reference=None):\n",
    "    \"\"\"Extended Multiplicative Signal Correction.\"\"\"\n",
    "    X = np.asarray(X)\n",
    "    if reference is None:\n",
    "        reference = np.mean(X, axis=0)  # Usa o espectro médio como referência\n",
    "    \n",
    "    X_corr = np.zeros_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        # Modelo: X[i] ≈ a + b*reference\n",
    "        model = np.vstack([np.ones_like(reference), reference]).T\n",
    "        params, _, _, _ = np.linalg.lstsq(model, X[i, :], rcond=None)\n",
    "        a, b = params[0], params[1]\n",
    "        X_corr[i,:] = (X[i, :] - a) / b\n",
    "    return X_corr\n",
    "\n",
    "def continuum_removal(X, wavelengths):\n",
    "    \"\"\"Continuum Removal.\"\"\"\n",
    "    X = np.asarray(X)\n",
    "    X_cr = np.zeros_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        spectrum = X[i, :]\n",
    "        # Encontra os pontos do casco convexo superior\n",
    "        q_u = [0]\n",
    "        for k in range(1, len(wavelengths) - 1):\n",
    "            s_k = (spectrum[len(wavelengths)-1] - spectrum[0]) / (wavelengths[-1] - wavelengths[0])\n",
    "            s_q = (spectrum[k] - spectrum[q_u[-1]]) / (wavelengths[k] - wavelengths[q_u[-1]])\n",
    "            if s_q > s_k:\n",
    "                q_u.append(k)\n",
    "        q_u.append(len(wavelengths)-1)\n",
    "        \n",
    "        # Interpolação linear entre os pontos do casco\n",
    "        continuum = np.interp(wavelengths, wavelengths[q_u], spectrum[q_u])\n",
    "        X_cr[i, :] = spectrum / continuum\n",
    "    return X_cr\n",
    "\n",
    "def wavelet_denoising(X, wavelet='db4', level=4):\n",
    "    \"\"\"Wavelet Transform para Denoising.\"\"\"\n",
    "    X = np.asarray(X)\n",
    "    original_length = X.shape[1]\n",
    "    denoised_list = []\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        # 1. Decomposição Wavelet\n",
    "        coeffs = pywt.wavedec(X[i, :], wavelet, level=level)\n",
    "\n",
    "        # 2. Cálculo do limiar (threshold)\n",
    "        sigma = np.median(np.abs(coeffs[-1])) / 0.6745\n",
    "        threshold = sigma * np.sqrt(2 * np.log(original_length))\n",
    "\n",
    "        # 3. Aplicação do filtro (soft thresholding) nos coeficientes de detalhe\n",
    "        coeffs[1:] = [pywt.threshold(c, value=threshold, mode='soft') for c in coeffs[1:]]\n",
    "\n",
    "        # 4. Reconstrução do sinal\n",
    "        reconstructed_signal = pywt.waverec(coeffs, wavelet)\n",
    "\n",
    "        # 5. Ajuste do tamanho\n",
    "        denoised_list.append(reconstructed_signal[:original_length])\n",
    "\n",
    "    return np.asarray(denoised_list)\n",
    "\n",
    "# Filtros dependentes de y (Orthogonal Signal Correction)\n",
    "class OrthogonalCorrection:\n",
    "    \"\"\"Orthogonal Signal Correction (OSC).\"\"\"\n",
    "    def __init__(self, n_components=1):\n",
    "        self.n_components = n_components\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        X, y = np.asarray(X), np.asarray(y).ravel()\n",
    "        self.w_ortho_ = []\n",
    "        self.p_ortho_ = []\n",
    "        self.X_corr_ = X.copy()\n",
    "        \n",
    "        for _ in range(self.n_components):\n",
    "            pls = PLSRegression(n_components=1)\n",
    "            pls.fit(self.X_corr_, y)\n",
    "            t = pls.x_scores_\n",
    "            w = pls.x_weights_\n",
    "            p = pls.x_loadings_\n",
    "            \n",
    "            # Componente Ortogonal\n",
    "            w_ortho = p - (np.dot(w.T, p) / np.dot(w.T, w)) * w\n",
    "            t_ortho = np.dot(self.X_corr_, w_ortho)\n",
    "            p_ortho = np.dot(t_ortho.T, self.X_corr_) / np.dot(t_ortho.T, t_ortho)\n",
    "            \n",
    "            # Remover variação ortogonal\n",
    "            self.X_corr_ -= np.dot(t_ortho, p_ortho)\n",
    "            self.w_ortho_.append(w_ortho)\n",
    "            self.p_ortho_.append(p_ortho)\n",
    "        \n",
    "        return self.X_corr_\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_res = np.asarray(X).copy()\n",
    "        for i in range(self.n_components):\n",
    "            t_ortho = np.dot(X_res, self.w_ortho_[i])\n",
    "            X_res -= np.dot(t_ortho, self.p_ortho_[i])\n",
    "        return X_res\n",
    "\n",
    "# Dicionário de filtros independentes de y\n",
    "filtros_independentes = {\n",
    "    'Raw': lambda X: X,\n",
    "    'MSC': msc,\n",
    "    'SNV': snv,\n",
    "    'SG_D1': lambda X: savitzky_golay(X, window_size=11, poly_order=2, deriv_order=1),\n",
    "    'SG_D2': lambda X: savitzky_golay(X, window_size=11, poly_order=2, deriv_order=2),\n",
    "    'Detrend': detrend_filter,\n",
    "    'Normalize': normalize,\n",
    "    'EMSC': emsc,\n",
    "    'Continuum_Removal': lambda X: continuum_removal(X, wavelength_values),\n",
    "    'Wavelet_Denoising': wavelet_denoising\n",
    "}\n",
    "\n",
    "# Dicionário de filtros dependentes de y\n",
    "filtros_dependentes = {\n",
    "    'OSC_1': lambda X, y: OrthogonalCorrection(n_components=1).fit_transform(X, y),\n",
    "    'OSC_2': lambda X, y: OrthogonalCorrection(n_components=2).fit_transform(X, y),\n",
    "    'MSC_SG_OSC': lambda X, y: OrthogonalCorrection(n_components=1).fit_transform(\n",
    "        savitzky_golay(msc(X), window_size=11, poly_order=2, deriv_order=1), y),\n",
    "    'OPLS1_SNV_SG_D1': lambda X, y: OrthogonalCorrection(n_components=1).fit_transform(\n",
    "        savitzky_golay(snv(X), window_size=11, poly_order=2, deriv_order=1), y),\n",
    "    'OPLS2_SNV_SG_D1': lambda X, y: OrthogonalCorrection(n_components=2).fit_transform(\n",
    "        savitzky_golay(snv(X), window_size=11, poly_order=2, deriv_order=1), y),\n",
    "    'SNV_Detrend_SG_D1': lambda X, y: savitzky_golay(detrend_filter(snv(X)), window_size=11, poly_order=2, deriv_order=1)\n",
    "}\n",
    "\n",
    "print(f'Filtros independentes de y: {list(filtros_independentes.keys())}')\n",
    "print(f'Filtros dependentes de y: {list(filtros_dependentes.keys())}')\n",
    "print(f'Total de filtros: {len(filtros_independentes) + len(filtros_dependentes)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a985bc2b",
   "metadata": {},
   "source": [
    "## 4. Remoção de Outliers Espectrais (PCA, T²/Q) nos Dados de Calibração"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8309e1a7",
   "metadata": {},
   "source": [
    "Outliers espectrais são amostras cujo espectro é muito diferente do resto da população, podendo ser causados por erros de medição, contaminação ou características únicas da amostra. Eles podem prejudicar significativamente a capacidade de generalização do modelo.\n",
    "- **Método PCA (T²/Q):** Usamos a Análise de Componentes Principais (PCA) para identificar esses outliers.\n",
    "    - **Estatística T² (Hotelling's T²):** Mede a variação de uma amostra *dentro* do modelo PCA. Valores altos indicam que a amostra é um outlier na combinação das variáveis principais.\n",
    "    - **Estatística Q (Resíduos):** Mede a variação da amostra *fora* do modelo PCA (o que o modelo não conseguiu capturar). Valores altos indicam que a estrutura do espectro da amostra é anormal.\n",
    "- **Lógica:** Uma amostra é considerada outlier se seu valor de T² ou Q ultrapassa um limite de confiança (geralmente 3 desvios padrão da média).\n",
    "- **Importante:** A remoção de outliers é feita **apenas no conjunto de calibração**. O conjunto de validação deve permanecer intacto para simular dados \"reais\" que o modelo encontrará no futuro."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

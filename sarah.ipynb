{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f27fb9a",
   "metadata": {},
   "source": [
    "# Análise Espectroscópica\n",
    "\n",
    "Este notebook segue as melhores práticas da literatura para análise espectroscópica robusta, incluindo:\n",
    "- Cross Fold\n",
    "- Remoção de outliers espectrais (PCA, T²/Q)\n",
    "- Remoção de outliers dos atributos (boxplot + seaborn)\n",
    "- Teste de todos os filtros (dependentes e independentes de y)\n",
    "- Modelagem com PLSR, PCR, RFR, SVMR (hiperparâmetros da literatura)\n",
    "- Plotagens robustas e salvamento de métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43087f04",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260fbaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necessários\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_predict, cross_val_score\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from matplotlib.mlab import detrend\n",
    "import pywt\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Cria diretórios para organizar todas as imagens salvas\n",
    "print(\"Criando diretórios para salvar os gráficos...\")\n",
    "os.makedirs('graficos_outliers_pca', exist_ok=True)\n",
    "os.makedirs('graficos_outliers_boxplot', exist_ok=True)\n",
    "os.makedirs('modelos_salvos', exist_ok=True)\n",
    "os.makedirs('graficos_modelos', exist_ok=True) \n",
    "os.makedirs('graficos_mlpr', exist_ok=True)\n",
    "os.makedirs('graficos_modelos/calibracao', exist_ok=True)\n",
    "os.makedirs('graficos_modelos/validacao', exist_ok=True)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurações de plot\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db46c4e",
   "metadata": {},
   "source": [
    "## 2. Carregamento dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19985b8b",
   "metadata": {},
   "source": [
    "O primeiro passo em qualquer projeto de ciência de dados é carregar e entender os dados. Nesta seção, realizamos as seguintes ações:\n",
    "- **Carregamento:** Utilizamos a biblioteca `pandas` para ler o arquivo `Avaliacao_Maturacao_Palmer_e_Tommy_Fieldspec.xlsx`, que contém os dados espectrais e os valores de referência (atributos físico-químicos).\n",
    "- **Separação de Dados:** A função `load_data` inteligentemente separa o arquivo em duas partes:\n",
    "    - `metadata`: Um DataFrame contendo as informações de referência (ex: Firmness, Dry Mass, TSS, TA, AA, Weight, Width, Lenght), que serão nossas variáveis-alvo (y).\n",
    "    - `wavelengths`: Um DataFrame contendo a resposta espectral (absorbância ou reflectância) em cada comprimento de onda, que serão nossas variáveis preditoras (X).\n",
    "- **Definição de Variáveis:** Convertemos os dados espectrais para um array NumPy (`X`) para otimizar os cálculos e definimos a lista de `atributos` que desejamos modelar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd3a7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para carregar dados\n",
    "def load_data(filepath):\n",
    "    \"\"\"Carrega dados espectrais e separa metadados de espectros.\"\"\"\n",
    "    df = pd.read_excel(filepath, engine='openpyxl')\n",
    "    \n",
    "    # Identificar colunas que são comprimentos de onda (numéricas)\n",
    "    numeric_cols = []\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            float(col)\n",
    "            numeric_cols.append(col)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    # Separar metadados e comprimentos de onda\n",
    "    metadata = df.drop(columns=numeric_cols)\n",
    "    wavelengths = df[numeric_cols]\n",
    "    \n",
    "    return metadata, wavelengths\n",
    "\n",
    "# Carregar dados\n",
    "filepath = 'Data/raw/Fruto/Avaliacao_Maturacao_Palmer_e_Tommy_Fieldspec.xlsx'\n",
    "metadata, wavelengths = load_data(filepath)\n",
    "X = wavelengths.values\n",
    "wavelength_values = wavelengths.columns.astype(float)\n",
    "# VARIÁVEIS DA SARAH\n",
    "atributos = ['Firmness (N)', 'Dry Mass (%)', 'TSS (Brix)', 'TA (g/mL)', 'AA (mg/100g)','Weight (g)','Width (mm)','Length (mm)']\n",
    "\n",
    "print(f'Dados carregados: {X.shape[0]} amostras, {X.shape[1]} comprimentos de onda')\n",
    "print(f'Faixa espectral: {wavelength_values.min():.1f} - {wavelength_values.max():.1f} nm')\n",
    "print(f'Atributos disponíveis: {list(metadata.columns)}')\n",
    "print(f'Atributos a analisar: {atributos}')\n",
    "\n",
    "dados_completos = {}\n",
    "\n",
    "for atributo in atributos:\n",
    "    y = metadata[atributo].values\n",
    "    original_indices = np.arange(len(y))\n",
    "    \n",
    "    # Remove apenas valores NaN\n",
    "    mask = ~np.isnan(y)\n",
    "    X_clean = X[mask]\n",
    "    y_clean = y[mask]\n",
    "    original_indices_clean = original_indices[mask]\n",
    "    \n",
    "    dados_completos[atributo] = {\n",
    "        'X': X_clean,\n",
    "        'y': y_clean,\n",
    "        'indices_orig': original_indices_clean\n",
    "    }\n",
    "    \n",
    "    print(f'{atributo}: {X_clean.shape[0]} amostras válidas')\n",
    "\n",
    "print(f'\\nPreparação dos dados concluída para {len(atributos)} atributos!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be6893f",
   "metadata": {},
   "source": [
    "## 3. Filtros de Pré-processamento (Independentes e Dependentes de y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a8977",
   "metadata": {},
   "source": [
    "Os dados espectrais brutos contêm diversas fontes de variação indesejada que podem mascarar a informação química relevante. Essas variações podem ser causadas por espalhamento de luz (efeitos físicos da amostra), mudanças na linha de base (drift do instrumento) e ruído. O pré-processing visa minimizar esses efeitos para que o modelo foque na correlação entre o espectro e o atributo de interesse.\n",
    "\n",
    "Neste notebook, testamos uma ampla gama de filtros, divididos em duas categorias:\n",
    "\n",
    "### Filtros Independentes de y\n",
    "\n",
    "São aplicados apenas aos espectros (`X`) e não utilizam a informação da variável alvo (`y`).\n",
    "\n",
    "-   **`Raw`**: Utiliza os dados espectrais brutos, sem nenhum tratamento. Serve como uma linha de base para comparar a eficácia dos outros filtros.\n",
    "-   **`MSC` (Multiplicative Scatter Correction)**: Corrige o espalhamento de luz (aditivo e multiplicativo) causado por variações no tamanho de partícula e compactação da amostra. Ele ajusta cada espectro para se parecer mais com um espectro \"ideal\" (geralmente a média de todos os espectros).\n",
    "-   **`SNV` (Standard Normal Variate)**: Alternativa ao MSC que também corrige o espalhamento de luz. A diferença é que o SNV padroniza cada espectro individualmente (subtrai a média e divide pelo desvio padrão daquele espectro), sem usar um espectro de referência.\n",
    "-   **`SG_D1` e `SG_D2` (Savitzky-Golay Derivatives)**: Calcula a primeira ou a segunda derivada do espectro. Derivadas são excelentes para remover desvios de linha de base (efeitos aditivos) e para resolver picos espectrais sobrepostos, realçando a informação de bandas de absorção específicas.\n",
    "-   **`Detrend`**: Remove tendências lineares ou polinomiais da linha de base do espectro. É muito eficaz para corrigir \"inclinações\" no espectro causadas por drift do instrumento.\n",
    "-   **`Normalize`**: Realiza uma normalização Min-Max, escalonando a intensidade de cada espectro para um intervalo fixo (geralmente [0, 1]). Ajuda a corrigir variações de intensidade causadas por diferenças na distância da amostra ou na potência da fonte de luz.\n",
    "-   **`EMSC` (Extended Multiplicative Signal Correction)**: Uma versão avançada do MSC. Além de corrigir os efeitos de espalhamento, o EMSC pode incluir termos polinomiais para modelar e remover efeitos de linha de base mais complexos e não-lineares.\n",
    "-   **`Continuum Removal`**: Técnica que normaliza os espectros para que as bandas de absorção possam ser comparadas em termos de sua profundidade, e não de sua intensidade absoluta. Ele ajusta um \"envelope\" (casco convexo) sobre o espectro e divide o espectro original por este envelope, realçando as características de absorção.\n",
    "-   **`Wavelet_Denoising`**: Utiliza a Transformada Wavelet para decompor o espectro em diferentes níveis de frequência. A técnica permite remover o ruído (geralmente presente em altas frequências) de forma muito eficaz, preservando as principais características do sinal espectral.\n",
    "\n",
    "### Filtros Dependentes de y e Combinações\n",
    "\n",
    "Estes filtros utilizam a variável alvo (`y`) para otimizar a remoção de variação não correlacionada em `X`, ou são combinações sequenciais de múltiplos filtros para um tratamento mais completo.\n",
    "\n",
    "-   **`OSC_1` e `OSC_2` (Orthogonal Signal Correction)**: Filtro que remove componentes (1 ou 2, neste caso) dos espectros `X` que são ortogonais (não correlacionados) à variável alvo `y`. O objetivo é limpar `X` da variação que não ajuda na predição, potencialmente melhorando o modelo subsequente. Esta técnica é frequentemente referida como um pré-processamento **OPLS** (Orthogonal Projections to Latent Structures).\n",
    "-   **`MSC_SG_OSC`**: Uma **cadeia de pré-processamentos** aplicada na seguinte ordem:\n",
    "    1.  `MSC` para corrigir o espalhamento.\n",
    "    2.  `Savitzky-Golay (1ª derivada)` para corrigir a linha de base.\n",
    "    3.  `OSC (1 componente)` para remover variação não correlacionada com `y`.\n",
    "-   **`OPLS1_SNV_SG_D1` e `OPLS2_SNV_SG_D1`**: Outra cadeia de processamento:\n",
    "    1.  `SNV` para correção de espalhamento.\n",
    "    2.  `Savitzky-Golay (1ª derivada)`.\n",
    "    3.  `OPLS/OSC` para remover 1 ou 2 componentes ortogonais a `y`.\n",
    "-   **`SNV_Detrend_SG_D1`**: Uma combinação de filtros independentes de `y`, mas que, por sua complexidade, é testada junto às outras cadeias:\n",
    "    1.  `SNV`.\n",
    "    2.  `Detrend` para remoção de tendência.\n",
    "    3.  `Savitzky-Golay (1ª derivada)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80ad97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação dos filtros de pré-processamento\n",
    "# Filtros independentes de y\n",
    "\n",
    "\n",
    "def msc(X):\n",
    "    \"\"\"Multiplicative Scatter Correction.\"\"\"\n",
    "    X = np.asarray(X)\n",
    "    mean_spectrum = np.mean(X, axis=0)\n",
    "    corrected_spectra = np.zeros_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        slope, intercept = np.polyfit(mean_spectrum, X[i, :], 1)\n",
    "        corrected_spectra[i, :] = (X[i, :] - intercept) / slope\n",
    "    return corrected_spectra\n",
    "\n",
    "def snv(X):\n",
    "    \"\"\"Standard Normal Variate.\"\"\"\n",
    "    X = np.asarray(X)\n",
    "    return (X - np.mean(X, axis=1, keepdims=True)) / np.std(X, axis=1, keepdims=True)\n",
    "\n",
    "def savitzky_golay(X, window_size=11, poly_order=2, deriv_order=1):\n",
    "    \"\"\"Savitzky-Golay filter.\"\"\"\n",
    "    return savgol_filter(X, window_length=window_size, polyorder=poly_order, deriv=deriv_order, axis=1)\n",
    "\n",
    "def detrend_filter(X):\n",
    "    \"\"\"Detrending filter.\"\"\"\n",
    "    return detrend(X, axis=1)\n",
    "\n",
    "def normalize(X):\n",
    "    \"\"\"Normalização Min-Max.\"\"\"\n",
    "    return (X - np.min(X, axis=1, keepdims=True)) / (np.max(X, axis=1, keepdims=True) - np.min(X, axis=1, keepdims=True))\n",
    "\n",
    "def emsc(X, reference=None):\n",
    "    \"\"\"Extended Multiplicative Signal Correction.\"\"\"\n",
    "    X = np.asarray(X)\n",
    "    if reference is None:\n",
    "        reference = np.mean(X, axis=0)  # Usa o espectro médio como referência\n",
    "    \n",
    "    X_corr = np.zeros_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        # Modelo: X[i] ≈ a + b*reference\n",
    "        model = np.vstack([np.ones_like(reference), reference]).T\n",
    "        params, _, _, _ = np.linalg.lstsq(model, X[i, :], rcond=None)\n",
    "        a, b = params[0], params[1]\n",
    "        X_corr[i,:] = (X[i, :] - a) / b\n",
    "    return X_corr\n",
    "\n",
    "def continuum_removal(X, wavelengths):\n",
    "    \"\"\"Continuum Removal.\"\"\"\n",
    "    X = np.asarray(X)\n",
    "    X_cr = np.zeros_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        spectrum = X[i, :]\n",
    "        # Encontra os pontos do casco convexo superior\n",
    "        q_u = [0]\n",
    "        for k in range(1, len(wavelengths) - 1):\n",
    "            s_k = (spectrum[len(wavelengths)-1] - spectrum[0]) / (wavelengths[-1] - wavelengths[0])\n",
    "            s_q = (spectrum[k] - spectrum[q_u[-1]]) / (wavelengths[k] - wavelengths[q_u[-1]])\n",
    "            if s_q > s_k:\n",
    "                q_u.append(k)\n",
    "        q_u.append(len(wavelengths)-1)\n",
    "        \n",
    "        # Interpolação linear entre os pontos do casco\n",
    "        continuum = np.interp(wavelengths, wavelengths[q_u], spectrum[q_u])\n",
    "        X_cr[i, :] = spectrum / continuum\n",
    "    return X_cr\n",
    "\n",
    "def wavelet_denoising(X, wavelet='db4', level=4):\n",
    "    \"\"\"Wavelet Transform para Denoising.\"\"\"\n",
    "    X = np.asarray(X)\n",
    "    original_length = X.shape[1]\n",
    "    denoised_list = []\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        # 1. Decomposição Wavelet\n",
    "        coeffs = pywt.wavedec(X[i, :], wavelet, level=level)\n",
    "\n",
    "        # 2. Cálculo do limiar (threshold)\n",
    "        sigma = np.median(np.abs(coeffs[-1])) / 0.6745\n",
    "        threshold = sigma * np.sqrt(2 * np.log(original_length))\n",
    "\n",
    "        # 3. Aplicação do filtro (soft thresholding) nos coeficientes de detalhe\n",
    "        coeffs[1:] = [pywt.threshold(c, value=threshold, mode='soft') for c in coeffs[1:]]\n",
    "\n",
    "        # 4. Reconstrução do sinal\n",
    "        reconstructed_signal = pywt.waverec(coeffs, wavelet)\n",
    "\n",
    "        # 5. Ajuste do tamanho\n",
    "        denoised_list.append(reconstructed_signal[:original_length])\n",
    "\n",
    "    return np.asarray(denoised_list)\n",
    "\n",
    "# Filtros dependentes de y (Orthogonal Signal Correction)\n",
    "class OrthogonalCorrection:\n",
    "    \"\"\"Orthogonal Signal Correction (OSC).\"\"\"\n",
    "    def __init__(self, n_components=1):\n",
    "        self.n_components = n_components\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        X, y = np.asarray(X), np.asarray(y).ravel()\n",
    "        self.w_ortho_ = []\n",
    "        self.p_ortho_ = []\n",
    "        self.X_corr_ = X.copy()\n",
    "        \n",
    "        for _ in range(self.n_components):\n",
    "            pls = PLSRegression(n_components=1)\n",
    "            pls.fit(self.X_corr_, y)\n",
    "            t = pls.x_scores_\n",
    "            w = pls.x_weights_\n",
    "            p = pls.x_loadings_\n",
    "            \n",
    "            # Componente Ortogonal\n",
    "            w_ortho = p - (np.dot(w.T, p) / np.dot(w.T, w)) * w\n",
    "            t_ortho = np.dot(self.X_corr_, w_ortho)\n",
    "            p_ortho = np.dot(t_ortho.T, self.X_corr_) / np.dot(t_ortho.T, t_ortho)\n",
    "            \n",
    "            # Remover variação ortogonal\n",
    "            self.X_corr_ -= np.dot(t_ortho, p_ortho)\n",
    "            self.w_ortho_.append(w_ortho)\n",
    "            self.p_ortho_.append(p_ortho)\n",
    "        \n",
    "        return self.X_corr_\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_res = np.asarray(X).copy()\n",
    "        for i in range(self.n_components):\n",
    "            t_ortho = np.dot(X_res, self.w_ortho_[i])\n",
    "            X_res -= np.dot(t_ortho, self.p_ortho_[i])\n",
    "        return X_res\n",
    "\n",
    "# Dicionário de filtros independentes de y\n",
    "filtros_independentes = {\n",
    "    'Raw': lambda X: X,\n",
    "    'MSC': msc,\n",
    "    'SNV': snv,\n",
    "    'SG_D1': lambda X: savitzky_golay(X, window_size=11, poly_order=2, deriv_order=1),\n",
    "    'SG_D2': lambda X: savitzky_golay(X, window_size=11, poly_order=2, deriv_order=2),\n",
    "    'Detrend': detrend_filter,\n",
    "    'Normalize': normalize,\n",
    "    'EMSC': emsc,\n",
    "    'Continuum_Removal': lambda X: continuum_removal(X, wavelength_values),\n",
    "    'Wavelet_Denoising': wavelet_denoising\n",
    "}\n",
    "\n",
    "# Dicionário de filtros dependentes de y\n",
    "filtros_dependentes = {\n",
    "    'OSC_1': lambda X, y: OrthogonalCorrection(n_components=1).fit_transform(X, y),\n",
    "    'OSC_2': lambda X, y: OrthogonalCorrection(n_components=2).fit_transform(X, y),\n",
    "    'MSC_SG_OSC': lambda X, y: OrthogonalCorrection(n_components=1).fit_transform(\n",
    "        savitzky_golay(msc(X), window_size=11, poly_order=2, deriv_order=1), y),\n",
    "    'OPLS1_SNV_SG_D1': lambda X, y: OrthogonalCorrection(n_components=1).fit_transform(\n",
    "        savitzky_golay(snv(X), window_size=11, poly_order=2, deriv_order=1), y),\n",
    "    'OPLS2_SNV_SG_D1': lambda X, y: OrthogonalCorrection(n_components=2).fit_transform(\n",
    "        savitzky_golay(snv(X), window_size=11, poly_order=2, deriv_order=1), y),\n",
    "    'SNV_Detrend_SG_D1': lambda X, y: savitzky_golay(detrend_filter(snv(X)), window_size=11, poly_order=2, deriv_order=1)\n",
    "}\n",
    "\n",
    "print(f'Filtros independentes de y: {list(filtros_independentes.keys())}')\n",
    "print(f'Filtros dependentes de y: {list(filtros_dependentes.keys())}')\n",
    "print(f'Total de filtros: {len(filtros_independentes) + len(filtros_dependentes)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f737e308",
   "metadata": {},
   "source": [
    "## 4. Remoção de Outliers dos Atributos (Boxplot + Seaborn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8309e1a7",
   "metadata": {},
   "source": [
    "Outliers espectrais são amostras cujo espectro é muito diferente do resto da população, podendo ser causados por erros de medição, contaminação ou características únicas da amostra. Eles podem prejudicar significativamente a capacidade de generalização do modelo.\n",
    "- **Método PCA (T²/Q):** Usamos a Análise de Componentes Principais (PCA) para identificar esses outliers.\n",
    "    - **Estatística T² (Hotelling's T²):** Mede a variação de uma amostra *dentro* do modelo PCA. Valores altos indicam que a amostra é um outlier na combinação das variáveis principais.\n",
    "    - **Estatística Q (Resíduos):** Mede a variação da amostra *fora* do modelo PCA (o que o modelo não conseguiu capturar). Valores altos indicam que a estrutura do espectro da amostra é anormal.\n",
    "- **Lógica:** Uma amostra é considerada outlier se seu valor de T² ou Q ultrapassa um limite de confiança (geralmente 3 desvios padrão da média).\n",
    "- **Importante:** A remoção de outliers é feita **apenas no conjunto de calibração**. O conjunto de validação deve permanecer intacto para simular dados \"reais\" que o modelo encontrará no futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f251846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_outlier_removal(X, n_components=10, threshold=3):\n",
    "    \"\"\"Remove outliers usando PCA com estatísticas T² e Q.\"\"\"\n",
    "    # Padronizar dados\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Aplicar PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Calcular estatística T² (Hotelling's T²)\n",
    "    T2 = np.sum((X_pca / np.std(X_pca, axis=0)) ** 2, axis=1)\n",
    "    \n",
    "    # Calcular estatística Q (resíduos)\n",
    "    X_reconstructed = pca.inverse_transform(X_pca)\n",
    "    Q = np.sum((X_scaled - X_reconstructed) ** 2, axis=1)\n",
    "    \n",
    "    # Definir limites (média + threshold * desvio padrão)\n",
    "    T2_limit = np.mean(T2) + threshold * np.std(T2)\n",
    "    Q_limit = np.mean(Q) + threshold * np.std(Q)\n",
    "    \n",
    "    # Identificar outliers\n",
    "    outliers_mask = (T2 > T2_limit) | (Q > Q_limit)\n",
    "    \n",
    "    # Retornamos a máscara de quem NÃO é outlier e o modelo PCA treinado\n",
    "    return ~outliers_mask, pca, T2, Q, T2_limit, Q_limit\n",
    "\n",
    "# Remover outliers espectrais dos dados de calibração\n",
    "dados_pca = {}\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" for atributo in atributos:\n",
    "    X_cal = dados_divididos[atributo]['X_cal']\n",
    "    y_cal = dados_divididos[atributo]['y_cal']\n",
    "    indices_orig_cal = dados_divididos[atributo]['indices_orig_cal']\n",
    "    \n",
    "    # Chama a função de remoção de outliers e armazena todos os resultados\n",
    "    keep_mask, pca_model, T2, Q, T2_limit, Q_limit = pca_outlier_removal(X_cal)\n",
    "    \n",
    "    # Identifica e informa os outliers removidos\n",
    "    outliers_indices = indices_orig_cal[~keep_mask]\n",
    "    print(f'{atributo}: {len(outliers_indices)} outliers espectrais removidos da calibração.')\n",
    "    if len(outliers_indices) > 0:\n",
    "        print(f'  Índices Originais Removidos: {outliers_indices}')\n",
    "    \n",
    "    # Filtra os dados de calibração\n",
    "    X_cal_clean = X_cal[keep_mask]\n",
    "    y_cal_clean = y_cal[keep_mask]\n",
    "    indices_orig_cal_clean = indices_orig_cal[keep_mask]\n",
    "    \n",
    "    dados_pca[atributo] = {\n",
    "        'X_cal': X_cal_clean,\n",
    "        'y_cal': y_cal_clean,\n",
    "        'indices_orig_cal': indices_orig_cal_clean,\n",
    "        'pca_model': pca_model,\n",
    "        'T2': T2,\n",
    "        'Q': Q,\n",
    "        'T2_limit': T2_limit,\n",
    "        'Q_limit': Q_limit,\n",
    "        'keep_mask': keep_mask\n",
    "    }\n",
    "    \n",
    "    print(f'  Calibração: {X_cal_clean.shape[0]} amostras')\n",
    "    print(f'  Validação: {dados_divididos[atributo][\"X_val\"].shape[0]} amostras')\n",
    "    print()\n",
    "\"\"\" \n",
    "for atributo in atributos:\n",
    "    X_full = dados_completos[atributo]['X']\n",
    "    y_full = dados_completos[atributo]['y']\n",
    "    indices_orig_full = dados_completos[atributo]['indices_orig']\n",
    "    \n",
    "    # Chama a função de remoção de outliers espectrais\n",
    "    keep_mask, pca_model, T2, Q, T2_limit, Q_limit = pca_outlier_removal(X_full)\n",
    "    \n",
    "    # Identifica e informa os outliers removidos\n",
    "    outliers_indices = indices_orig_full[~keep_mask]\n",
    "    print(f'{atributo}: {len(outliers_indices)} outliers espectrais removidos.')\n",
    "    if len(outliers_indices) > 0:\n",
    "        print(f'  Índices Originais Removidos: {outliers_indices}')\n",
    "    \n",
    "    # Filtra os dados\n",
    "    X_clean = X_full[keep_mask]\n",
    "    y_clean = y_full[keep_mask]\n",
    "    indices_orig_clean = indices_orig_full[keep_mask]\n",
    "    \n",
    "    dados_pca[atributo] = {\n",
    "        'X': X_clean,\n",
    "        'y': y_clean,\n",
    "        'indices_orig': indices_orig_clean,\n",
    "        'pca_model': pca_model,\n",
    "        'T2': T2,\n",
    "        'Q': Q,\n",
    "        'T2_limit': T2_limit,\n",
    "        'Q_limit': Q_limit,\n",
    "        'keep_mask': keep_mask\n",
    "    }\n",
    "    \n",
    "    print(f'  Dataset final: {X_clean.shape[0]} amostras')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36f82ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Verificação de Outliers na Validação\n",
    "print(\"--- Verificação de Outliers no Conjunto de Validação ---\")\n",
    "for atributo in atributos:\n",
    "    # Carrega o modelo PCA e os dados de validação\n",
    "    pca_model = dados_pca[atributo]['pca_model']\n",
    "    X_val = dados_divididos[atributo]['X_val']\n",
    "    indices_orig_val = dados_divididos[atributo]['indices_orig_val']\n",
    "    \n",
    "    # Recalcula os limites usando o modelo salvo, se necessário\n",
    "    # (Ou usa os limites já calculados e salvos anteriormente)\n",
    "    \n",
    "    # Padroniza e projeta os dados de validação\n",
    "    scaler_val = StandardScaler().fit(dados_divididos[atributo]['X_cal']) # Usa o scaler da calibração\n",
    "    X_val_scaled = scaler_val.transform(X_val)\n",
    "    X_val_pca = pca_model.transform(X_val_scaled)\n",
    "\n",
    "    # Calcula T² e Q para a validação\n",
    "    T2_val = np.sum((X_val_pca / np.std(X_val_pca, axis=0)) ** 2, axis=1)\n",
    "    X_val_reconstructed = pca_model.inverse_transform(X_val_pca)\n",
    "    Q_val = np.sum((X_val_scaled - X_val_reconstructed) ** 2, axis=1)\n",
    "\n",
    "    # Identifica outliers da validação usando os limites da CALIBRAÇÃO\n",
    "    outliers_val_mask = (T2_val > T2_limit) | (Q_val > Q_limit)\n",
    "    outliers_val_indices = indices_orig_val[outliers_val_mask]\n",
    "    \n",
    "    print(f'\\nAtributo: {atributo}')\n",
    "    if len(outliers_val_indices) > 0:\n",
    "        print(f'  {len(outliers_val_indices)} amostras de VALIDAÇÃO excedem os limites da calibração.')\n",
    "        print(f'  Índices Originais: {outliers_val_indices}')\n",
    "    else:\n",
    "        print('  Nenhuma amostra de validação excede os limites da calibração.')\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f96f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotagem dos gráficos de outliers PCA\n",
    "for atributo in atributos:\n",
    "    d = dados_pca[atributo]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Gráfico 1: Variância explicada cumulativa\n",
    "    cumulative_variance = np.cumsum(d['pca_model'].explained_variance_ratio_)\n",
    "    axes[0].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'o-', linewidth=2, markersize=6)\n",
    "    axes[0].set_xlabel('Número de Componentes Principais')\n",
    "    axes[0].set_ylabel('Variância Explicada Cumulativa')\n",
    "    axes[0].set_title(f'Variância Explicada - {atributo}')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].axhline(y=0.95, color='r', linestyle='--', alpha=0.7, label='95%')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Gráfico 2: T² vs Q (detecção de outliers)\n",
    "    scatter = axes[1].scatter(d['T2'], d['Q'], c=d['keep_mask'], cmap='coolwarm', \n",
    "                              edgecolor='k', alpha=0.7, s=50)\n",
    "    axes[1].axhline(d['Q_limit'], color='r', linestyle='--', linewidth=2, label=f'Q Limit: {d[\"Q_limit\"]:.2f}')\n",
    "    axes[1].axvline(d['T2_limit'], color='g', linestyle='--', linewidth=2, label=f'T² Limit: {d[\"T2_limit\"]:.2f}')\n",
    "    axes[1].set_xlabel(\"Hotelling's T²\")\n",
    "    axes[1].set_ylabel('Q Residual')\n",
    "    axes[1].set_title(f'Detecção de Outliers - {atributo}')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Adicionar legenda de cores\n",
    "    legend1 = axes[1].legend(*scatter.legend_elements(), title=\"Status\")\n",
    "    axes[1].add_artist(legend1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Salvar gráfico\n",
    "    nome_arquivo = f'graficos_outliers_pca/pca_outliers_{atributo.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"/\", \"_\")}.png'\n",
    "    plt.savefig(nome_arquivo, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'Gráficos de outliers PCA para {atributo} salvos em: {nome_arquivo}')\n",
    "    print()\n",
    "    \n",
    "\"\"\"\n",
    "for atributo in atributos:\n",
    "    d = dados_pca[atributo]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Gráfico 1: Variância explicada cumulativa\n",
    "    # Acessa o modelo PCA com a chave correta 'pca_model'\n",
    "    cumulative_variance = np.cumsum(d['pca_model'].explained_variance_ratio_)\n",
    "    axes[0].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'o-', linewidth=2, markersize=6)\n",
    "    axes[0].set_xlabel('Número de Componentes Principais')\n",
    "    axes[0].set_ylabel('Variância Explicada Cumulativa')\n",
    "    axes[0].set_title(f'Variância Explicada - {atributo}')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].axhline(y=0.95, color='r', linestyle='--', alpha=0.7, label='95%')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Gráfico 2: T² vs Q (detecção de outliers)\n",
    "    # Acessa as variáveis T2, Q, T2_limit, Q_limit e keep_mask com as chaves corretas\n",
    "    scatter = axes[1].scatter(d['T2'], d['Q'], c=d['keep_mask'], cmap='coolwarm', \n",
    "                              edgecolor='k', alpha=0.7, s=50)\n",
    "    axes[1].axhline(d['Q_limit'], color='r', linestyle='--', linewidth=2, label=f'Q Limit: {d[\"Q_limit\"]:.2f}')\n",
    "    axes[1].axvline(d['T2_limit'], color='g', linestyle='--', linewidth=2, label=f'T² Limit: {d[\"T2_limit\"]:.2f}')\n",
    "    axes[1].set_xlabel(\"Hotelling's T²\")\n",
    "    axes[1].set_ylabel('Q Residual')\n",
    "    axes[1].set_title(f'Detecção de Outliers - {atributo}')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Adicionar legenda de cores (esta parte também foi ajustada para a chave 'keep_mask')\n",
    "    legend1 = axes[1].legend(*scatter.legend_elements(), title=\"Status\")\n",
    "    axes[1].add_artist(legend1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'Gráficos de outliers para {atributo} exibidos.')\n",
    "    print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdf7e26",
   "metadata": {},
   "source": [
    "## Função para o Gráfico de CALIBRAÇÃO (Ref vs Predito vs CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447242ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Função para o Gráfico de CALIBRAÇÃO (Ref vs Predito vs CV) ---\n",
    "def save_calibration_plot(y_cal, y_pred_cal, y_pred_cv, atributo, filtro, modelo, file_path):\n",
    "    \"\"\"\n",
    "    Gera e salva um gráfico comparando predições de treino e de validação cruzada\n",
    "    no conjunto de calibração.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Garante que o diretório exista\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "        # Métricas para Predição no Treino Completo\n",
    "        slope_pred, offset_pred = np.polyfit(y_cal, y_pred_cal, 1)\n",
    "        rmse_pred = np.sqrt(mean_squared_error(y_cal, y_pred_cal))\n",
    "        r2_pred = r2_score(y_cal, y_pred_cal)\n",
    "\n",
    "        # Métricas para Predição da Validação Cruzada\n",
    "        slope_cv, offset_cv = np.polyfit(y_cal, y_pred_cv, 1)\n",
    "        rmse_cv = np.sqrt(mean_squared_error(y_cal, y_pred_cv))\n",
    "        r2_cv = r2_score(y_cal, y_pred_cv)\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(y_cal, y_pred_cal, color='blue', marker='o', alpha=0.6, label=f'Predição no Treino (R²={r2_pred:.3f})')\n",
    "        plt.scatter(y_cal, y_pred_cv, color='red', marker='x', alpha=0.7, label=f'Predição CV (R²={r2_cv:.3f})')\n",
    "        \n",
    "        # Linha ideal 1:1\n",
    "        min_val = min(min(y_cal), min(y_pred_cal), min(y_pred_cv))\n",
    "        max_val = max(max(y_cal), max(y_pred_cal), max(y_pred_cv))\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], 'k--', label='Linha Ideal (1:1)')\n",
    "\n",
    "        plt.xlabel(\"Valores Reais (Calibração)\")\n",
    "        plt.ylabel(\"Valores Preditos\")\n",
    "        plt.title(f'Desempenho na Calibração: {atributo} | {modelo} | {filtro}')\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.legend(loc='lower right')\n",
    "        \n",
    "        # Adiciona texto com métricas\n",
    "        stats_text = (\n",
    "            f'Treino - RMSE: {rmse_pred:.3f}, Slope: {slope_pred:.3f}, Offset: {offset_pred:.3f}\\n'\n",
    "            f'CV     - RMSE: {rmse_cv:.3f}, Slope: {slope_cv:.3f}, Offset: {offset_cv:.3f}'\n",
    "        )\n",
    "        plt.text(0.05, 0.95, stats_text, transform=plt.gca().transAxes, fontsize=10,\n",
    "                 verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5))\n",
    "\n",
    "        plt.savefig(file_path, format='png', dpi=200, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao gerar gráfico de calibração para {modelo} com {filtro}: {e}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# --- Função para o Gráfico de VALIDAÇÃO (Predito vs Real) ---\n",
    "def save_validation_plot(y_val, y_pred_val, atributo, filtro, modelo, file_path):\n",
    "    \"\"\"\n",
    "    Gera e salva o gráfico de predições no conjunto de validação (teste).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Garante que o diretório exista\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        \n",
    "        # Métricas\n",
    "        r2 = r2_score(y_val, y_pred_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
    "        slope, offset = np.polyfit(y_val, y_pred_val, 1)\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.scatter(y_val, y_pred_val, alpha=0.7, edgecolors='k', label='Dados de Validação')\n",
    "        \n",
    "        # Linha ideal 1:1\n",
    "        min_val = min(min(y_val), min(y_pred_val))\n",
    "        max_val = max(max(y_val), max(y_pred_val))\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Linha Ideal (1:1)')\n",
    "\n",
    "        plt.xlabel('Valores Reais (Validação)')\n",
    "        plt.ylabel('Valores Preditos')\n",
    "        plt.title(f'Desempenho na Validação: {atributo} | {modelo} | {filtro}')\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        stats_text = f'R² = {r2:.4f}\\nRMSE = {rmse:.4f}\\nSlope = {slope:.4f}'\n",
    "        plt.text(0.05, 0.95, stats_text, transform=plt.gca().transAxes, fontsize=12,\n",
    "                 verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', fc='lightblue', alpha=0.5))\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.savefig(file_path, format='png', dpi=200, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao gerar gráfico de validação para {modelo} com {filtro}: {e}\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a985bc2b",
   "metadata": {},
   "source": [
    "## 5. Remoção de Outliers Espectrais (PCA, T²/Q) nos Dados de Calibração"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cc30d7",
   "metadata": {},
   "source": [
    "Além dos outliers espectrais (em X), podemos ter outliers nos valores de referência (em y). Por exemplo, um valor de pH ou SST que é analiticamente improvável ou resultado de um erro de anotação.\n",
    "- **Método (IQR):** O método do Intervalo Interquartil (IQR) é uma forma estatística robusta de identificar esses pontos.\n",
    "    - `IQR = Q3 (percentil 75) - Q1 (percentil 25)`\n",
    "    - Um valor é considerado outlier se estiver abaixo de `Q1 - 1.5 * IQR` ou acima de `Q3 + 1.5 * IQR`.\n",
    "- **Visualização:** O `boxplot` é a ferramenta visual perfeita para essa análise, pois ele desenha os \"bigodes\" exatamente nesses limites de 1.5 * IQR, mostrando os outliers como pontos individuais.\n",
    "- **Ação:** Novamente, a remoção é feita apenas no conjunto de calibração."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9493fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "# Remover outliers dos atributos usando boxplot e IQR\n",
    "dados_final = {}\n",
    "\n",
    "for atributo in atributos:\n",
    "    # Pega os dados já limpos de outliers espectrais\n",
    "    d = dados_pca[atributo]\n",
    "    X_cal = d['X_cal']\n",
    "    y_cal = d['y_cal']\n",
    "    indices_orig_cal = d['indices_orig_cal']\n",
    "    \n",
    "    # Boxplot (seu código de plotagem original pode ser mantido aqui)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x=y_cal)\n",
    "    sns.stripplot(x=y_cal, color='red', alpha=0.6)\n",
    "    plt.title(f'Boxplot com Dispersão - {atributo}')\n",
    "    \n",
    "    nome_arquivo = f'graficos_outliers_boxplot/boxplot_outliers_{atributo}.png'\n",
    "    plt.savefig(nome_arquivo, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    # Lógica de remoção de outliers\n",
    "    Q1 = np.percentile(y_cal, 25)\n",
    "    Q3 = np.percentile(y_cal, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    keep_mask_attr = (y_cal >= lower_bound) & (y_cal <= upper_bound)\n",
    "    \n",
    "    # Identifica e informa os outliers removidos\n",
    "    outliers_attr_indices = indices_orig_cal[~keep_mask_attr]\n",
    "    print(f'{atributo}: {len(outliers_attr_indices)} outliers de atributo removidos.')\n",
    "    if len(outliers_attr_indices) > 0:\n",
    "        print(f'  Índices Originais Removidos: {outliers_attr_indices}')\n",
    "    \n",
    "    # Armazena os dados finais e limpos para modelagem\n",
    "    dados_final[atributo] = {\n",
    "        'X_cal': X_cal[keep_mask_attr],\n",
    "        'y_cal': y_cal[keep_mask_attr],\n",
    "        'X_val': dados_divididos[atributo]['X_val'], # Validação original\n",
    "        'y_val': dados_divididos[atributo]['y_val']   # Validação original\n",
    "    }\n",
    "    \n",
    "    print(f'  Calibração final: {dados_final[atributo][\"X_cal\"].shape[0]} amostras')\n",
    "    print(f'  Validação: {dados_final[atributo][\"X_val\"].shape[0]} amostras')\n",
    "    print()\n",
    "\"\"\"\n",
    "\n",
    "dados_final = {}\n",
    "\n",
    "for atributo in atributos:\n",
    "    # Pega os dados já limpos de outliers espectrais\n",
    "    d = dados_pca[atributo]\n",
    "    X_clean = d['X']\n",
    "    y_clean = d['y']\n",
    "    indices_orig_clean = d['indices_orig']\n",
    "    \n",
    "    # Boxplot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x=y_clean)\n",
    "    sns.stripplot(x=y_clean, color='red', alpha=0.6)\n",
    "    plt.title(f'Boxplot com Dispersão - {atributo}')\n",
    "    \n",
    "    nome_arquivo = f'graficos_outliers_boxplot/boxplot_outliers_{atributo.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"/\", \"_\")}.png'\n",
    "    plt.savefig(nome_arquivo, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Lógica de remoção de outliers dos atributos\n",
    "    Q1 = np.percentile(y_clean, 25)\n",
    "    Q3 = np.percentile(y_clean, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    keep_mask_attr = (y_clean >= lower_bound) & (y_clean <= upper_bound)\n",
    "    \n",
    "    # Identifica e informa os outliers removidos\n",
    "    outliers_attr_indices = indices_orig_clean[~keep_mask_attr]\n",
    "    print(f'{atributo}: {len(outliers_attr_indices)} outliers de atributo removidos.')\n",
    "    if len(outliers_attr_indices) > 0:\n",
    "        print(f'  Índices Originais Removidos: {outliers_attr_indices}')\n",
    "    \n",
    "    # Armazena os dados finais e limpos para modelagem\n",
    "    dados_final[atributo] = {\n",
    "        'X': X_clean[keep_mask_attr],\n",
    "        'y': y_clean[keep_mask_attr]\n",
    "    }\n",
    "    \n",
    "    print(f'  Dataset final para modelagem: {dados_final[atributo][\"X\"].shape[0]} amostras')\n",
    "    print()\n",
    "\n",
    "print(\"Remoção de outliers concluída!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57596e5f",
   "metadata": {},
   "source": [
    "## 6. Avaliação dos Filtros e Seleção dos Melhores Modelos com Validação Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cb3076",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_calibration_plot_cv(y_true, y_pred_cv, atributo, filtro, modelo, file_path):\n",
    " \n",
    "    try:\n",
    "        # Garante que o diretório exista\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "        # Métricas para Predição da Validação Cruzada\n",
    "        slope_cv, offset_cv = np.polyfit(y_true, y_pred_cv, 1)\n",
    "        rmse_cv = np.sqrt(mean_squared_error(y_true, y_pred_cv))\n",
    "        r2_cv = r2_score(y_true, y_pred_cv)\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(y_true, y_pred_cv, color='blue', marker='o', alpha=0.7, label=f'Cross-Validation (R²={r2_cv:.3f})')\n",
    "        \n",
    "        # Linha ideal 1:1\n",
    "        min_val = min(min(y_true), min(y_pred_cv))\n",
    "        max_val = max(max(y_true), max(y_pred_cv))\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], 'k--', label='Linha Ideal (1:1)')\n",
    "\n",
    "        plt.xlabel(\"Valores Reais\")\n",
    "        plt.ylabel(\"Valores Preditos (CV)\")\n",
    "        plt.title(f'Cross-Validation: {atributo} | {modelo} | {filtro}')\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.legend(loc='lower right')\n",
    "        \n",
    "        # Adiciona texto com métricas\n",
    "        stats_text = f'CV - R²: {r2_cv:.3f}, RMSE: {rmse_cv:.3f}, Slope: {slope_cv:.3f}, Offset: {offset_cv:.3f}'\n",
    "        plt.text(0.05, 0.95, stats_text, transform=plt.gca().transAxes, fontsize=10,\n",
    "                 verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5))\n",
    "\n",
    "        plt.savefig(file_path, format='png', dpi=200, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao gerar gráfico de CV para {modelo} com {filtro}: {e}\")\n",
    "        plt.close()\n",
    "\n",
    "modelos = {\n",
    "   # 'PLSR': {\n",
    "   #     'estimador': PLSRegression(),\n",
    "    #    'params': {'n_components': [5, 10, 15, 20]}\n",
    "   # },\n",
    "   # 'PCR': {\n",
    "    #    'estimador': Pipeline([('pca', PCA()), ('regressor', LinearRegression())]),\n",
    "   #     'params': {'pca__n_components': [5, 10, 15, 20]}\n",
    "   # },\n",
    "     'RFR': {\n",
    "        'estimador': RandomForestRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [200, 300, 400], \n",
    "            'max_depth': [20, 30, None],\n",
    "            'min_samples_split': [2, 3], \n",
    "            'min_samples_leaf': [1, 2],\n",
    "            'bootstrap': [True]\n",
    "        } #60min raw\n",
    "    }, #ORIGINAL\n",
    "       # 'params': {\n",
    "        #    'n_estimators': [100, 150, 200, 250, 300],\n",
    "         #   'max_depth': [10, 20, 30, None],\n",
    "          #  'min_samples_split': [2, 5, 10],\n",
    "           # 'min_samples_leaf': [1, 2, 4],\n",
    "            #'warm_start': [True, False],\n",
    "            #'bootstrap': [True, False]\n",
    "        #}\n",
    "    \n",
    "    'SVMR': {\n",
    "        'estimador': SVR(),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
    "            'kernel': ['rbf', 'linear'],\n",
    "            'epsilon': [0.1, 0.2, 0.5, 1.0]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Combina todos os filtros\n",
    "todos_filtros = {**filtros_independentes, **filtros_dependentes}\n",
    "\n",
    "# Lista para armazenar todos os resultados\n",
    "lista_resultados_finais = []\n",
    "\n",
    "cv_folds = 10  # Número de folds para cross-validation\n",
    "kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "for atributo in [\"Firmness (N)\"]: #apenas para Firmness (N) e com RFR e SVMR grids menores\n",
    "    print(f'\\n{\"=\"*30}')\n",
    "    print(f'INICIANDO MODELAGEM PARA: {atributo}')\n",
    "    print(f'{\"=\"*30}')\n",
    "\n",
    "    melhor_r2_atributo = -999  # Inicia com um valor muito baixo\n",
    "    melhor_combinacao_info = {}  # Dicionário para guardar as infos do melhor\n",
    "    \n",
    "    # Dicionário para armazenar as 5 melhores combinações por modelo\n",
    "    melhores_por_modelo = {nome: [] for nome in modelos.keys()}\n",
    "\n",
    "    dados = dados_final[atributo]\n",
    "    X_full, y_full = dados['X'], dados['y']\n",
    "\n",
    "    for nome_modelo, info_modelo in modelos.items():\n",
    "        print(f'\\n--- Modelo: {nome_modelo} ---')\n",
    "        \n",
    "        combinacoes_deste_modelo = [] # Lista temporária para este modelo\n",
    "\n",
    "        for nome_filtro, funcao_filtro in todos_filtros.items():\n",
    "            start_time = time.time()\n",
    "            print(f'  Testando filtro: {nome_filtro}...', end='')\n",
    "\n",
    "            if nome_filtro in filtros_independentes:\n",
    "                X_filtered = funcao_filtro(X_full)\n",
    "                tipo_filtro = 'independente'\n",
    "            else:  # Filtro dependente de Y\n",
    "                X_filtered = funcao_filtro(X_full, y_full)\n",
    "                tipo_filtro = 'dependente'\n",
    "            \n",
    "            grid = GridSearchCV(\n",
    "                info_modelo['estimador'], \n",
    "                info_modelo['params'], \n",
    "                cv=kf,  # Usa o mesmo KFold para consistência\n",
    "                scoring='r2', \n",
    "                n_jobs=-1\n",
    "            )\n",
    "            grid.fit(X_filtered, y_full)\n",
    "            \n",
    "            melhor_modelo = grid.best_estimator_\n",
    "            \n",
    "            cv_scores = cross_val_score(melhor_modelo, X_filtered, y_full, cv=kf, scoring='r2')\n",
    "            cv_r2_mean = cv_scores.mean()\n",
    "            cv_r2_std = cv_scores.std()\n",
    "            \n",
    "            # Predições de cross-validation para plotagem\n",
    "            y_pred_cv = cross_val_predict(melhor_modelo, X_filtered, y_full, cv=kf)\n",
    "            cv_rmse = np.sqrt(mean_squared_error(y_full, y_pred_cv))\n",
    "            \n",
    "            end_time = time.time()\n",
    "            print(f' CV R²: {cv_r2_mean:.4f} (±{cv_r2_std:.4f}) | Concluído em {end_time - start_time:.2f}s')\n",
    "\n",
    "            # Salvar o resultado da combinação atual\n",
    "            resultados_atuais = {\n",
    "                'Atributo': atributo,\n",
    "                'Modelo': nome_modelo,\n",
    "                'Filtro': nome_filtro,\n",
    "                'Tipo_Filtro': tipo_filtro,\n",
    "                'CV_R2_Mean': cv_r2_mean,\n",
    "                'CV_R2_Std': cv_r2_std,\n",
    "                'CV_RMSE': cv_rmse,\n",
    "                'Grid_Best_Score': grid.best_score_,\n",
    "                'Melhores_Params': str(grid.best_params_),\n",
    "                'Tempo_s': end_time - start_time,\n",
    "                'objeto_modelo': melhor_modelo \n",
    "            }\n",
    "            \n",
    "            lista_resultados_finais.append(resultados_atuais)\n",
    "            combinacoes_deste_modelo.append(resultados_atuais)\n",
    "\n",
    "            # Lógica para salvar o melhor modelo geral\n",
    "            if cv_r2_mean > melhor_r2_atributo:\n",
    "                melhor_r2_atributo = cv_r2_mean\n",
    "                melhor_combinacao_info = {\n",
    "                    'objeto_modelo': melhor_modelo,\n",
    "                    'nome_modelo': nome_modelo,\n",
    "                    'nome_filtro': nome_filtro,\n",
    "                    'parametros': grid.best_params_\n",
    "                }\n",
    "            \n",
    "            path_cv = f'graficos_modelos/cross_validation/{atributo}_{nome_modelo}_{nome_filtro}.png'\n",
    "            os.makedirs('graficos_modelos/cross_validation', exist_ok=True)\n",
    "            save_calibration_plot_cv(y_full, y_pred_cv, atributo, nome_filtro, nome_modelo, path_cv)\n",
    "\n",
    "        # Ordenar e salvar as 5 melhores para este modelo\n",
    "        combinacoes_deste_modelo.sort(key=lambda x: x['CV_R2_Mean'], reverse=True)\n",
    "        melhores_por_modelo[nome_modelo] = combinacoes_deste_modelo[:5]\n",
    "\n",
    "    if melhor_combinacao_info:  # Garante que encontrou pelo menos um modelo\n",
    "        melhor_obj = melhor_combinacao_info['objeto_modelo']\n",
    "        nome_arquivo_modelo = f\"modelos_salvos/melhor_modelo_{atributo.replace(' ', '_')}.joblib\"\n",
    "        \n",
    "        joblib.dump(melhor_obj, nome_arquivo_modelo)\n",
    "        \n",
    "        print(f\"\\n MELHOR COMBINAÇÃO GERAL PARA '{atributo}' FOI SALVA!\")\n",
    "        print(f\"   - Arquivo: {nome_arquivo_modelo}\")\n",
    "        print(f\"   - Modelo: {melhor_combinacao_info['nome_modelo']}\")\n",
    "        print(f\"   - Filtro: {melhor_combinacao_info['nome_filtro']}\")\n",
    "        print(f\"   - Melhor R² CV: {melhor_r2_atributo:.4f}\")\n",
    "        print(f\"   - Melhores Parâmetros: {melhor_combinacao_info['parametros']}\")\n",
    "    \n",
    "    # Salvar os 5 melhores modelos para cada tipo\n",
    "    print(f\"\\n--- Salvando os 5 melhores modelos para {atributo} ---\")\n",
    "    for nome_modelo, lista_combinacoes in melhores_por_modelo.items():\n",
    "        for i, combinacao in enumerate(lista_combinacoes):\n",
    "            nome_arquivo = f\"modelos_salvos_top5/top_{i+1}_{atributo.replace(' ', '_')}_{combinacao['Modelo']}_{combinacao['Filtro']}.joblib\"\n",
    "            os.makedirs('modelos_salvos_top5', exist_ok=True)\n",
    "            joblib.dump(combinacao['objeto_modelo'], nome_arquivo)\n",
    "            print(f\"  - Top {i+1} do modelo {nome_modelo} salvo em: {nome_arquivo}\")\n",
    "\n",
    "print('\\n\\n Modelagem com Cross-Validation concluída!')\n",
    "\n",
    "df_completo = pd.DataFrame(lista_resultados_finais)\n",
    "df_completo_ordenado = df_completo.sort_values(by=['Atributo', 'CV_R2_Mean'], ascending=[True, False])\n",
    "df_completo_ordenado.to_excel('analise_completa_resultados_cv.xlsx', index=False)\n",
    "print(' Resultados salvos em \"analise_completa_resultados_cv.xlsx\"')\n",
    "\n",
    "print(\"\\n--- Melhores Resultados por Atributo (Cross-Validation) ---\")\n",
    "melhores_resultados = df_completo_ordenado.groupby('Atributo').first().reset_index()\n",
    "\n",
    "# Arredondar colunas numéricas para melhor visualização\n",
    "colunas_numericas_para_arredondar = ['CV_R2_Mean', 'CV_R2_Std', 'CV_RMSE', 'Grid_Best_Score']\n",
    "melhores_resultados[colunas_numericas_para_arredondar] = melhores_resultados[colunas_numericas_para_arredondar].round(4)\n",
    "\n",
    "# Selecionar e exibir as colunas mais importantes\n",
    "colunas_para_exibir = ['Atributo', 'Modelo', 'Filtro', 'CV_R2_Mean', 'CV_R2_Std', 'CV_RMSE', 'Melhores_Params']\n",
    "print(melhores_resultados[colunas_para_exibir].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1659ece7",
   "metadata": {},
   "source": [
    "## 7. Análise Comparativa dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7323c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Carregar os resultados salvos para análise\n",
    "df_analise = pd.read_excel('analise_completa_resultados_cv.xlsx')\n",
    "\n",
    "idx = df_analise.groupby(['Atributo', 'Modelo'])['CV_R2_Mean'].transform(max) == df_analise['CV_R2_Mean']\n",
    "df_melhores_por_modelo = df_analise[idx]\n",
    "\n",
    "print(\"--- Tabela Comparativa dos Melhores Resultados por Modelo (Cross-Validation) ---\")\n",
    "print(df_melhores_por_modelo[['Atributo', 'Modelo', 'Filtro', 'CV_R2_Mean', 'CV_R2_Std', 'CV_RMSE']].round(4))\n",
    "\n",
    "print(\"\\n--- Melhores Modelos Gerais por Atributo (baseado no R² de Cross-Validation) ---\")\n",
    "idx_geral = df_melhores_por_modelo.groupby(['Atributo'])['CV_R2_Mean'].transform(max) == df_melhores_por_modelo['CV_R2_Mean']\n",
    "df_melhores_gerais = df_melhores_por_modelo[idx_geral]\n",
    "print(df_melhores_gerais[['Atributo', 'Modelo', 'Filtro', 'CV_R2_Mean', 'CV_R2_Std', 'CV_RMSE']].round(4))\n",
    "\n",
    "# Análise estatística adicional\n",
    "print(\"\\n--- Análise Estatística dos Resultados ---\")\n",
    "print(f\"Número total de combinações testadas: {len(df_analise)}\")\n",
    "print(f\"Número de atributos analisados: {df_analise['Atributo'].nunique()}\")\n",
    "print(f\"Número de modelos testados: {df_analise['Modelo'].nunique()}\")\n",
    "print(f\"Número de filtros testados: {df_analise['Filtro'].nunique()}\")\n",
    "\n",
    "print(f\"\\nEstatísticas do R² de Cross-Validation:\")\n",
    "print(f\"  Média geral: {df_analise['CV_R2_Mean'].mean():.4f}\")\n",
    "print(f\"  Desvio padrão: {df_analise['CV_R2_Mean'].std():.4f}\")\n",
    "print(f\"  Melhor resultado: {df_analise['CV_R2_Mean'].max():.4f}\")\n",
    "print(f\"  Pior resultado: {df_analise['CV_R2_Mean'].min():.4f}\")\n",
    "\n",
    "# Análise por modelo\n",
    "print(f\"\\n--- Desempenho Médio por Modelo ---\")\n",
    "desempenho_por_modelo = df_analise.groupby('Modelo')['CV_R2_Mean'].agg(['mean', 'std', 'max', 'min']).round(4)\n",
    "print(desempenho_por_modelo)\n",
    "\n",
    "# Análise por filtro\n",
    "print(f\"\\n--- Top 10 Filtros com Melhor Desempenho Médio ---\")\n",
    "desempenho_por_filtro = df_analise.groupby('Filtro')['CV_R2_Mean'].agg(['mean', 'std', 'count']).round(4)\n",
    "desempenho_por_filtro = desempenho_por_filtro.sort_values('mean', ascending=False)\n",
    "print(desempenho_por_filtro.head(10))\n",
    "\n",
    "print(\"\\n✅ Análise de resultados concluída!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f32e04",
   "metadata": {},
   "source": [
    "## Testes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f27fb9a",
   "metadata": {},
   "source": [
    "# An√°lise Espectrosc√≥pica\n",
    "\n",
    "Este notebook segue as melhores pr√°ticas da literatura para an√°lise espectrosc√≥pica robusta, incluindo:\n",
    "- Cross Validation\n",
    "- Remo√ß√£o de outliers espectrais (PCA, T¬≤/Q)\n",
    "- Remo√ß√£o de outliers dos atributos (boxplot + seaborn)\n",
    "- Teste de todos os filtros (dependentes e independentes de y)\n",
    "- Modelagem com PLSR, PCR, RFR, SVMR (hiperpar√¢metros da literatura)\n",
    "- Plotagens robustas e salvamento de m√©tricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43087f04",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260fbaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necess√°rios\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_predict, cross_val_score\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from matplotlib.mlab import detrend\n",
    "import pywt\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Cria diret√≥rios para organizar todas as imagens salvas\n",
    "print(\"Criando diret√≥rios para salvar os gr√°ficos...\")\n",
    "os.makedirs('graficos_outliers_pca', exist_ok=True)\n",
    "os.makedirs('graficos_outliers_boxplot', exist_ok=True)\n",
    "os.makedirs('modelos_salvos', exist_ok=True)\n",
    "os.makedirs('graficos_modelos', exist_ok=True) \n",
    "os.makedirs('graficos_mlpr', exist_ok=True)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configura√ß√µes de plot\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db46c4e",
   "metadata": {},
   "source": [
    "## 2. Carregamento dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19985b8b",
   "metadata": {},
   "source": [
    "O primeiro passo em qualquer projeto de ci√™ncia de dados √© carregar e entender os dados. Nesta se√ß√£o, realizamos as seguintes a√ß√µes:\n",
    "- **Carregamento:** Utilizamos a biblioteca `pandas` para ler o arquivo `Simulacao_Exportacao_Keitt_e_Kent_Fieldspec.xlsx`, que cont√©m os dados espectrais e os valores de refer√™ncia (atributos f√≠sico-qu√≠micos).\n",
    "- **Separa√ß√£o de Dados:** A fun√ß√£o `load_data` inteligentemente separa o arquivo em duas partes:\n",
    "    - `metadata`: Um DataFrame contendo as informa√ß√µes de refer√™ncia (ex: Firmness, Dry Mass, TSS, TA, AA), que ser√£o nossas vari√°veis-alvo (y).\n",
    "    - `wavelengths`: Um DataFrame contendo a resposta espectral (absorb√¢ncia ou reflect√¢ncia) em cada comprimento de onda, que ser√£o nossas vari√°veis preditoras (X).\n",
    "- **Defini√ß√£o de Vari√°veis:** Convertemos os dados espectrais para um array NumPy (`X`) para otimizar os c√°lculos e definimos a lista de `atributos` que desejamos modelar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd3a7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para carregar dados\n",
    "def load_data(filepath):\n",
    "    \"\"\"Carrega dados espectrais e separa metadados de espectros.\"\"\"\n",
    "    df = pd.read_excel(filepath, engine='openpyxl')\n",
    "    \n",
    "    # Identificar colunas que s√£o comprimentos de onda (num√©ricas)\n",
    "    numeric_cols = []\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            float(col)\n",
    "            numeric_cols.append(col)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    # Separar metadados e comprimentos de onda\n",
    "    metadata = df.drop(columns=numeric_cols)\n",
    "    wavelengths = df[numeric_cols]\n",
    "    \n",
    "    return metadata, wavelengths\n",
    "\n",
    "# Carregar dados\n",
    "filepath = 'Data/raw/Fruto/Simulacao_Exportacao_Keitt_e_Kent_Fieldspec.xlsx'\n",
    "metadata, wavelengths = load_data(filepath)\n",
    "X = wavelengths.values\n",
    "wavelength_values = wavelengths.columns.astype(float)\n",
    "\n",
    "# VARI√ÅVEIS DA SARAH\n",
    "atributos = ['Firmness (N)', 'Dry Mass (%)', 'TSS (Brix)', 'TA (g/mL)', 'AA (mg/100g)']\n",
    "\n",
    "print(f'Dados carregados: {X.shape[0]} amostras, {X.shape[1]} comprimentos de onda')\n",
    "print(f'Faixa espectral: {wavelength_values.min():.1f} - {wavelength_values.max():.1f} nm')\n",
    "print(f'Atributos dispon√≠veis: {list(metadata.columns)}')\n",
    "print(f'Atributos a analisar: {atributos}')\n",
    "\n",
    "# remo√ß√£o de NaN antes de passar pelo pr√©-processamento e confer√™ncia\n",
    "\n",
    "dados_completos = {}\n",
    "for atributo in atributos:\n",
    "    y = metadata[atributo].values\n",
    "    original_indices = np.arange(len(y))\n",
    "    \n",
    "    # Remove apenas valores NaN\n",
    "    mask = ~np.isnan(y)\n",
    "    X_clean = X[mask]\n",
    "    y_clean = y[mask]\n",
    "    original_indices_clean = original_indices[mask]\n",
    "    \n",
    "    dados_completos[atributo] = {\n",
    "        'X': X_clean,\n",
    "        'y': y_clean,\n",
    "        'indices_orig': original_indices_clean\n",
    "    }\n",
    "    \n",
    "    print(f'{atributo}: {X_clean.shape[0]} amostras v√°lidas')\n",
    "\n",
    "print(f'\\nPrepara√ß√£o dos dados conclu√≠da para {len(atributos)} atributos!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be6893f",
   "metadata": {},
   "source": [
    "## 3. Filtros de Pr√©-processamento (Independentes e Dependentes de y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a8977",
   "metadata": {},
   "source": [
    "Os dados espectrais brutos cont√™m diversas fontes de varia√ß√£o indesejada que podem mascarar a informa√ß√£o qu√≠mica relevante. Essas varia√ß√µes podem ser causadas por espalhamento de luz (efeitos f√≠sicos da amostra), mudan√ßas na linha de base (drift do instrumento) e ru√≠do. O pr√©-processing visa minimizar esses efeitos para que o modelo foque na correla√ß√£o entre o espectro e o atributo de interesse.\n",
    "\n",
    "Neste notebook, testamos uma ampla gama de filtros, divididos em duas categorias:\n",
    "\n",
    "### Filtros Independentes de y\n",
    "\n",
    "S√£o aplicados apenas aos espectros (`X`) e n√£o utilizam a informa√ß√£o da vari√°vel alvo (`y`).\n",
    "\n",
    "-   **`Raw`**: Utiliza os dados espectrais brutos, sem nenhum tratamento. Serve como uma linha de base para comparar a efic√°cia dos outros filtros.\n",
    "-   **`MSC` (Multiplicative Scatter Correction)**: Corrige o espalhamento de luz (aditivo e multiplicativo) causado por varia√ß√µes no tamanho de part√≠cula e compacta√ß√£o da amostra. Ele ajusta cada espectro para se parecer mais com um espectro \"ideal\" (geralmente a m√©dia de todos os espectros).\n",
    "-   **`SNV` (Standard Normal Variate)**: Alternativa ao MSC que tamb√©m corrige o espalhamento de luz. A diferen√ßa √© que o SNV padroniza cada espectro individualmente (subtrai a m√©dia e divide pelo desvio padr√£o daquele espectro), sem usar um espectro de refer√™ncia.\n",
    "-   **`SG_D1` e `SG_D2` (Savitzky-Golay Derivatives)**: Calcula a primeira ou a segunda derivada do espectro. Derivadas s√£o excelentes para remover desvios de linha de base (efeitos aditivos) e para resolver picos espectrais sobrepostos, real√ßando a informa√ß√£o de bandas de absor√ß√£o espec√≠ficas.\n",
    "-   **`Detrend`**: Remove tend√™ncias lineares ou polinomiais da linha de base do espectro. √â muito eficaz para corrigir \"inclina√ß√µes\" no espectro causadas por drift do instrumento.\n",
    "-   **`Normalize`**: Realiza uma normaliza√ß√£o Min-Max, escalonando a intensidade de cada espectro para um intervalo fixo (geralmente [0, 1]). Ajuda a corrigir varia√ß√µes de intensidade causadas por diferen√ßas na dist√¢ncia da amostra ou na pot√™ncia da fonte de luz.\n",
    "-   **`EMSC` (Extended Multiplicative Signal Correction)**: Uma vers√£o avan√ßada do MSC. Al√©m de corrigir os efeitos de espalhamento, o EMSC pode incluir termos polinomiais para modelar e remover efeitos de linha de base mais complexos e n√£o-lineares.\n",
    "-   **`Continuum Removal`**: T√©cnica que normaliza os espectros para que as bandas de absor√ß√£o possam ser comparadas em termos de sua profundidade, e n√£o de sua intensidade absoluta. Ele ajusta um \"envelope\" (casco convexo) sobre o espectro e divide o espectro original por este envelope, real√ßando as caracter√≠sticas de absor√ß√£o.\n",
    "-   **`Wavelet_Denoising`**: Utiliza a Transformada Wavelet para decompor o espectro em diferentes n√≠veis de frequ√™ncia. A t√©cnica permite remover o ru√≠do (geralmente presente em altas frequ√™ncias) de forma muito eficaz, preservando as principais caracter√≠sticas do sinal espectral.\n",
    "\n",
    "### Filtros Dependentes de y e Combina√ß√µes\n",
    "\n",
    "Estes filtros utilizam a vari√°vel alvo (`y`) para otimizar a remo√ß√£o de varia√ß√£o n√£o correlacionada em `X`, ou s√£o combina√ß√µes sequenciais de m√∫ltiplos filtros para um tratamento mais completo.\n",
    "\n",
    "-   **`OSC_1` e `OSC_2` (Orthogonal Signal Correction)**: Filtro que remove componentes (1 ou 2, neste caso) dos espectros `X` que s√£o ortogonais (n√£o correlacionados) √† vari√°vel alvo `y`. O objetivo √© limpar `X` da varia√ß√£o que n√£o ajuda na predi√ß√£o, potencialmente melhorando o modelo subsequente. Esta t√©cnica √© frequentemente referida como um pr√©-processamento **OPLS** (Orthogonal Projections to Latent Structures).\n",
    "-   **`MSC_SG_OSC`**: Uma **cadeia de pr√©-processamentos** aplicada na seguinte ordem:\n",
    "    1.  `MSC` para corrigir o espalhamento.\n",
    "    2.  `Savitzky-Golay (1¬™ derivada)` para corrigir a linha de base.\n",
    "    3.  `OSC (1 componente)` para remover varia√ß√£o n√£o correlacionada com `y`.\n",
    "-   **`OPLS1_SNV_SG_D1` e `OPLS2_SNV_SG_D1`**: Outra cadeia de processamento:\n",
    "    1.  `SNV` para corre√ß√£o de espalhamento.\n",
    "    2.  `Savitzky-Golay (1¬™ derivada)`.\n",
    "    3.  `OPLS/OSC` para remover 1 ou 2 componentes ortogonais a `y`.\n",
    "-   **`SNV_Detrend_SG_D1`**: Uma combina√ß√£o de filtros independentes de `y`, mas que, por sua complexidade, √© testada junto √†s outras cadeias:\n",
    "    1.  `SNV`.\n",
    "    2.  `Detrend` para remo√ß√£o de tend√™ncia.\n",
    "    3.  `Savitzky-Golay (1¬™ derivada)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80ad97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementa√ß√£o dos filtros de pr√©-processamento\n",
    "# Filtros independentes de y\n",
    "\n",
    "def msc(X):\n",
    "    \"\"\"Multiplicative Scatter Correction.\"\"\"\n",
    "    X = np.asarray(X)\n",
    "    mean_spectrum = np.mean(X, axis=0)\n",
    "    corrected_spectra = np.zeros_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        slope, intercept = np.polyfit(mean_spectrum, X[i, :], 1)\n",
    "        corrected_spectra[i, :] = (X[i, :] - intercept) / slope\n",
    "    return corrected_spectra\n",
    "\n",
    "def snv(X):\n",
    "    \"\"\"Standard Normal Variate.\"\"\"\n",
    "    X = np.asarray(X)\n",
    "    return (X - np.mean(X, axis=1, keepdims=True)) / np.std(X, axis=1, keepdims=True)\n",
    "\n",
    "def savitzky_golay(X, window_size=11, poly_order=2, deriv_order=1):\n",
    "    \"\"\"Savitzky-Golay filter.\"\"\"\n",
    "    return savgol_filter(X, window_length=window_size, polyorder=poly_order, deriv=deriv_order, axis=1)\n",
    "\n",
    "def detrend_filter(X):\n",
    "    \"\"\"Detrending filter.\"\"\"\n",
    "    return detrend(X, axis=1)\n",
    "\n",
    "def normalize(X):\n",
    "    \"\"\"Normaliza√ß√£o Min-Max.\"\"\"\n",
    "    return (X - np.min(X, axis=1, keepdims=True)) / (np.max(X, axis=1, keepdims=True) - np.min(X, axis=1, keepdims=True))\n",
    "\n",
    "def emsc(X, reference=None):\n",
    "    \"\"\"Extended Multiplicative Signal Correction.\"\"\"\n",
    "    X = np.asarray(X)\n",
    "    if reference is None:\n",
    "        reference = np.mean(X, axis=0)  # Usa o espectro m√©dio como refer√™ncia\n",
    "    \n",
    "    X_corr = np.zeros_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        # Modelo: X[i] ‚âà a + b*reference\n",
    "        model = np.vstack([np.ones_like(reference), reference]).T\n",
    "        params, _, _, _ = np.linalg.lstsq(model, X[i, :], rcond=None)\n",
    "        a, b = params[0], params[1]\n",
    "        X_corr[i,:] = (X[i, :] - a) / b\n",
    "    return X_corr\n",
    "\n",
    "def continuum_removal(X, wavelengths):\n",
    "    \"\"\"Continuum Removal.\"\"\"\n",
    "    X = np.asarray(X)\n",
    "    X_cr = np.zeros_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        spectrum = X[i, :]\n",
    "        # Encontra os pontos do casco convexo superior\n",
    "        q_u = [0]\n",
    "        for k in range(1, len(wavelengths) - 1):\n",
    "            s_k = (spectrum[len(wavelengths)-1] - spectrum[0]) / (wavelengths[-1] - wavelengths[0])\n",
    "            s_q = (spectrum[k] - spectrum[q_u[-1]]) / (wavelengths[k] - wavelengths[q_u[-1]])\n",
    "            if s_q > s_k:\n",
    "                q_u.append(k)\n",
    "        q_u.append(len(wavelengths)-1)\n",
    "        \n",
    "        # Interpola√ß√£o linear entre os pontos do casco\n",
    "        continuum = np.interp(wavelengths, wavelengths[q_u], spectrum[q_u])\n",
    "        X_cr[i, :] = spectrum / continuum\n",
    "    return X_cr\n",
    "\n",
    "def wavelet_denoising(X, wavelet='db4', level=4):\n",
    "    \"\"\"Wavelet Transform para Denoising.\"\"\"\n",
    "    X = np.asarray(X)\n",
    "    original_length = X.shape[1]\n",
    "    denoised_list = []\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        # 1. Decomposi√ß√£o Wavelet\n",
    "        coeffs = pywt.wavedec(X[i, :], wavelet, level=level)\n",
    "\n",
    "        # 2. C√°lculo do limiar (threshold)\n",
    "        sigma = np.median(np.abs(coeffs[-1])) / 0.6745\n",
    "        threshold = sigma * np.sqrt(2 * np.log(original_length))\n",
    "\n",
    "        # 3. Aplica√ß√£o do filtro (soft thresholding) nos coeficientes de detalhe\n",
    "        coeffs[1:] = [pywt.threshold(c, value=threshold, mode='soft') for c in coeffs[1:]]\n",
    "\n",
    "        # 4. Reconstru√ß√£o do sinal\n",
    "        reconstructed_signal = pywt.waverec(coeffs, wavelet)\n",
    "\n",
    "        # 5. Ajuste do tamanho\n",
    "        denoised_list.append(reconstructed_signal[:original_length])\n",
    "\n",
    "    return np.asarray(denoised_list)\n",
    "\n",
    "# Filtros dependentes de y (Orthogonal Signal Correction)\n",
    "class OrthogonalCorrection:\n",
    "    \"\"\"Orthogonal Signal Correction (OSC).\"\"\"\n",
    "    def __init__(self, n_components=1):\n",
    "        self.n_components = n_components\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        X, y = np.asarray(X), np.asarray(y).ravel()\n",
    "        self.w_ortho_ = []\n",
    "        self.p_ortho_ = []\n",
    "        self.X_corr_ = X.copy()\n",
    "        \n",
    "        for _ in range(self.n_components):\n",
    "            pls = PLSRegression(n_components=1)\n",
    "            pls.fit(self.X_corr_, y)\n",
    "            t = pls.x_scores_\n",
    "            w = pls.x_weights_\n",
    "            p = pls.x_loadings_\n",
    "            \n",
    "            # Componente Ortogonal\n",
    "            w_ortho = p - (np.dot(w.T, p) / np.dot(w.T, w)) * w\n",
    "            t_ortho = np.dot(self.X_corr_, w_ortho)\n",
    "            p_ortho = np.dot(t_ortho.T, self.X_corr_) / np.dot(t_ortho.T, t_ortho)\n",
    "            \n",
    "            # Remover varia√ß√£o ortogonal\n",
    "            self.X_corr_ -= np.dot(t_ortho, p_ortho)\n",
    "            self.w_ortho_.append(w_ortho)\n",
    "            self.p_ortho_.append(p_ortho)\n",
    "        \n",
    "        return self.X_corr_\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_res = np.asarray(X).copy()\n",
    "        for i in range(self.n_components):\n",
    "            t_ortho = np.dot(X_res, self.w_ortho_[i])\n",
    "            X_res -= np.dot(t_ortho, self.p_ortho_[i])\n",
    "        return X_res\n",
    "\n",
    "# Dicion√°rio de filtros independentes de y\n",
    "filtros_independentes = {\n",
    "    'Raw': lambda X: X,\n",
    "    'MSC': msc,\n",
    "    'SNV': snv,\n",
    "    'SG_D1': lambda X: savitzky_golay(X, window_size=11, poly_order=2, deriv_order=1),\n",
    "    'SG_D2': lambda X: savitzky_golay(X, window_size=11, poly_order=2, deriv_order=2),\n",
    "    'Detrend': detrend_filter,\n",
    "    'Normalize': normalize,\n",
    "    'EMSC': emsc,\n",
    "    'Continuum_Removal': lambda X: continuum_removal(X, wavelength_values),\n",
    "    'Wavelet_Denoising': wavelet_denoising\n",
    "}\n",
    "\n",
    "# Dicion√°rio de filtros dependentes de y\n",
    "filtros_dependentes = {\n",
    "    'OSC_1': lambda X, y: OrthogonalCorrection(n_components=1).fit_transform(X, y),\n",
    "    'OSC_2': lambda X, y: OrthogonalCorrection(n_components=2).fit_transform(X, y),\n",
    "    'MSC_SG_OSC': lambda X, y: OrthogonalCorrection(n_components=1).fit_transform(\n",
    "        savitzky_golay(msc(X), window_size=11, poly_order=2, deriv_order=1), y),\n",
    "    'OPLS1_SNV_SG_D1': lambda X, y: OrthogonalCorrection(n_components=1).fit_transform(\n",
    "        savitzky_golay(snv(X), window_size=11, poly_order=2, deriv_order=1), y),\n",
    "    'OPLS2_SNV_SG_D1': lambda X, y: OrthogonalCorrection(n_components=2).fit_transform(\n",
    "        savitzky_golay(snv(X), window_size=11, poly_order=2, deriv_order=1), y),\n",
    "    'SNV_Detrend_SG_D1': lambda X, y: savitzky_golay(detrend_filter(snv(X)), window_size=11, poly_order=2, deriv_order=1)\n",
    "}\n",
    "\n",
    "print(f'Filtros independentes de y: {list(filtros_independentes.keys())}')\n",
    "print(f'Filtros dependentes de y: {list(filtros_dependentes.keys())}')\n",
    "print(f'Total de filtros: {len(filtros_independentes) + len(filtros_dependentes)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f737e308",
   "metadata": {},
   "source": [
    "## 4. Remo√ß√£o de Outliers Espectrais (PCA, T¬≤/Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8309e1a7",
   "metadata": {},
   "source": [
    "Outliers espectrais s√£o amostras cujo espectro √© muito diferente do resto da popula√ß√£o, podendo ser causados por erros de medi√ß√£o, contamina√ß√£o ou caracter√≠sticas √∫nicas da amostra. Eles podem prejudicar significativamente a capacidade de generaliza√ß√£o do modelo.\n",
    "- **M√©todo PCA (T¬≤/Q):** Usamos a An√°lise de Componentes Principais (PCA) para identificar esses outliers.\n",
    "    - **Estat√≠stica T¬≤ (Hotelling's T¬≤):** Mede a varia√ß√£o de uma amostra *dentro* do modelo PCA. Valores altos indicam que a amostra √© um outlier na combina√ß√£o das vari√°veis principais.\n",
    "    - **Estat√≠stica Q (Res√≠duos):** Mede a varia√ß√£o da amostra *fora* do modelo PCA (o que o modelo n√£o conseguiu capturar). Valores altos indicam que a estrutura do espectro da amostra √© anormal.\n",
    "- **L√≥gica:** Uma amostra √© considerada outlier se seu valor de T¬≤ ou Q ultrapassa um limite de confian√ßa (geralmente 3 desvios padr√£o da m√©dia).\n",
    "- **Importante:** A remo√ß√£o de outliers √© feita **apenas no conjunto de calibra√ß√£o**. O conjunto de valida√ß√£o deve permanecer intacto para simular dados \"reais\" que o modelo encontrar√° no futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f251846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_outlier_removal(X, n_components=10, threshold=3):\n",
    "    \"\"\"Remove outliers usando PCA com estat√≠sticas T¬≤ e Q.\"\"\"\n",
    "    # Padronizar dados\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Aplicar PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Calcular estat√≠stica T¬≤ (Hotelling's T¬≤)\n",
    "    T2 = np.sum((X_pca / np.std(X_pca, axis=0)) ** 2, axis=1)\n",
    "    \n",
    "    # Calcular estat√≠stica Q (res√≠duos)\n",
    "    X_reconstructed = pca.inverse_transform(X_pca)\n",
    "    Q = np.sum((X_scaled - X_reconstructed) ** 2, axis=1)\n",
    "    \n",
    "    # Definir limites (m√©dia + threshold * desvio padr√£o)\n",
    "    T2_limit = np.mean(T2) + threshold * np.std(T2)\n",
    "    Q_limit = np.mean(Q) + threshold * np.std(Q)\n",
    "    \n",
    "    # Identificar outliers\n",
    "    outliers_mask = (T2 > T2_limit) | (Q > Q_limit)\n",
    "    \n",
    "    # Retornamos a m√°scara de quem N√ÉO √© outlier e o modelo PCA treinado\n",
    "    return ~outliers_mask, pca, T2, Q, T2_limit, Q_limit\n",
    "\n",
    "dados_pca = {}\n",
    "\n",
    "#para todo o conjunto de dado e n√£o separado em cal e val\n",
    "for atributo in atributos:\n",
    "    X_full = dados_completos[atributo]['X']\n",
    "    y_full = dados_completos[atributo]['y']\n",
    "    indices_orig_full = dados_completos[atributo]['indices_orig']\n",
    "    \n",
    "    # Chama a fun√ß√£o de remo√ß√£o de outliers espectrais\n",
    "    keep_mask, pca_model, T2, Q, T2_limit, Q_limit = pca_outlier_removal(X_full)\n",
    "    \n",
    "    # Identifica e informa os outliers removidos\n",
    "    outliers_indices = indices_orig_full[~keep_mask]\n",
    "    print(f'{atributo}: {len(outliers_indices)} outliers espectrais removidos.')\n",
    "    if len(outliers_indices) > 0:\n",
    "        print(f'  √çndices Originais Removidos: {outliers_indices}')\n",
    "    \n",
    "    # Filtra os dados\n",
    "    X_clean = X_full[keep_mask]\n",
    "    y_clean = y_full[keep_mask]\n",
    "    indices_orig_clean = indices_orig_full[keep_mask]\n",
    "    \n",
    "    dados_pca[atributo] = {\n",
    "        'X': X_clean,\n",
    "        'y': y_clean,\n",
    "        'indices_orig': indices_orig_clean,\n",
    "        'pca_model': pca_model,\n",
    "        'T2': T2,\n",
    "        'Q': Q,\n",
    "        'T2_limit': T2_limit,\n",
    "        'Q_limit': Q_limit,\n",
    "        'keep_mask': keep_mask\n",
    "    }\n",
    "    \n",
    "    print(f'  Dataset final: {X_clean.shape[0]} amostras')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c273ba",
   "metadata": {},
   "source": [
    "### 4.1 Gr√°ficos de Outliers T¬≤ e Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f96f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotagem dos gr√°ficos de outliers PCA\n",
    "for atributo in atributos:\n",
    "    d = dados_pca[atributo]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Gr√°fico 1: Vari√¢ncia explicada cumulativa\n",
    "    # Acessa o modelo PCA com a chave correta 'pca_model'\n",
    "\n",
    "    cumulative_variance = np.cumsum(d['pca_model'].explained_variance_ratio_)\n",
    "    axes[0].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'o-', linewidth=2, markersize=6)\n",
    "    axes[0].set_xlabel('N√∫mero de Componentes Principais')\n",
    "    axes[0].set_ylabel('Vari√¢ncia Explicada Cumulativa')\n",
    "    axes[0].set_title(f'Vari√¢ncia Explicada - {atributo}')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].axhline(y=0.95, color='r', linestyle='--', alpha=0.7, label='95%')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Gr√°fico 2: T¬≤ vs Q (detec√ß√£o de outliers)\n",
    "    # Acessa as vari√°veis T2, Q, T2_limit, Q_limit e keep_mask com as chaves corretas\n",
    "    scatter = axes[1].scatter(d['T2'], d['Q'], c=d['keep_mask'], cmap='coolwarm', \n",
    "                              edgecolor='k', alpha=0.7, s=50)\n",
    "    axes[1].axhline(d['Q_limit'], color='r', linestyle='--', linewidth=2, label=f'Q Limit: {d[\"Q_limit\"]:.2f}')\n",
    "    axes[1].axvline(d['T2_limit'], color='g', linestyle='--', linewidth=2, label=f'T¬≤ Limit: {d[\"T2_limit\"]:.2f}')\n",
    "    axes[1].set_xlabel(\"Hotelling's T¬≤\")\n",
    "    axes[1].set_ylabel('Q Residual')\n",
    "    axes[1].set_title(f'Detec√ß√£o de Outliers - {atributo}')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Adicionar legenda de cores\n",
    "    legend1 = axes[1].legend(*scatter.legend_elements(), title=\"Status\")\n",
    "    axes[1].add_artist(legend1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Salvar gr√°fico\n",
    "    nome_arquivo = f'graficos_outliers_pca/sim-kk-field-pca_outliers_{atributo.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"/\", \"_\")}.png'\n",
    "    plt.savefig(nome_arquivo, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'Gr√°ficos de outliers PCA para {atributo} salvos em: {nome_arquivo}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a985bc2b",
   "metadata": {},
   "source": [
    "## 5. Remo√ß√£o de Outliers dos Atributos (Boxplot + Seaborn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cc30d7",
   "metadata": {},
   "source": [
    "Al√©m dos outliers espectrais (em X), podemos ter outliers nos valores de refer√™ncia (em y). Por exemplo, um valor de pH ou SST que √© analiticamente improv√°vel ou resultado de um erro de anota√ß√£o.\n",
    "- **M√©todo (IQR):** O m√©todo do Intervalo Interquartil (IQR) √© uma forma estat√≠stica robusta de identificar esses pontos.\n",
    "    - `IQR = Q3 (percentil 75) - Q1 (percentil 25)`\n",
    "    - Um valor √© considerado outlier se estiver abaixo de `Q1 - 1.5 * IQR` ou acima de `Q3 + 1.5 * IQR`.\n",
    "- **Visualiza√ß√£o:** O `boxplot` √© a ferramenta visual perfeita para essa an√°lise, pois ele desenha os \"bigodes\" exatamente nesses limites de 1.5 * IQR, mostrando os outliers como pontos individuais.\n",
    "- **A√ß√£o:** Novamente, a remo√ß√£o √© feita apenas no conjunto de calibra√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9493fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover outliers dos atributos usando boxplot e IQR\n",
    "\n",
    "dados_final = {}\n",
    "\n",
    "for atributo in atributos:\n",
    "    # Pega os dados j√° limpos de outliers espectrais\n",
    "    d = dados_pca[atributo]\n",
    "    X_clean = d['X']\n",
    "    y_clean = d['y']\n",
    "    indices_orig_clean = d['indices_orig']\n",
    "    \n",
    "    # Boxplot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x=y_clean)\n",
    "    sns.stripplot(x=y_clean, color='red', alpha=0.6)\n",
    "    plt.title(f'Boxplot com Dispers√£o - {atributo}')\n",
    "    \n",
    "    nome_arquivo = f'graficos_outliers_boxplot/sim-kk-field-boxplot_outliers_{atributo.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"/\", \"_\")}.png'\n",
    "    plt.savefig(nome_arquivo, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # L√≥gica de remo√ß√£o de outliers dos atributos\n",
    "    Q1 = np.percentile(y_clean, 25)\n",
    "    Q3 = np.percentile(y_clean, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    keep_mask_attr = (y_clean >= lower_bound) & (y_clean <= upper_bound)\n",
    "    \n",
    "    # Identifica e informa os outliers removidos\n",
    "    outliers_attr_indices = indices_orig_clean[~keep_mask_attr]\n",
    "    print(f'{atributo}: {len(outliers_attr_indices)} outliers de atributo removidos.')\n",
    "    if len(outliers_attr_indices) > 0:\n",
    "        print(f'  √çndices Originais Removidos: {outliers_attr_indices}')\n",
    "    \n",
    "    # Armazena os dados finais e limpos para modelagem\n",
    "    dados_final[atributo] = {\n",
    "        'X': X_clean[keep_mask_attr],\n",
    "        'y': y_clean[keep_mask_attr]\n",
    "    }\n",
    "    \n",
    "    print(f'  Dataset final para modelagem: {dados_final[atributo][\"X\"].shape[0]} amostras')\n",
    "    print()\n",
    "\n",
    "print(\"Remo√ß√£o de outliers conclu√≠da!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447242ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fun√ß√£o para o Gr√°fico de CALIBRA√á√ÉO (Ref vs Predito vs CV) ---\n",
    "def save_calibration_plot(y_cal, y_pred_cal, y_pred_cv, atributo, filtro, modelo, file_path):\n",
    "    \"\"\"\n",
    "    Gera e salva um gr√°fico comparando predi√ß√µes de treino e de valida√ß√£o cruzada\n",
    "    no conjunto de calibra√ß√£o.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Garante que o diret√≥rio exista\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "        # M√©tricas para Predi√ß√£o no Treino Completo\n",
    "        slope_pred, offset_pred = np.polyfit(y_cal, y_pred_cal, 1)\n",
    "        rmse_pred = np.sqrt(mean_squared_error(y_cal, y_pred_cal))\n",
    "        r2_pred = r2_score(y_cal, y_pred_cal)\n",
    "\n",
    "        # M√©tricas para Predi√ß√£o da Valida√ß√£o Cruzada\n",
    "        slope_cv, offset_cv = np.polyfit(y_cal, y_pred_cv, 1)\n",
    "        rmse_cv = np.sqrt(mean_squared_error(y_cal, y_pred_cv))\n",
    "        r2_cv = r2_score(y_cal, y_pred_cv)\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(y_cal, y_pred_cal, color='blue', marker='o', alpha=0.6, label=f'Predi√ß√£o no Treino (R¬≤={r2_pred:.3f})')\n",
    "        plt.scatter(y_cal, y_pred_cv, color='red', marker='x', alpha=0.7, label=f'Predi√ß√£o CV (R¬≤={r2_cv:.3f})')\n",
    "        \n",
    "        # Linha ideal 1:1\n",
    "        min_val = min(min(y_cal), min(y_pred_cal), min(y_pred_cv))\n",
    "        max_val = max(max(y_cal), max(y_pred_cal), max(y_pred_cv))\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], 'k--', label='Linha Ideal (1:1)')\n",
    "\n",
    "        plt.xlabel(\"Valores Reais (Calibra√ß√£o)\")\n",
    "        plt.ylabel(\"Valores Preditos\")\n",
    "        plt.title(f'Desempenho na Calibra√ß√£o: {atributo} | {modelo} | {filtro}')\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.legend(loc='lower right')\n",
    "        \n",
    "        # Adiciona texto com m√©tricas\n",
    "        stats_text = (\n",
    "            f'Treino - RMSE: {rmse_pred:.3f}, Slope: {slope_pred:.3f}, Offset: {offset_pred:.3f}\\n'\n",
    "            f'CV     - RMSE: {rmse_cv:.3f}, Slope: {slope_cv:.3f}, Offset: {offset_cv:.3f}'\n",
    "        )\n",
    "        plt.text(0.05, 0.95, stats_text, transform=plt.gca().transAxes, fontsize=10,\n",
    "                 verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5))\n",
    "\n",
    "        plt.savefig(file_path, format='png', dpi=200, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao gerar gr√°fico de calibra√ß√£o para {modelo} com {filtro}: {e}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# --- Fun√ß√£o para o Gr√°fico de VALIDA√á√ÉO (Predito vs Real) ---\n",
    "def save_validation_plot(y_val, y_pred_val, atributo, filtro, modelo, file_path):\n",
    "    \"\"\"\n",
    "    Gera e salva o gr√°fico de predi√ß√µes no conjunto de valida√ß√£o (teste).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Garante que o diret√≥rio exista\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        \n",
    "        # M√©tricas\n",
    "        r2 = r2_score(y_val, y_pred_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
    "        slope, offset = np.polyfit(y_val, y_pred_val, 1)\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.scatter(y_val, y_pred_val, alpha=0.7, edgecolors='k', label='Dados de Valida√ß√£o')\n",
    "        \n",
    "        # Linha ideal 1:1\n",
    "        min_val = min(min(y_val), min(y_pred_val))\n",
    "        max_val = max(max(y_val), max(y_pred_val))\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Linha Ideal (1:1)')\n",
    "\n",
    "        plt.xlabel('Valores Reais (Valida√ß√£o)')\n",
    "        plt.ylabel('Valores Preditos')\n",
    "        plt.title(f'Desempenho na Valida√ß√£o: {atributo} | {modelo} | {filtro}')\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        stats_text = f'R¬≤ = {r2:.4f}\\nRMSE = {rmse:.4f}\\nSlope = {slope:.4f}'\n",
    "        plt.text(0.05, 0.95, stats_text, transform=plt.gca().transAxes, fontsize=12,\n",
    "                 verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', fc='lightblue', alpha=0.5))\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.savefig(file_path, format='png', dpi=200, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao gerar gr√°fico de valida√ß√£o para {modelo} com {filtro}: {e}\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57596e5f",
   "metadata": {},
   "source": [
    "## 6. Avalia√ß√£o dos Filtros e Sele√ß√£o dos Melhores Modelos com Valida√ß√£o Cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd77dd3f",
   "metadata": {},
   "source": [
    "Nesta etapa crucial, constru√≠mos e avaliamos os modelos de regress√£o. Em vez de testar cada modelo separadamente, criamos um fluxo automatizado e exaustivo para garantir que encontremos a melhor performance poss√≠vel.\n",
    "- **Estrat√©gia:**\n",
    "    1. **Itera√ß√£o por Atributo:** Um loop externo passa por cada vari√°vel alvo (AT, PH, etc.).\n",
    "    2. **Itera√ß√£o por Modelo:** Para cada atributo, testamos uma lista de modelos quimiom√©tricos:\n",
    "        - `PLSR`: Padr√£o da √°rea, lida bem com multicolinearidade.\n",
    "        - `PCR`: Alternativa que primeiro reduz a dimensionalidade.\n",
    "        - `RFR` (Random Forest): Modelo n√£o-linear, robusto e poderoso.\n",
    "        - `SVMR` (Support Vector Machine): Eficaz em espa√ßos de alta dimens√£o.\n",
    "    3. **Itera√ß√£o por Filtro:** Para cada modelo, aplicamos **todos os 16 filtros** de pr√©-processamento definidos na se√ß√£o anterior.\n",
    "    4. **Otimiza√ß√£o (GridSearchCV):** Para cada combina√ß√£o `(atributo, modelo, filtro)`, usamos `GridSearchCV` com valida√ß√£o cruzada (10-fold) para encontrar os melhores hiperpar√¢metros do modelo (ex: n√∫mero de componentes para PLS, par√¢metros C e gamma para SVM).\n",
    "- **Avalia√ß√£o:** O desempenho de cada combina√ß√£o √© medido pelo R¬≤ da valida√ß√£o cruzada. O melhor modelo √© ent√£o treinado com todos os dados de calibra√ß√£o e sua performance final √© avaliada no conjunto de valida√ß√£o, que foi mantido separado durante todo o processo.\n",
    "- **Sa√≠das:** Ao final, teremos os resultados de centenas de modelos, permitindo uma compara√ß√£o justa e a sele√ß√£o do campe√£o para cada atributo. Todos os gr√°ficos de predi√ß√£o e as m√©tricas s√£o salvos automaticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cb3076",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_calibration_plot_cv(y_true, y_pred_cv, atributo, filtro, modelo, file_path):\n",
    " \n",
    "    try:\n",
    "        # Garante que o diret√≥rio exista\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "        # M√©tricas para Predi√ß√£o da Valida√ß√£o Cruzada\n",
    "        slope_cv, offset_cv = np.polyfit(y_true, y_pred_cv, 1)\n",
    "        rmse_cv = np.sqrt(mean_squared_error(y_true, y_pred_cv))\n",
    "        r2_cv = r2_score(y_true, y_pred_cv)\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(y_true, y_pred_cv, color='blue', marker='o', alpha=0.7, label=f'Cross-Validation (R¬≤={r2_cv:.3f})')\n",
    "        \n",
    "        # Linha ideal 1:1\n",
    "        min_val = min(min(y_true), min(y_pred_cv))\n",
    "        max_val = max(max(y_true), max(y_pred_cv))\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], 'k--', label='Linha Ideal (1:1)')\n",
    "\n",
    "        plt.xlabel(\"Valores Reais\")\n",
    "        plt.ylabel(\"Valores Preditos (CV)\")\n",
    "        plt.title(f'Cross-Validation: {atributo} | {modelo} | {filtro}')\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.legend(loc='lower right')\n",
    "        \n",
    "        # Adiciona texto com m√©tricas\n",
    "        stats_text = f'CV - R¬≤: {r2_cv:.3f}, RMSE: {rmse_cv:.3f}, Slope: {slope_cv:.3f}, Offset: {offset_cv:.3f}'\n",
    "        plt.text(0.05, 0.95, stats_text, transform=plt.gca().transAxes, fontsize=10,\n",
    "                 verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5))\n",
    "\n",
    "        plt.savefig(file_path, format='png', dpi=200, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao gerar gr√°fico de CV para {modelo} com {filtro}: {e}\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a8d11c",
   "metadata": {},
   "source": [
    "### 6.1 Modelos e Grids de Par√¢metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8214c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos = {\n",
    "    'PLSR': {\n",
    "        'estimador': PLSRegression(),\n",
    "        'params': {'n_components': [5, 10, 15, 20]}\n",
    "    },\n",
    "    'PCR': {\n",
    "        'estimador': Pipeline([('pca', PCA()), ('regressor', LinearRegression())]),\n",
    "        'params': {'pca__n_components': [5, 10, 15, 20]}\n",
    "    },\n",
    "#     'RFR': {\n",
    "#        'estimador': RandomForestRegressor(random_state=42),\n",
    "#        'params': {\n",
    "#            'n_estimators': [100, 150, 200, 250, 300],\n",
    "#            'max_depth': [10, 20, 30, None],\n",
    "#            'min_samples_split': [2, 5, 10],\n",
    "#            'min_samples_leaf': [1, 2, 4],\n",
    "#            'warm_start': [True, False],\n",
    "#            'bootstrap': [True, False]\n",
    "#        }\n",
    "#     },\n",
    "    'SVMR': {\n",
    "        'estimador': SVR(),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
    "            'kernel': ['rbf', 'linear'],\n",
    "            'epsilon': [0.1, 0.2, 0.5, 1.0]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Combina todos os filtros\n",
    "todos_filtros = {**filtros_independentes, **filtros_dependentes}\n",
    "\n",
    "# Lista para armazenar todos os resultados\n",
    "lista_resultados_finais = []\n",
    "\n",
    "#AQUI COME√áA O CROSS VALIDATION -- PRINCIPAIS ALTERA√á√ïES DO C√ìDIGO DE ANDRESSA\n",
    "\n",
    "cv_folds = 10  # N√∫mero de folds para cross-validation\n",
    "kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "for atributo in [\"Firmness (N)\"]: #apenas para Firmness (N) e com RFR e SVMR grids menores\n",
    "    print(f'\\n{\"=\"*30}')\n",
    "    print(f'INICIANDO MODELAGEM PARA: {atributo}')\n",
    "    print(f'{\"=\"*30}')\n",
    "\n",
    "    melhor_r2_atributo = -999  # Inicia com um valor muito baixo\n",
    "    melhor_combinacao_info = {}  # Dicion√°rio para guardar as infos do melhor\n",
    "    \n",
    "    # Dicion√°rio para armazenar as 5 melhores combina√ß√µes por modelo\n",
    "    melhores_por_modelo = {nome: [] for nome in modelos.keys()}\n",
    "\n",
    "    dados = dados_final[atributo]\n",
    "    X_full, y_full = dados['X'], dados['y']\n",
    "\n",
    "    for nome_modelo, info_modelo in modelos.items():\n",
    "        print(f'\\n--- Modelo: {nome_modelo} ---')\n",
    "        \n",
    "        combinacoes_deste_modelo = [] # Lista tempor√°ria para este modelo\n",
    "\n",
    "        for nome_filtro, funcao_filtro in todos_filtros.items():\n",
    "            start_time = time.time()\n",
    "            print(f'  Testando filtro: {nome_filtro}...', end='')\n",
    "\n",
    "            if nome_filtro in filtros_independentes:\n",
    "                X_filtered = funcao_filtro(X_full)\n",
    "                tipo_filtro = 'independente'\n",
    "            else:  # Filtro dependente de Y\n",
    "                X_filtered = funcao_filtro(X_full, y_full)\n",
    "                tipo_filtro = 'dependente'\n",
    "            \n",
    "            grid = GridSearchCV(\n",
    "                info_modelo['estimador'], \n",
    "                info_modelo['params'], \n",
    "                cv=kf,  # Usa o mesmo KFold para consist√™ncia\n",
    "                scoring='r2', \n",
    "                n_jobs=-1\n",
    "            )\n",
    "            grid.fit(X_filtered, y_full)\n",
    "            \n",
    "            melhor_modelo = grid.best_estimator_\n",
    "            \n",
    "            cv_scores = cross_val_score(melhor_modelo, X_filtered, y_full, cv=kf, scoring='r2')\n",
    "            cv_r2_mean = cv_scores.mean()\n",
    "            cv_r2_std = cv_scores.std()\n",
    "            \n",
    "            # Predi√ß√µes de cross-validation para plotagem\n",
    "            y_pred_cv = cross_val_predict(melhor_modelo, X_filtered, y_full, cv=kf)\n",
    "            cv_rmse = np.sqrt(mean_squared_error(y_full, y_pred_cv))\n",
    "            \n",
    "            end_time = time.time()\n",
    "            print(f' CV R¬≤: {cv_r2_mean:.4f} (¬±{cv_r2_std:.4f}) | Conclu√≠do em {end_time - start_time:.2f}s')\n",
    "\n",
    "            # Salvar o resultado da combina√ß√£o atual\n",
    "            resultados_atuais = {\n",
    "                'Atributo': atributo,\n",
    "                'Modelo': nome_modelo,\n",
    "                'Filtro': nome_filtro,\n",
    "                'Tipo_Filtro': tipo_filtro,\n",
    "                'CV_R2_Mean': cv_r2_mean,\n",
    "                'CV_R2_Std': cv_r2_std,\n",
    "                'CV_RMSE': cv_rmse,\n",
    "                'Grid_Best_Score': grid.best_score_,\n",
    "                'Melhores_Params': str(grid.best_params_),\n",
    "                'Tempo_s': end_time - start_time,\n",
    "                'objeto_modelo': melhor_modelo \n",
    "            }\n",
    "            \n",
    "            lista_resultados_finais.append(resultados_atuais)\n",
    "            combinacoes_deste_modelo.append(resultados_atuais)\n",
    "\n",
    "            # L√≥gica para salvar o melhor modelo geral\n",
    "            if cv_r2_mean > melhor_r2_atributo:\n",
    "                melhor_r2_atributo = cv_r2_mean\n",
    "                melhor_combinacao_info = {\n",
    "                    'objeto_modelo': melhor_modelo,\n",
    "                    'nome_modelo': nome_modelo,\n",
    "                    'nome_filtro': nome_filtro,\n",
    "                    'parametros': grid.best_params_\n",
    "                }\n",
    "            \n",
    "            path_cv = f'graficos_modelos/cross_validation/sim-kk-field-{atributo}_{nome_modelo}_{nome_filtro}.png'\n",
    "            os.makedirs('graficos_modelos/cross_validation', exist_ok=True)\n",
    "            save_calibration_plot_cv(y_full, y_pred_cv, atributo, nome_filtro, nome_modelo, path_cv)\n",
    "\n",
    "        # Ordenar e salvar as 5 melhores para este modelo\n",
    "        combinacoes_deste_modelo.sort(key=lambda x: x['CV_R2_Mean'], reverse=True)\n",
    "        melhores_por_modelo[nome_modelo] = combinacoes_deste_modelo[:5]\n",
    "\n",
    "    if melhor_combinacao_info:  # Garante que encontrou pelo menos um modelo\n",
    "        melhor_obj = melhor_combinacao_info['objeto_modelo']\n",
    "        nome_arquivo_modelo = f\"modelos_salvos/sim-kk-field-melhor_modelo_{atributo.replace(' ', '_')}.joblib\"\n",
    "        \n",
    "        joblib.dump(melhor_obj, nome_arquivo_modelo)\n",
    "        \n",
    "        print(f\"\\n üèÜ MELHOR COMBINA√á√ÉO GERAL PARA '{atributo}' FOI SALVA!\")\n",
    "        print(f\"   - Arquivo: {nome_arquivo_modelo}\")\n",
    "        print(f\"   - Modelo: {melhor_combinacao_info['nome_modelo']}\")\n",
    "        print(f\"   - Filtro: {melhor_combinacao_info['nome_filtro']}\")\n",
    "        print(f\"   - Melhor R¬≤ CV: {melhor_r2_atributo:.4f}\")\n",
    "        print(f\"   - Melhores Par√¢metros: {melhor_combinacao_info['parametros']}\")\n",
    "    \n",
    "    # Salvar os 5 melhores modelos para cada tipo\n",
    "    print(f\"\\n--- Salvando os 5 melhores modelos para {atributo} ---\")\n",
    "    for nome_modelo, lista_combinacoes in melhores_por_modelo.items():\n",
    "        for i, combinacao in enumerate(lista_combinacoes):\n",
    "            nome_arquivo = f\"modelos_salvos_top5/sim-kk-field-top_{i+1}_{atributo.replace(' ', '_')}_{combinacao['Modelo']}_{combinacao['Filtro']}.joblib\"\n",
    "            os.makedirs('modelos_salvos_top5', exist_ok=True)\n",
    "            joblib.dump(combinacao['objeto_modelo'], nome_arquivo)\n",
    "            print(f\"  - Top {i+1} do modelo {nome_modelo} salvo em: {nome_arquivo}\")\n",
    "\n",
    "print('\\n\\n ‚úÖ Modelagem exaustiva com Cross-Validation!')\n",
    "\n",
    "df_completo = pd.DataFrame(lista_resultados_finais)\n",
    "df_completo_ordenado = df_completo.sort_values(by=['Atributo', 'CV_R2_Mean'], ascending=[True, False])\n",
    "df_completo_ordenado.to_excel('sim-kk-field-analise_completa_resultados_cv.xlsx', index=False)\n",
    "print('  ‚úÖResultados salvos em \"analise_completa_resultados_cv.xlsx\"')\n",
    "\n",
    "print(\"\\n--- Melhores Resultados por Atributo (Cross-Validation) ---\")\n",
    "melhores_resultados = df_completo_ordenado.groupby('Atributo').first().reset_index()\n",
    "\n",
    "# Arredondar colunas num√©ricas para melhor visualiza√ß√£o\n",
    "colunas_numericas_para_arredondar = ['CV_R2_Mean', 'CV_R2_Std', 'CV_RMSE', 'Grid_Best_Score']\n",
    "melhores_resultados[colunas_numericas_para_arredondar] = melhores_resultados[colunas_numericas_para_arredondar].round(4)\n",
    "\n",
    "# Selecionar e exibir as colunas mais importantes\n",
    "colunas_para_exibir = ['Atributo', 'Modelo', 'Filtro', 'CV_R2_Mean', 'CV_R2_Std', 'CV_RMSE', 'Melhores_Params']\n",
    "print(melhores_resultados[colunas_para_exibir].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1659ece7",
   "metadata": {},
   "source": [
    "## 7. An√°lise Comparativa dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7323c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Carregar os resultados salvos para an√°lise\n",
    "df_analise = pd.read_excel('sim-kk-field-analise_completa_resultados_cv.xlsx')\n",
    "\n",
    "idx = df_analise.groupby(['Atributo', 'Modelo'])['CV_R2_Mean'].transform(max) == df_analise['CV_R2_Mean']\n",
    "df_melhores_por_modelo = df_analise[idx]\n",
    "\n",
    "print(\"--- Tabela Comparativa dos Melhores Resultados por Modelo (Cross-Validation) ---\")\n",
    "print(df_melhores_por_modelo[['Atributo', 'Modelo', 'Filtro', 'CV_R2_Mean', 'CV_R2_Std', 'CV_RMSE']].round(4))\n",
    "\n",
    "print(\"\\n--- Melhores Modelos Gerais por Atributo (baseado no R¬≤ de Cross-Validation) ---\")\n",
    "idx_geral = df_melhores_por_modelo.groupby(['Atributo'])['CV_R2_Mean'].transform(max) == df_melhores_por_modelo['CV_R2_Mean']\n",
    "df_melhores_gerais = df_melhores_por_modelo[idx_geral]\n",
    "print(df_melhores_gerais[['Atributo', 'Modelo', 'Filtro', 'CV_R2_Mean', 'CV_R2_Std', 'CV_RMSE']].round(4))\n",
    "\n",
    "# An√°lise estat√≠stica adicional\n",
    "print(\"\\n--- An√°lise Estat√≠stica dos Resultados ---\")\n",
    "print(f\"N√∫mero total de combina√ß√µes testadas: {len(df_analise)}\")\n",
    "print(f\"N√∫mero de atributos analisados: {df_analise['Atributo'].nunique()}\")\n",
    "print(f\"N√∫mero de modelos testados: {df_analise['Modelo'].nunique()}\")\n",
    "print(f\"N√∫mero de filtros testados: {df_analise['Filtro'].nunique()}\")\n",
    "\n",
    "print(f\"\\nEstat√≠sticas do R¬≤ de Cross-Validation:\")\n",
    "print(f\"  M√©dia geral: {df_analise['CV_R2_Mean'].mean():.4f}\")\n",
    "print(f\"  Desvio padr√£o: {df_analise['CV_R2_Mean'].std():.4f}\")\n",
    "print(f\"  Melhor resultado: {df_analise['CV_R2_Mean'].max():.4f}\")\n",
    "print(f\"  Pior resultado: {df_analise['CV_R2_Mean'].min():.4f}\")\n",
    "\n",
    "# An√°lise por modelo\n",
    "print(f\"\\n--- Desempenho M√©dio por Modelo ---\")\n",
    "desempenho_por_modelo = df_analise.groupby('Modelo')['CV_R2_Mean'].agg(['mean', 'std', 'max', 'min']).round(4)\n",
    "print(desempenho_por_modelo)\n",
    "\n",
    "# An√°lise por filtro\n",
    "print(f\"\\n--- Top 10 Filtros com Melhor Desempenho M√©dio ---\")\n",
    "desempenho_por_filtro = df_analise.groupby('Filtro')['CV_R2_Mean'].agg(['mean', 'std', 'count']).round(4)\n",
    "desempenho_por_filtro = desempenho_por_filtro.sort_values('mean', ascending=False)\n",
    "print(desempenho_por_filtro.head(10))\n",
    "\n",
    "print(\"\\n‚úÖ An√°lise de resultados conclu√≠da!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f32e04",
   "metadata": {},
   "source": [
    "## Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e9025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linha de teste"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

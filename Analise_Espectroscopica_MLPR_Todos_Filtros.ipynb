{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Análise Espectroscópica Multilayer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este notebook segue as melhores práticas da literatura para análise espectroscópica robusta, incluindo:\n",
        "- Divisão dos dados por Kennard-Stone (70-30)\n",
        "- Remoção de outliers espectrais (PCA, T²/Q)\n",
        "- Remoção de outliers dos atributos (boxplot + seaborn)\n",
        "- Teste de todos os filtros (dependentes e independentes de y)\n",
        "- Modelagem MLPR com TODOS os filtros disponíveis (análise completa)\n",
        "- Plotagens robustas e salvamento de métricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports necessários\n",
        "import os\n",
        "import pywt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.signal import savgol_filter, detrend\n",
        "import warnings\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurações de plot\n",
        "plt.style.use('default')\n",
        "sns.set_palette('husl')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Carregamento dos Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A etapa inicial consiste em carregar os dados do arquivo Excel. A função `load_data` realiza a separação crucial entre:\n",
        "-   `metadata`: As variáveis de referência (alvo ou **y**), como pH, SST, Firmeza, etc.\n",
        "-   `wavelengths`: Os dados espectrais (preditoras ou **X**), que representam a absorbância/reflectância em cada comprimento de onda.\n",
        "\n",
        "Essa separação organiza os dados para as etapas subsequentes de pré-processamento e modelagem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Função para carregar dados\n",
        "def load_data(filepath):\n",
        "    \"\"\"Carrega dados espectrais e separa metadados de espectros.\"\"\"\n",
        "    df = pd.read_excel(filepath, engine='openpyxl')\n",
        "    \n",
        "    # Identificar colunas que são comprimentos de onda (numéricas)\n",
        "    numeric_cols = []\n",
        "    for col in df.columns:\n",
        "        try:\n",
        "            float(col)\n",
        "            numeric_cols.append(col)\n",
        "        except ValueError:\n",
        "            continue\n",
        "    \n",
        "    # Separar metadados e comprimentos de onda\n",
        "    metadata = df.drop(columns=numeric_cols)\n",
        "    wavelengths = df[numeric_cols]\n",
        "    \n",
        "    return metadata, wavelengths\n",
        "\n",
        "# Carregar dados\n",
        "filepath = 'Data/Original/dataset_cotton_fruit.xlsx'\n",
        "metadata, wavelengths = load_data(filepath)\n",
        "X = wavelengths.values\n",
        "wavelength_values = wavelengths.columns.astype(float)\n",
        "atributos = ['AT', 'FIRMEZA (N)', 'PH', 'SST', 'UBS (%)']\n",
        "\n",
        "print(f'Dados carregados: {X.shape[0]} amostras, {X.shape[1]} comprimentos de onda')\n",
        "print(f'Faixa espectral: {wavelength_values.min():.1f} - {wavelength_values.max():.1f} nm')\n",
        "print(f'Atributos disponíveis: {list(metadata.columns)}')\n",
        "print(f'Atributos a analisar: {atributos}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Divisão dos Dados com Kennard-Stone (70-30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para treinar e validar o modelo de forma confiável, dividimos os dados usando o algoritmo **Kennard-Stone (KS)**. Diferente de uma divisão aleatória, o KS seleciona amostras para o conjunto de calibração que cobrem a máxima variabilidade espectral, criando um modelo mais robusto e generalista.\n",
        "\n",
        "-   **Rastreamento de Amostras:** O código foi ajustado para manter um registro dos **índices originais** de cada amostra nos conjuntos de calibração e validação. Isso é fundamental para saber exatamente quais amostras são identificadas como outliers nas próximas etapas.\n",
        "-   **Processo por Atributo:** A divisão é refeita para cada atributo para garantir que amostras com valores ausentes (`NaN`) sejam tratadas corretamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def kennard_stone(X, train_size=0.7):\n",
        "    \"\"\"Implementa o algoritmo Kennard-Stone para divisão de dados.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_train = int(n_samples * train_size)\n",
        "    \n",
        "    if n_train < 2:\n",
        "        raise ValueError(\"O conjunto de treinamento precisa ter pelo menos duas amostras!\")\n",
        "    \n",
        "    distances = cdist(X, X, metric='euclidean')\n",
        "    \n",
        "    mean_sample = np.mean(X, axis=0)\n",
        "    first_sample = np.argmax(np.linalg.norm(X - mean_sample, axis=1))\n",
        "    selected = [first_sample]\n",
        "    \n",
        "    remaining = list(range(n_samples))\n",
        "    remaining.remove(first_sample)\n",
        "    \n",
        "    for _ in range(1, n_train):\n",
        "        dist_to_selected = distances[selected, :]\n",
        "        min_dist_to_selected = np.min(dist_to_selected[:, remaining], axis=0)\n",
        "        \n",
        "        next_sample_idx_in_remaining = np.argmax(min_dist_to_selected)\n",
        "        next_sample = remaining.pop(next_sample_idx_in_remaining)\n",
        "        \n",
        "        selected.append(next_sample)\n",
        "        \n",
        "    return np.array(selected), np.array(remaining)\n",
        "\n",
        "# Divisão para cada atributo\n",
        "dados_divididos = {}\n",
        "\n",
        "for atributo in atributos:\n",
        "    y = metadata[atributo].values\n",
        "    original_indices = np.arange(len(y)) # Guarda os índices originais\n",
        "    \n",
        "    mask = ~np.isnan(y)\n",
        "    X_clean = X[mask]\n",
        "    y_clean = y[mask]\n",
        "    original_indices_clean = original_indices[mask] # Filtra os índices também\n",
        "    \n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_clean)\n",
        "    \n",
        "    idx_cal_split, idx_val_split = kennard_stone(X_scaled, train_size=0.7)\n",
        "    \n",
        "    X_cal, X_val = X_clean[idx_cal_split], X_clean[idx_val_split]\n",
        "    y_cal, y_val = y_clean[idx_cal_split], y_clean[idx_val_split]\n",
        "    \n",
        "    # Guarda os índices originais de cada conjunto\n",
        "    indices_orig_cal = original_indices_clean[idx_cal_split]\n",
        "    indices_orig_val = original_indices_clean[idx_val_split]\n",
        "    \n",
        "    dados_divididos[atributo] = {\n",
        "        'X_cal': X_cal, 'X_val': X_val,\n",
        "        'y_cal': y_cal, 'y_val': y_val,\n",
        "        'indices_orig_cal': indices_orig_cal,\n",
        "        'indices_orig_val': indices_orig_val\n",
        "    }\n",
        "    \n",
        "    print(f'{atributo}: {X_cal.shape[0]} calibração, {X_val.shape[0]} validação')\n",
        "\n",
        "print(f'\\nDivisão Kennard-Stone concluída para {len(atributos)} atributos!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Remoção de Outliers Espectrais (PCA, T²/Q) nos Dados de Calibração"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Outliers espectrais são amostras com perfis anômalos que podem distorcer o modelo. Usamos a Análise de Componentes Principais (PCA) para detectá-los, baseando-nos em duas métricas:\n",
        "-   **Estatística T² (Hotelling's T²):** Mede a variação de uma amostra *dentro* do modelo PCA.\n",
        "-   **Estatística Q (Resíduos):** Mede a variação da amostra *fora* do modelo, ou seja, o que o PCA não conseguiu modelar.\n",
        "\n",
        "Uma amostra é marcada como outlier se exceder os limites estatísticos (média + 3 desvios padrão) de T² ou Q.\n",
        "\n",
        "**Ponto Chave:** Esta limpeza é feita **apenas no conjunto de calibração**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pca_outlier_removal(X, n_components=10, threshold=3):\n",
        "    \"\"\"Remove outliers usando PCA com estatísticas T² e Q.\"\"\"\n",
        "    # Padronizar dados\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    \n",
        "    # Aplicar PCA\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "    \n",
        "    # Calcular estatística T² (Hotelling's T²)\n",
        "    T2 = np.sum((X_pca / np.std(X_pca, axis=0)) ** 2, axis=1)\n",
        "    \n",
        "    # Calcular estatística Q (resíduos)\n",
        "    X_reconstructed = pca.inverse_transform(X_pca)\n",
        "    Q = np.sum((X_scaled - X_reconstructed) ** 2, axis=1)\n",
        "    \n",
        "    # Definir limites (média + threshold * desvio padrão)\n",
        "    T2_limit = np.mean(T2) + threshold * np.std(T2)\n",
        "    Q_limit = np.mean(Q) + threshold * np.std(Q)\n",
        "    \n",
        "    # Identificar outliers\n",
        "    outliers_mask = (T2 > T2_limit) | (Q > Q_limit)\n",
        "    \n",
        "    # Retornamos a máscara de quem NÃO é outlier e o modelo PCA treinado\n",
        "    return ~outliers_mask, pca, T2, Q, T2_limit, Q_limit\n",
        "\n",
        "# Remover outliers espectrais dos dados de calibração\n",
        "dados_pca = {}\n",
        "\n",
        "for atributo in atributos:\n",
        "    X_cal = dados_divididos[atributo]['X_cal']\n",
        "    y_cal = dados_divididos[atributo]['y_cal']\n",
        "    indices_orig_cal = dados_divididos[atributo]['indices_orig_cal']\n",
        "    \n",
        "    # Chama a função de remoção de outliers e armazena todos os resultados\n",
        "    keep_mask, pca_model, T2, Q, T2_limit, Q_limit = pca_outlier_removal(X_cal)\n",
        "    \n",
        "    # Identifica e informa os outliers removidos\n",
        "    outliers_indices = indices_orig_cal[~keep_mask]\n",
        "    print(f'{atributo}: {len(outliers_indices)} outliers espectrais removidos da calibração.')\n",
        "    if len(outliers_indices) > 0:\n",
        "        print(f'  Índices Originais Removidos: {outliers_indices}')\n",
        "    \n",
        "    # Filtra os dados de calibração\n",
        "    X_cal_clean = X_cal[keep_mask]\n",
        "    y_cal_clean = y_cal[keep_mask]\n",
        "    indices_orig_cal_clean = indices_orig_cal[keep_mask]\n",
        "    \n",
        "    dados_pca[atributo] = {\n",
        "        'X_cal': X_cal_clean,\n",
        "        'y_cal': y_cal_clean,\n",
        "        'indices_orig_cal': indices_orig_cal_clean,\n",
        "        'pca_model': pca_model,\n",
        "        'T2': T2,\n",
        "        'Q': Q,\n",
        "        'T2_limit': T2_limit,\n",
        "        'Q_limit': Q_limit,\n",
        "        'keep_mask': keep_mask\n",
        "    }\n",
        "    \n",
        "    print(f'  Calibração: {X_cal_clean.shape[0]} amostras')\n",
        "    print(f'  Validação: {dados_divididos[atributo][\"X_val\"].shape[0]} amostras')\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Verificação de Outliers Espectrais no Conjunto de Validação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Após construir o modelo de PCA com os dados de calibração limpos, podemos usá-lo para avaliar o conjunto de validação. O objetivo aqui **não é remover** amostras da validação, mas sim entender se ele contém amostras que seriam consideradas \"estranhas\" pelo modelo.\n",
        "- **Metodologia:**\n",
        "    1. Padronizamos os dados de validação usando o `scaler` treinado na calibração.\n",
        "    2. Projetamos esses dados no espaço PCA do modelo de calibração.\n",
        "    3. Calculamos as estatísticas T² e Q para as amostras de validação.\n",
        "    4. Comparamos esses valores com os **limites (T²_limit e Q_limit) definidos na calibração**.\n",
        "- **Interpretação:** Se amostras de validação excederem os limites, isso indica que o modelo pode ter dificuldade em predizê-las, pois elas representam uma variabilidade não vista (ou removida) no treino. Isso é uma informação valiosa sobre a robustez do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificação de Outliers na Validação\n",
        "print(\"--- Verificação de Outliers no Conjunto de Validação ---\")\n",
        "for atributo in atributos:\n",
        "    # Carrega o modelo PCA e os dados de validação\n",
        "    pca_model = dados_pca[atributo]['pca_model']\n",
        "    X_val = dados_divididos[atributo]['X_val']\n",
        "    indices_orig_val = dados_divididos[atributo]['indices_orig_val']\n",
        "    \n",
        "    # Recalcula os limites usando o modelo salvo, se necessário\n",
        "    # (Ou usa os limites já calculados e salvos anteriormente)\n",
        "    \n",
        "    # Padroniza e projeta os dados de validação\n",
        "    scaler_val = StandardScaler().fit(dados_divididos[atributo]['X_cal']) # Usa o scaler da calibração\n",
        "    X_val_scaled = scaler_val.transform(X_val)\n",
        "    X_val_pca = pca_model.transform(X_val_scaled)\n",
        "\n",
        "    # Calcula T² e Q para a validação\n",
        "    T2_val = np.sum((X_val_pca / np.std(X_val_pca, axis=0)) ** 2, axis=1)\n",
        "    X_val_reconstructed = pca_model.inverse_transform(X_val_pca)\n",
        "    Q_val = np.sum((X_val_scaled - X_val_reconstructed) ** 2, axis=1)\n",
        "\n",
        "    # Identifica outliers da validação usando os limites da CALIBRAÇÃO\n",
        "    outliers_val_mask = (T2_val > T2_limit) | (Q_val > Q_limit)\n",
        "    outliers_val_indices = indices_orig_val[outliers_val_mask]\n",
        "    \n",
        "    print(f'\\nAtributo: {atributo}')\n",
        "    if len(outliers_val_indices) > 0:\n",
        "        print(f'  {len(outliers_val_indices)} amostras de VALIDAÇÃO excedem os limites da calibração.')\n",
        "        print(f'  Índices Originais: {outliers_val_indices}')\n",
        "    else:\n",
        "        print('  Nenhuma amostra de validação excede os limites da calibração.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Gráficos de Outliers T² e Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for atributo in atributos:\n",
        "    d = dados_pca[atributo]\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Gráfico 1: Variância explicada cumulativa\n",
        "    # Acessa o modelo PCA com a chave correta 'pca_model'\n",
        "    cumulative_variance = np.cumsum(d['pca_model'].explained_variance_ratio_)\n",
        "    axes[0].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'o-', linewidth=2, markersize=6)\n",
        "    axes[0].set_xlabel('Número de Componentes Principais')\n",
        "    axes[0].set_ylabel('Variância Explicada Cumulativa')\n",
        "    axes[0].set_title(f'Variância Explicada - {atributo}')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[0].axhline(y=0.95, color='r', linestyle='--', alpha=0.7, label='95%')\n",
        "    axes[0].legend()\n",
        "    \n",
        "    # Gráfico 2: T² vs Q (detecção de outliers)\n",
        "    # Acessa as variáveis T2, Q, T2_limit, Q_limit e keep_mask com as chaves corretas\n",
        "    scatter = axes[1].scatter(d['T2'], d['Q'], c=d['keep_mask'], cmap='coolwarm', \n",
        "                              edgecolor='k', alpha=0.7, s=50)\n",
        "    axes[1].axhline(d['Q_limit'], color='r', linestyle='--', linewidth=2, label=f'Q Limit: {d[\"Q_limit\"]:.2f}')\n",
        "    axes[1].axvline(d['T2_limit'], color='g', linestyle='--', linewidth=2, label=f'T² Limit: {d[\"T2_limit\"]:.2f}')\n",
        "    axes[1].set_xlabel(\"Hotelling's T²\")\n",
        "    axes[1].set_ylabel('Q Residual')\n",
        "    axes[1].set_title(f'Detecção de Outliers - {atributo}')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Adicionar legenda de cores (esta parte também foi ajustada para a chave 'keep_mask')\n",
        "    legend1 = axes[1].legend(*scatter.legend_elements(), title=\"Status\")\n",
        "    axes[1].add_artist(legend1)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f'Gráficos de outliers para {atributo} exibidos.')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Remoção de Outliers dos Atributos (Boxplot + Seaborn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Além dos outliers espectrais (em X), podemos ter outliers nos valores de referência (em y). Por exemplo, um valor de pH ou SST que é analiticamente improvável ou resultado de um erro de anotação.\n",
        "- **Método (IQR):** O método do Intervalo Interquartil (IQR) é uma forma estatística robusta de identificar esses pontos.\n",
        "    - `IQR = Q3 (percentil 75) - Q1 (percentil 25)`\n",
        "    - Um valor é considerado outlier se estiver abaixo de `Q1 - 1.5 * IQR` ou acima de `Q3 + 1.5 * IQR`.\n",
        "- **Visualização:** O `boxplot` é a ferramenta visual perfeita para essa análise, pois ele desenha os \"bigodes\" exatamente nesses limites de 1.5 * IQR, mostrando os outliers como pontos individuais.\n",
        "- **Ação:** Novamente, a remoção é feita apenas no conjunto de calibração."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remover outliers dos atributos usando boxplot e IQR\n",
        "dados_final = {}\n",
        "\n",
        "for atributo in atributos:\n",
        "    # Pega os dados já limpos de outliers espectrais\n",
        "    d = dados_pca[atributo]\n",
        "    X_cal = d['X_cal']\n",
        "    y_cal = d['y_cal']\n",
        "    indices_orig_cal = d['indices_orig_cal']\n",
        "    \n",
        "    # Boxplot (seu código de plotagem original pode ser mantido aqui)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(x=y_cal)\n",
        "    sns.stripplot(x=y_cal, color='red', alpha=0.6)\n",
        "    plt.title(f'Boxplot com Dispersão - {atributo}')\n",
        "    plt.show()\n",
        "\n",
        "    # Lógica de remoção de outliers\n",
        "    Q1 = np.percentile(y_cal, 25)\n",
        "    Q3 = np.percentile(y_cal, 75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    \n",
        "    keep_mask_attr = (y_cal >= lower_bound) & (y_cal <= upper_bound)\n",
        "    \n",
        "    # Identifica e informa os outliers removidos\n",
        "    outliers_attr_indices = indices_orig_cal[~keep_mask_attr]\n",
        "    print(f'{atributo}: {len(outliers_attr_indices)} outliers de atributo removidos.')\n",
        "    if len(outliers_attr_indices) > 0:\n",
        "        print(f'  Índices Originais Removidos: {outliers_attr_indices}')\n",
        "    \n",
        "    # Armazena os dados finais e limpos para modelagem\n",
        "    dados_final[atributo] = {\n",
        "        'X_cal': X_cal[keep_mask_attr],\n",
        "        'y_cal': y_cal[keep_mask_attr],\n",
        "        'X_val': dados_divididos[atributo]['X_val'], # Validação original\n",
        "        'y_val': dados_divididos[atributo]['y_val']   # Validação original\n",
        "    }\n",
        "    \n",
        "    print(f'  Calibração final: {dados_final[atributo][\"X_cal\"].shape[0]} amostras')\n",
        "    print(f'  Validação: {dados_final[atributo][\"X_val\"].shape[0]} amostras')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Filtros de Pré-processamento (Independentes e Dependentes de y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Os dados espectrais brutos contêm diversas fontes de variação indesejada que podem mascarar a informação química relevante. Essas variações podem ser causadas por espalhamento de luz (efeitos físicos da amostra), mudanças na linha de base (drift do instrumento) e ruído. O pré-processing visa minimizar esses efeitos para que o modelo foque na correlação entre o espectro e o atributo de interesse.\n",
        "\n",
        "Neste notebook, testamos uma ampla gama de filtros, divididos em duas categorias:\n",
        "\n",
        "### Filtros Independentes de y\n",
        "\n",
        "São aplicados apenas aos espectros (`X`) e não utilizam a informação da variável alvo (`y`).\n",
        "\n",
        "-   **`Raw`**: Utiliza os dados espectrais brutos, sem nenhum tratamento. Serve como uma linha de base para comparar a eficácia dos outros filtros.\n",
        "-   **`MSC` (Multiplicative Scatter Correction)**: Corrige o espalhamento de luz (aditivo e multiplicativo) causado por variações no tamanho de partícula e compactação da amostra. Ele ajusta cada espectro para se parecer mais com um espectro \"ideal\" (geralmente a média de todos os espectros).\n",
        "-   **`SNV` (Standard Normal Variate)**: Alternativa ao MSC que também corrige o espalhamento de luz. A diferença é que o SNV padroniza cada espectro individualmente (subtrai a média e divide pelo desvio padrão daquele espectro), sem usar um espectro de referência.\n",
        "-   **`SG_D1` e `SG_D2` (Savitzky-Golay Derivatives)**: Calcula a primeira ou a segunda derivada do espectro. Derivadas são excelentes para remover desvios de linha de base (efeitos aditivos) e para resolver picos espectrais sobrepostos, realçando a informação de bandas de absorção específicas.\n",
        "-   **`Detrend`**: Remove tendências lineares ou polinomiais da linha de base do espectro. É muito eficaz para corrigir \"inclinações\" no espectro causadas por drift do instrumento.\n",
        "-   **`Normalize`**: Realiza uma normalização Min-Max, escalonando a intensidade de cada espectro para um intervalo fixo (geralmente [0, 1]). Ajuda a corrigir variações de intensidade causadas por diferenças na distância da amostra ou na potência da fonte de luz.\n",
        "-   **`EMSC` (Extended Multiplicative Signal Correction)**: Uma versão avançada do MSC. Além de corrigir os efeitos de espalhamento, o EMSC pode incluir termos polinomiais para modelar e remover efeitos de linha de base mais complexos e não-lineares.\n",
        "-   **`Continuum Removal`**: Técnica que normaliza os espectros para que as bandas de absorção possam ser comparadas em termos de sua profundidade, e não de sua intensidade absoluta. Ele ajusta um \"envelope\" (casco convexo) sobre o espectro e divide o espectro original por este envelope, realçando as características de absorção.\n",
        "-   **`Wavelet_Denoising`**: Utiliza a Transformada Wavelet para decompor o espectro em diferentes níveis de frequência. A técnica permite remover o ruído (geralmente presente em altas frequências) de forma muito eficaz, preservando as principais características do sinal espectral.\n",
        "\n",
        "### Filtros Dependentes de y e Combinações\n",
        "\n",
        "Estes filtros utilizam a variável alvo (`y`) para otimizar a remoção de variação não correlacionada em `X`, ou são combinações sequenciais de múltiplos filtros para um tratamento mais completo.\n",
        "\n",
        "-   **`OSC_1` e `OSC_2` (Orthogonal Signal Correction)**: Filtro que remove componentes (1 ou 2, neste caso) dos espectros `X` que são ortogonais (não correlacionados) à variável alvo `y`. O objetivo é limpar `X` da variação que não ajuda na predição, potencialmente melhorando o modelo subsequente. Esta técnica é frequentemente referida como um pré-processamento **OPLS** (Orthogonal Projections to Latent Structures).\n",
        "-   **`MSC_SG_OSC`**: Uma **cadeia de pré-processamentos** aplicada na seguinte ordem:\n",
        "    1.  `MSC` para corrigir o espalhamento.\n",
        "    2.  `Savitzky-Golay (1ª derivada)` para corrigir a linha de base.\n",
        "    3.  `OSC (1 componente)` para remover variação não correlacionada com `y`.\n",
        "-   **`OPLS1_SNV_SG_D1` e `OPLS2_SNV_SG_D1`**: Outra cadeia de processamento:\n",
        "    1.  `SNV` para correção de espalhamento.\n",
        "    2.  `Savitzky-Golay (1ª derivada)`.\n",
        "    3.  `OPLS/OSC` para remover 1 ou 2 componentes ortogonais a `y`.\n",
        "-   **`SNV_Detrend_SG_D1`**: Uma combinação de filtros independentes de `y`, mas que, por sua complexidade, é testada junto às outras cadeias:\n",
        "    1.  `SNV`.\n",
        "    2.  `Detrend` para remoção de tendência.\n",
        "    3.  `Savitzky-Golay (1ª derivada)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementação dos filtros de pré-processamento\n",
        "# Filtros independentes de y\n",
        "def msc(X):\n",
        "    \"\"\"Multiplicative Scatter Correction.\"\"\"\n",
        "    X = np.asarray(X)\n",
        "    mean_spectrum = np.mean(X, axis=0)\n",
        "    corrected_spectra = np.zeros_like(X)\n",
        "    for i in range(X.shape[0]):\n",
        "        slope, intercept = np.polyfit(mean_spectrum, X[i, :], 1)\n",
        "        corrected_spectra[i, :] = (X[i, :] - intercept) / slope\n",
        "    return corrected_spectra\n",
        "\n",
        "def snv(X):\n",
        "    \"\"\"Standard Normal Variate.\"\"\"\n",
        "    X = np.asarray(X)\n",
        "    return (X - np.mean(X, axis=1, keepdims=True)) / np.std(X, axis=1, keepdims=True)\n",
        "\n",
        "def savitzky_golay(X, window_size=11, poly_order=2, deriv_order=1):\n",
        "    \"\"\"Savitzky-Golay filter.\"\"\"\n",
        "    return savgol_filter(X, window_length=window_size, polyorder=poly_order, deriv=deriv_order, axis=1)\n",
        "\n",
        "def detrend_filter(X):\n",
        "    \"\"\"Detrending filter.\"\"\"\n",
        "    return detrend(X, axis=1)\n",
        "\n",
        "def normalize(X):\n",
        "    \"\"\"Normalização Min-Max.\"\"\"\n",
        "    return (X - np.min(X, axis=1, keepdims=True)) / (np.max(X, axis=1, keepdims=True) - np.min(X, axis=1, keepdims=True))\n",
        "\n",
        "def emsc(X, reference=None):\n",
        "    \"\"\"Extended Multiplicative Signal Correction.\"\"\"\n",
        "    X = np.asarray(X)\n",
        "    if reference is None:\n",
        "        reference = np.mean(X, axis=0)  # Usa o espectro médio como referência\n",
        "    \n",
        "    X_corr = np.zeros_like(X)\n",
        "    for i in range(X.shape[0]):\n",
        "        # Modelo: X[i] ≈ a + b*reference\n",
        "        model = np.vstack([np.ones_like(reference), reference]).T\n",
        "        params, _, _, _ = np.linalg.lstsq(model, X[i, :], rcond=None)\n",
        "        a, b = params[0], params[1]\n",
        "        X_corr[i,:] = (X[i, :] - a) / b\n",
        "    return X_corr\n",
        "\n",
        "def continuum_removal(X, wavelengths):\n",
        "    \"\"\"Continuum Removal.\"\"\"\n",
        "    X = np.asarray(X)\n",
        "    X_cr = np.zeros_like(X)\n",
        "    for i in range(X.shape[0]):\n",
        "        spectrum = X[i, :]\n",
        "        # Encontra os pontos do casco convexo superior\n",
        "        q_u = [0]\n",
        "        for k in range(1, len(wavelengths) - 1):\n",
        "            s_k = (spectrum[len(wavelengths)-1] - spectrum[0]) / (wavelengths[-1] - wavelengths[0])\n",
        "            s_q = (spectrum[k] - spectrum[q_u[-1]]) / (wavelengths[k] - wavelengths[q_u[-1]])\n",
        "            if s_q > s_k:\n",
        "                q_u.append(k)\n",
        "        q_u.append(len(wavelengths)-1)\n",
        "        \n",
        "        # Interpolação linear entre os pontos do casco\n",
        "        continuum = np.interp(wavelengths, wavelengths[q_u], spectrum[q_u])\n",
        "        X_cr[i, :] = spectrum / continuum\n",
        "    return X_cr\n",
        "\n",
        "def wavelet_denoising(X, wavelet='db4', level=4):\n",
        "    \"\"\"Wavelet Transform para Denoising.\"\"\"\n",
        "    X = np.asarray(X)\n",
        "    original_length = X.shape[1]\n",
        "    denoised_list = []\n",
        "\n",
        "    for i in range(X.shape[0]):\n",
        "        # 1. Decomposição Wavelet\n",
        "        coeffs = pywt.wavedec(X[i, :], wavelet, level=level)\n",
        "\n",
        "        # 2. Cálculo do limiar (threshold)\n",
        "        sigma = np.median(np.abs(coeffs[-1])) / 0.6745\n",
        "        threshold = sigma * np.sqrt(2 * np.log(original_length))\n",
        "\n",
        "        # 3. Aplicação do filtro (soft thresholding) nos coeficientes de detalhe\n",
        "        coeffs[1:] = [pywt.threshold(c, value=threshold, mode='soft') for c in coeffs[1:]]\n",
        "\n",
        "        # 4. Reconstrução do sinal\n",
        "        reconstructed_signal = pywt.waverec(coeffs, wavelet)\n",
        "\n",
        "        # 5. Ajuste do tamanho\n",
        "        denoised_list.append(reconstructed_signal[:original_length])\n",
        "\n",
        "    return np.asarray(denoised_list)\n",
        "\n",
        "# Filtros dependentes de y (Orthogonal Signal Correction)\n",
        "class OrthogonalCorrection:\n",
        "    \"\"\"Orthogonal Signal Correction (OSC).\"\"\"\n",
        "    def __init__(self, n_components=1):\n",
        "        self.n_components = n_components\n",
        "    \n",
        "    def fit_transform(self, X, y):\n",
        "        X, y = np.asarray(X), np.asarray(y).ravel()\n",
        "        self.w_ortho_ = []\n",
        "        self.p_ortho_ = []\n",
        "        self.X_corr_ = X.copy()\n",
        "        \n",
        "        for _ in range(self.n_components):\n",
        "            pls = PLSRegression(n_components=1)\n",
        "            pls.fit(self.X_corr_, y)\n",
        "            t = pls.x_scores_\n",
        "            w = pls.x_weights_\n",
        "            p = pls.x_loadings_\n",
        "            \n",
        "            # Componente Ortogonal\n",
        "            w_ortho = p - (np.dot(w.T, p) / np.dot(w.T, w)) * w\n",
        "            t_ortho = np.dot(self.X_corr_, w_ortho)\n",
        "            p_ortho = np.dot(t_ortho.T, self.X_corr_) / np.dot(t_ortho.T, t_ortho)\n",
        "            \n",
        "            # Remover variação ortogonal\n",
        "            self.X_corr_ -= np.dot(t_ortho, p_ortho)\n",
        "            self.w_ortho_.append(w_ortho)\n",
        "            self.p_ortho_.append(p_ortho)\n",
        "        \n",
        "        return self.X_corr_\n",
        "    \n",
        "    def transform(self, X):\n",
        "        X_res = np.asarray(X).copy()\n",
        "        for i in range(self.n_components):\n",
        "            t_ortho = np.dot(X_res, self.w_ortho_[i])\n",
        "            X_res -= np.dot(t_ortho, self.p_ortho_[i])\n",
        "        return X_res\n",
        "\n",
        "# Dicionário de filtros independentes de y\n",
        "filtros_independentes = {\n",
        "    'Raw': lambda X: X,\n",
        "    'MSC': msc,\n",
        "    'SNV': snv,\n",
        "    'SG_D1': lambda X: savitzky_golay(X, window_size=11, poly_order=2, deriv_order=1),\n",
        "    'SG_D2': lambda X: savitzky_golay(X, window_size=11, poly_order=2, deriv_order=2),\n",
        "    'Detrend': detrend_filter,\n",
        "    'Normalize': normalize,\n",
        "    'EMSC': emsc,\n",
        "    'Continuum_Removal': lambda X: continuum_removal(X, wavelength_values),\n",
        "    'Wavelet_Denoising': wavelet_denoising\n",
        "}\n",
        "\n",
        "# Dicionário de filtros dependentes de y\n",
        "filtros_dependentes = {\n",
        "    'OSC_1': lambda X, y: OrthogonalCorrection(n_components=1).fit_transform(X, y),\n",
        "    'OSC_2': lambda X, y: OrthogonalCorrection(n_components=2).fit_transform(X, y),\n",
        "    'MSC_SG_OSC': lambda X, y: OrthogonalCorrection(n_components=1).fit_transform(\n",
        "        savitzky_golay(msc(X), window_size=11, poly_order=2, deriv_order=1), y),\n",
        "    'OPLS1_SNV_SG_D1': lambda X, y: OrthogonalCorrection(n_components=1).fit_transform(\n",
        "        savitzky_golay(snv(X), window_size=11, poly_order=2, deriv_order=1), y),\n",
        "    'OPLS2_SNV_SG_D1': lambda X, y: OrthogonalCorrection(n_components=2).fit_transform(\n",
        "        savitzky_golay(snv(X), window_size=11, poly_order=2, deriv_order=1), y),\n",
        "    'SNV_Detrend_SG_D1': lambda X, y: savitzky_golay(detrend_filter(snv(X)), window_size=11, poly_order=2, deriv_order=1)\n",
        "}\n",
        "\n",
        "print(f'Filtros independentes de y: {list(filtros_independentes.keys())}')\n",
        "print(f'Filtros dependentes de y: {list(filtros_dependentes.keys())}')\n",
        "print(f'Total de filtros: {len(filtros_independentes) + len(filtros_dependentes)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Avaliação dos Filtros e Seleção dos Melhores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esta seção automatiza o processo de treinamento, otimização e avaliação do modelo MLP Regressor (Rede Neural). O fluxo é projetado para testar rigorosamente todas as combinações de pré-processamento.\n",
        "\n",
        "-   **Fluxo de Trabalho Automatizado:**\n",
        "    1.  **Iteração por Atributo e Filtro:** O código itera sobre cada atributo e, para cada um, aplica todos os 16 filtros de pré-processamento definidos anteriormente.\n",
        "    2.  **Padronização dos Dados:** É **essencial** escalar os dados (`StandardScaler`) antes de alimentar uma rede neural. Isso garante que o modelo convirja de forma mais rápida e estável.\n",
        "    3.  **Otimização de Hiperparâmetros (`GridSearchCV`):** Para cada filtro, uma busca em grade otimiza os principais hiperparâmetros do MLPR, como a arquitetura das camadas ocultas (`hidden_layer_sizes`), a função de ativação e o método de otimização (`solver`).\n",
        "    4.  **Avaliação e Salvamento:** O melhor modelo de cada combinação é avaliado no conjunto de validação. As métricas de desempenho, os melhores parâmetros e os gráficos de predição vs. real são salvos automaticamente para análise posterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Função para o Gráfico de CALIBRAÇÃO (Ref vs Predito vs CV) ---\n",
        "def save_calibration_plot(y_cal, y_pred_cal, y_pred_cv, atributo, filtro, modelo, file_path):\n",
        "    \"\"\"\n",
        "    Gera e salva um gráfico comparando predições de treino e de validação cruzada\n",
        "    no conjunto de calibração.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Garante que o diretório exista\n",
        "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "        # Métricas para Predição no Treino Completo\n",
        "        slope_pred, offset_pred = np.polyfit(y_cal, y_pred_cal, 1)\n",
        "        rmse_pred = np.sqrt(mean_squared_error(y_cal, y_pred_cal))\n",
        "        r2_pred = r2_score(y_cal, y_pred_cal)\n",
        "\n",
        "        # Métricas para Predição da Validação Cruzada\n",
        "        slope_cv, offset_cv = np.polyfit(y_cal, y_pred_cv, 1)\n",
        "        rmse_cv = np.sqrt(mean_squared_error(y_cal, y_pred_cv))\n",
        "        r2_cv = r2_score(y_cal, y_pred_cv)\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.scatter(y_cal, y_pred_cal, color='blue', marker='o', alpha=0.6, label=f'Predição no Treino (R²={r2_pred:.3f})')\n",
        "        plt.scatter(y_cal, y_pred_cv, color='red', marker='x', alpha=0.7, label=f'Predição CV (R²={r2_cv:.3f})')\n",
        "        \n",
        "        # Linha ideal 1:1\n",
        "        min_val = min(min(y_cal), min(y_pred_cal), min(y_pred_cv))\n",
        "        max_val = max(max(y_cal), max(y_pred_cal), max(y_pred_cv))\n",
        "        plt.plot([min_val, max_val], [min_val, max_val], 'k--', label='Linha Ideal (1:1)')\n",
        "\n",
        "        plt.xlabel(\"Valores Reais (Calibração)\")\n",
        "        plt.ylabel(\"Valores Preditos\")\n",
        "        plt.title(f'Desempenho na Calibração: {atributo} | {modelo} | {filtro}')\n",
        "        plt.grid(True, linestyle='--', alpha=0.6)\n",
        "        plt.legend(loc='lower right')\n",
        "        \n",
        "        # Adiciona texto com métricas\n",
        "        stats_text = (\n",
        "            f'Treino - RMSE: {rmse_pred:.3f}, Slope: {slope_pred:.3f}, Offset: {offset_pred:.3f}\\n'\n",
        "            f'CV     - RMSE: {rmse_cv:.3f}, Slope: {slope_cv:.3f}, Offset: {offset_cv:.3f}'\n",
        "        )\n",
        "        plt.text(0.05, 0.95, stats_text, transform=plt.gca().transAxes, fontsize=10,\n",
        "                 verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5))\n",
        "\n",
        "        plt.savefig(file_path, format='png', dpi=200, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao gerar gráfico de calibração para {modelo} com {filtro}: {e}\")\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "# --- Função para o Gráfico de VALIDAÇÃO (Predito vs Real) ---\n",
        "def save_validation_plot(y_val, y_pred_val, atributo, filtro, modelo, file_path):\n",
        "    \"\"\"\n",
        "    Gera e salva o gráfico de predições no conjunto de validação (teste).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Garante que o diretório exista\n",
        "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "        \n",
        "        # Métricas\n",
        "        r2 = r2_score(y_val, y_pred_val)\n",
        "        rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
        "        slope, offset = np.polyfit(y_val, y_pred_val, 1)\n",
        "\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.scatter(y_val, y_pred_val, alpha=0.7, edgecolors='k', label='Dados de Validação')\n",
        "        \n",
        "        # Linha ideal 1:1\n",
        "        min_val = min(min(y_val), min(y_pred_val))\n",
        "        max_val = max(max(y_val), max(y_pred_val))\n",
        "        plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Linha Ideal (1:1)')\n",
        "\n",
        "        plt.xlabel('Valores Reais (Validação)')\n",
        "        plt.ylabel('Valores Preditos')\n",
        "        plt.title(f'Desempenho na Validação: {atributo} | {modelo} | {filtro}')\n",
        "        plt.grid(True, linestyle='--', alpha=0.6)\n",
        "        \n",
        "        stats_text = f'R² = {r2:.4f}\\nRMSE = {rmse:.4f}\\nSlope = {slope:.4f}'\n",
        "        plt.text(0.05, 0.95, stats_text, transform=plt.gca().transAxes, fontsize=12,\n",
        "                 verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', fc='lightblue', alpha=0.5))\n",
        "        \n",
        "        plt.legend()\n",
        "        plt.savefig(file_path, format='png', dpi=200, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao gerar gráfico de validação para {modelo} com {filtro}: {e}\")\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Definir o modelo MLPR e seu grid de parâmetros\n",
        "# (Grid menor para um teste mais rápido, pode ser expandido)\n",
        "param_grid_mlpr = {\n",
        "    'hidden_layer_sizes': [\n",
        "        (120, 10,1),\n",
        "        (5,), (8,), (12,), (30,),\n",
        "        (10,), (15,), (20,), (25,), (32,),\n",
        "        (20, 10), (32, 16), (64, 32), (50, 20),\n",
        "        (20, 15, 10),\n",
        "        (16, 8), (20, 20),\n",
        "        \n",
        "        ],\n",
        "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
        "    'activation': ['relu', 'tanh', 'logistic'],\n",
        "    'solver': ['adam'],\n",
        "    'early_stopping': [True],\n",
        "    'max_iter': [1000]\n",
        "}\n",
        "\n",
        "# Combina todos os filtros\n",
        "todos_filtros = {**filtros_independentes, **filtros_dependentes}\n",
        "\n",
        "# Lista para armazenar todos os resultados\n",
        "lista_resultados_finais_mlpr = []\n",
        "os.makedirs('graficos_mlpr', exist_ok=True) # Cria pasta para salvar gráficos\n",
        "\n",
        "# 2. Loop principal de modelagem\n",
        "for atributo in atributos:\n",
        "    print(f'\\n{\"=\"*30}')\n",
        "    print(f'INICIANDO MODELAGEM MLPR PARA: {atributo}')\n",
        "    print(f'{\"=\"*30}')\n",
        "    \n",
        "    dados = dados_final[atributo]\n",
        "    X_cal, y_cal = dados['X_cal'], dados['y_cal']\n",
        "    X_val, y_val = dados['X_val'], dados['y_val']\n",
        "\n",
        "    # É crucial escalar os dados para o MLPR\n",
        "    scaler = StandardScaler()\n",
        "    X_cal_scaled = scaler.fit_transform(X_cal)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "    for nome_filtro, funcao_filtro in todos_filtros.items():\n",
        "        start_time = time.time()\n",
        "        print(f'  Testando filtro: {nome_filtro}...', end='')\n",
        "\n",
        "        # Aplicar filtro aos dados já escalados\n",
        "        if nome_filtro in filtros_independentes:\n",
        "            X_cal_f = funcao_filtro(X_cal_scaled)\n",
        "            X_val_f = funcao_filtro(X_val_scaled)\n",
        "        else: # Filtro dependente de Y\n",
        "            osc_model = OrthogonalCorrection(n_components=1)\n",
        "            X_cal_f = osc_model.fit_transform(X_cal_scaled, y_cal)\n",
        "            X_val_f = osc_model.transform(X_val_scaled)\n",
        "        \n",
        "        # GridSearchCV\n",
        "        mlpr = MLPRegressor(random_state=42)\n",
        "        grid = GridSearchCV(mlpr, param_grid_mlpr, cv=5, scoring='r2', n_jobs=-1, verbose=0)\n",
        "        grid.fit(X_cal_f, y_cal)\n",
        "        \n",
        "        melhor_modelo = grid.best_estimator_\n",
        "        \n",
        "        # Avaliação final no conjunto de validação\n",
        "        y_pred = melhor_modelo.predict(X_val_f)\n",
        "        r2_val = r2_score(y_val, y_pred)\n",
        "        rmse_val = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "        \n",
        "        end_time = time.time()\n",
        "        print(f' R² Val: {r2_val:.4f} | Concluído em {end_time - start_time:.2f}s')\n",
        "        \n",
        "        # Salvar resultados\n",
        "        lista_resultados_finais_mlpr.append({\n",
        "            'Atributo': atributo,\n",
        "            'Modelo': 'MLPR',\n",
        "            'Filtro': nome_filtro,\n",
        "            'CV_R2': grid.best_score_,\n",
        "            'Val_R2': r2_val,\n",
        "            'Val_RMSE': rmse_val,\n",
        "            'Melhores_Params': str(grid.best_params_),\n",
        "            'Tempo_s': end_time - start_time\n",
        "        })\n",
        "        \n",
        "        # --- Lógica de plotagem nova e aprimorada para MLPR ---\n",
        "\n",
        "        # 1. Gera as predições necessárias para os gráficos\n",
        "        y_pred_cal = melhor_modelo.predict(X_cal_f)\n",
        "        # Para o CV do MLPR, usamos um modelo base para agilizar\n",
        "        base_mlpr = MLPRegressor(random_state=42, max_iter=1000) \n",
        "        y_pred_cv = cross_val_predict(base_mlpr, X_cal_f, y_cal, cv=5)\n",
        "\n",
        "        # 2. Chama a função para o gráfico de CALIBRAÇÃO\n",
        "        path_cal = f'graficos_mlpr/calibracao/{atributo}_MLPR_{nome_filtro}.png'\n",
        "        save_calibration_plot(y_cal, y_pred_cal, y_pred_cv, atributo, nome_filtro, 'MLPR', path_cal)\n",
        "\n",
        "        # 3. Chama a função para o gráfico de VALIDAÇÃO\n",
        "        #    (y_pred já foi calculado antes como a predição em y_val)\n",
        "        path_val = f'graficos_mlpr/validacao/{atributo}_MLPR_{nome_filtro}.png'\n",
        "        save_validation_plot(y_val, y_pred, atributo, nome_filtro, 'MLPR', path_val)\n",
        "\n",
        "print('\\n\\n✅ Modelagem MLPR exaustiva concluída!')\n",
        "\n",
        "# 3. Criar DataFrame e exportar para Excel\n",
        "df_mlpr = pd.DataFrame(lista_resultados_finais_mlpr)\n",
        "df_mlpr_ordenado = df_mlpr.sort_values(by=['Atributo', 'Val_R2'], ascending=[True, False])\n",
        "df_mlpr_ordenado.to_excel('resultados_modelagem_mlpr.xlsx', index=False)\n",
        "\n",
        "print('✅ Resultados salvos em \"resultados_modelagem_mlpr.xlsx\"')\n",
        "\n",
        "# 4. Exibir melhores resultados\n",
        "print(\"\\n--- Melhores Resultados por Atributo (MLPR) ---\")\n",
        "melhores_mlpr = df_mlpr_ordenado.groupby('Atributo').first().reset_index()\n",
        "colunas_numericas_para_arredondar = ['Val_R2', 'Val_RMSE']\n",
        "melhores_mlpr[colunas_numericas_para_arredondar] = melhores_mlpr[colunas_numericas_para_arredondar].round(4)\n",
        "\n",
        "colunas_para_exibir = ['Atributo', 'Filtro', 'Val_R2', 'Val_RMSE', 'Melhores_Params']\n",
        "print(melhores_mlpr[colunas_para_exibir])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Exploração Aprofundada dos Melhores Filtros com Ajuste de Parâmetros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nesta seção, realizamos uma nova rodada de modelagem utilizando apenas os melhores filtros identificados na análise anterior. Para filtros que aceitam parâmetros ajustáveis (como Savitzky-Golay, Wavelet Denoising e OSC), testamos múltiplas combinações desses parâmetros a fim de maximizar a performance do modelo. Usamos novamente uma abordagem com GridSearch para o MLPRegressor e reportamos os melhores parâmetros de cada combinação. Além disso, geramos gráficos de calibração e validação para facilitar a interpretação visual dos resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*50)\n",
        "print(\"🔬 INICIANDO OTIMIZAÇÃO FINA DOS MELHORES FILTROS 🔬\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 1. Selecionar os melhores filtros identificados na análise anterior\n",
        "melhores_filtros_por_atributo = {\n",
        "    'AT': ['Detrend', 'Normalize', 'Raw', 'Wavelet_Denoising', 'OSC_1', 'MSC', 'EMSC', 'OSC_2'],\n",
        "    'FIRMEZA (N)': ['EMSC', 'MSC', 'Wavelet_Denoising', 'Raw', 'SNV', 'Detrend', 'Normalize', 'OSC_1'],\n",
        "    'PH': ['OSC_1', 'Detrend', 'Raw', 'Wavelet_Denoising', 'OSC_2', 'SG_D1', 'SNV', 'SG_D1'],\n",
        "    'SST': ['SNV_Detrend_SG_D1', 'OPLS2_SNV_SG_D1', 'Detrend', 'SNV', 'OSC_1', 'OPLS1_SNV_SG_D1'],\n",
        "    'UBS (%)': ['OPLS2_SNV_SG_D1', 'SNV_Detrend_SG_D1', 'OPLS1_SNV_SG_D1', 'MSC_SG_OSC', 'SNV']\n",
        "}\n",
        "\n",
        "# 2. Definir os grids de parâmetros para os filtros que aceitam otimização\n",
        "sg_params_to_test = {\n",
        "    'window_size': [5, 11, 15, 21, 27, 35, 41],\n",
        "        'poly_order': [2, 3, 4],\n",
        "        'deriv_order': [1, 2]\n",
        "}\n",
        "\n",
        "osc_params_to_test = {\n",
        "    'n_components': [1, 2, 3]\n",
        "}\n",
        "\n",
        "wavelet_params_to_test = {\n",
        "    'wavelet': ['db1', 'db2', 'db3', 'db4', 'db5'],\n",
        "    'level': [1,2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "# 3. Definir o grid do modelo MLPR (o mesmo de antes)\n",
        "param_grid_mlpr = {\n",
        "    'hidden_layer_sizes': [\n",
        "        (120, 10,1),\n",
        "        (5,), (8,), (12,), (30,),\n",
        "        (10,), (15,), (20,), (25,), (32,),\n",
        "        (20, 10), (32, 16), (64, 32), (50, 20),\n",
        "        (20, 15, 10),\n",
        "        (16, 8), (20, 20),\n",
        "        \n",
        "        ],\n",
        "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
        "    'activation': ['relu', 'tanh', 'logistic'],\n",
        "    'solver': ['adam'],\n",
        "    'early_stopping': [True],\n",
        "    'max_iter': [1000]\n",
        "}\n",
        "\n",
        "# Lista para armazenar os novos resultados\n",
        "lista_resultados_otimizados_mlpr = []\n",
        "\n",
        "# 4. Loop principal de modelagem e otimização\n",
        "for atributo, lista_melhores_filtros in melhores_filtros_por_atributo.items():\n",
        "    print(f'\\n{\"=\"*30}')\n",
        "    print(f'OTIMIZANDO FILTROS PARA: {atributo}')\n",
        "    print(f'{\"=\"*30}')\n",
        "    \n",
        "    # Carregar dados (assumindo que 'dados_final' está disponível)\n",
        "    dados = dados_final[atributo]\n",
        "    X_cal, y_cal = dados['X_cal'], dados['y_cal']\n",
        "    X_val, y_val = dados['X_val'], dados['y_val']\n",
        "\n",
        "    # Escalar os dados brutos uma única vez\n",
        "    scaler = StandardScaler()\n",
        "    X_cal_scaled = scaler.fit_transform(X_cal)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "    for nome_filtro in lista_melhores_filtros:\n",
        "        print(f'\\n--- Otimizando filtro: {nome_filtro} ---')\n",
        "        start_time_total = time.time()\n",
        "        \n",
        "        # Variáveis para guardar o melhor resultado para ESTE filtro\n",
        "        melhor_resultado_do_filtro = {\n",
        "            'Val_R2': -np.inf, \n",
        "            'Val_RMSE': np.inf,\n",
        "            'Melhores_Params_Filtro': 'N/A',\n",
        "            'Melhores_Params_Modelo': None,\n",
        "            'Tempo_s': 0\n",
        "        }\n",
        "        \n",
        "        # Lógica para aplicar os grids de parâmetros dos filtros\n",
        "        \n",
        "        # CASO 1: Filtros que usam Savitzky-Golay (SG) e OSC/OPLS\n",
        "        if ('SG' in nome_filtro) and ('OSC' in nome_filtro or 'OPLS' in nome_filtro):\n",
        "            print(f'  (Grid Search em SG e OSC/OPLS ativado)')\n",
        "            sg_grid = ParameterGrid(sg_params_to_test)\n",
        "            \n",
        "            for n_comp in osc_params_to_test['n_components']:\n",
        "                for sg_p in sg_grid:\n",
        "                    # Evitar combinação inválida\n",
        "                    if sg_p['poly_order'] >= sg_p['window_size']:\n",
        "                        continue\n",
        "                        \n",
        "                    params_filtro_str = f\"OSC_n_comp={n_comp}, SG_params={sg_p}\"\n",
        "                    print(f'    Testando: {params_filtro_str}...', end='')\n",
        "                    start_time_iter = time.time()\n",
        "                    \n",
        "                    try:\n",
        "                        # Aplicar pré-filtros (SNV, etc.)\n",
        "                        X_cal_pre, X_val_pre = snv(X_cal_scaled), snv(X_val_scaled)\n",
        "                        # Aplicar SG\n",
        "                        X_cal_sg = savitzky_golay(X_cal_pre, **sg_p)\n",
        "                        X_val_sg = savitzky_golay(X_val_pre, **sg_p)\n",
        "                        # Aplicar OSC/OPLS\n",
        "                        osc_model = OrthogonalCorrection(n_components=n_comp)\n",
        "                        X_cal_f = osc_model.fit_transform(X_cal_sg, y_cal)\n",
        "                        X_val_f = osc_model.transform(X_val_sg)\n",
        "                    except Exception as e:\n",
        "                        print(f\" Erro ao aplicar filtro: {e}\")\n",
        "                        continue\n",
        "\n",
        "                    # Roda o GridSearchCV para o MLPR\n",
        "                    mlpr = MLPRegressor(random_state=42)\n",
        "                    grid = GridSearchCV(mlpr, param_grid_mlpr, cv=5, scoring='r2', n_jobs=-1)\n",
        "                    grid.fit(X_cal_f, y_cal)\n",
        "                    \n",
        "                    # Avaliação\n",
        "                    y_pred = grid.best_estimator_.predict(X_val_f)\n",
        "                    r2_val = r2_score(y_val, y_pred)\n",
        "                    \n",
        "                    print(f' R² Val: {r2_val:.4f}')\n",
        "                    \n",
        "                    # Atualiza se o resultado for melhor\n",
        "                    if r2_val > melhor_resultado_do_filtro['Val_R2']:\n",
        "                        melhor_resultado_do_filtro['Val_R2'] = r2_val\n",
        "                        melhor_resultado_do_filtro['Val_RMSE'] = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "                        melhor_resultado_do_filtro['Melhores_Params_Filtro'] = params_filtro_str\n",
        "                        melhor_resultado_do_filtro['Melhores_Params_Modelo'] = str(grid.best_params_)\n",
        "            \n",
        "        # CASO 2: Filtros que usam SÓ Savitzky-Golay (SG)\n",
        "        elif 'SG' in nome_filtro:\n",
        "            print(f'  (Grid Search em SG ativado)')\n",
        "            sg_grid = ParameterGrid(sg_params_to_test)\n",
        "            for sg_p in sg_grid:\n",
        "                 if sg_p['poly_order'] >= sg_p['window_size']:\n",
        "                        continue\n",
        "                \n",
        "                 params_filtro_str = f\"SG_params={sg_p}\"\n",
        "                 print(f'    Testando: {params_filtro_str}...', end='')\n",
        "\n",
        "                 try:\n",
        "                    # Aplicar pré-filtros\n",
        "                    X_cal_pre, X_val_pre = snv(X_cal_scaled), snv(X_val_scaled)\n",
        "                    X_cal_pre, X_val_pre = detrend_filter(X_cal_pre), detrend_filter(X_val_pre)\n",
        "                    # Aplicar SG\n",
        "                    X_cal_f = savitzky_golay(X_cal_pre, **sg_p)\n",
        "                    X_val_f = savitzky_golay(X_val_pre, **sg_p)\n",
        "                 except Exception as e:\n",
        "                    print(f\" Erro ao aplicar filtro: {e}\")\n",
        "                    continue\n",
        "\n",
        "                 mlpr = MLPRegressor(random_state=42)\n",
        "                 grid = GridSearchCV(mlpr, param_grid_mlpr, cv=5, scoring='r2', n_jobs=-1)\n",
        "                 grid.fit(X_cal_f, y_cal)\n",
        "                 y_pred = grid.best_estimator_.predict(X_val_f)\n",
        "                 r2_val = r2_score(y_val, y_pred)\n",
        "                 print(f' R² Val: {r2_val:.4f}')\n",
        "                 if r2_val > melhor_resultado_do_filtro['Val_R2']:\n",
        "                    melhor_resultado_do_filtro['Val_R2'] = r2_val\n",
        "                    melhor_resultado_do_filtro['Val_RMSE'] = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "                    melhor_resultado_do_filtro['Melhores_Params_Filtro'] = params_filtro_str\n",
        "                    melhor_resultado_do_filtro['Melhores_Params_Modelo'] = str(grid.best_params_)\n",
        "\n",
        "        # CASO 3: Filtros que usam SÓ OSC/OPLS (e não SG)\n",
        "        elif 'OSC' in nome_filtro:\n",
        "            print(f'  (Grid Search em OSC ativado)')\n",
        "            for n_comp in osc_params_to_test['n_components']:\n",
        "                params_filtro_str = f\"OSC_n_comp={n_comp}\"\n",
        "                print(f'    Testando: {params_filtro_str}...', end='')\n",
        "                \n",
        "                osc_model = OrthogonalCorrection(n_components=n_comp)\n",
        "                X_cal_f = osc_model.fit_transform(X_cal_scaled, y_cal)\n",
        "                X_val_f = osc_model.transform(X_val_scaled)\n",
        "                \n",
        "                mlpr = MLPRegressor(random_state=42)\n",
        "                grid = GridSearchCV(mlpr, param_grid_mlpr, cv=5, scoring='r2', n_jobs=-1)\n",
        "                grid.fit(X_cal_f, y_cal)\n",
        "                y_pred = grid.best_estimator_.predict(X_val_f)\n",
        "                r2_val = r2_score(y_val, y_pred)\n",
        "                print(f' R² Val: {r2_val:.4f}')\n",
        "                if r2_val > melhor_resultado_do_filtro['Val_R2']:\n",
        "                    melhor_resultado_do_filtro['Val_R2'] = r2_val\n",
        "                    melhor_resultado_do_filtro['Val_RMSE'] = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "                    melhor_resultado_do_filtro['Melhores_Params_Filtro'] = params_filtro_str\n",
        "                    melhor_resultado_do_filtro['Melhores_Params_Modelo'] = str(grid.best_params_)\n",
        "\n",
        "        # CASO 4: Filtros não parametrizáveis (rodam apenas uma vez)\n",
        "        else:\n",
        "            print(f'  (Filtro não parametrizável, rodando com configuração padrão)')\n",
        "            # Aplica o filtro (aqui usamos um dicionário para chamar a função certa)\n",
        "            filtros_nao_parametrizaveis = {\n",
        "                'Detrend': detrend_filter,\n",
        "                'Normalize': normalize,\n",
        "                'EMSC': emsc,\n",
        "                'MSC': msc,\n",
        "                'SNV': snv # Adicionado para completude\n",
        "            }\n",
        "            funcao_filtro = filtros_nao_parametrizaveis.get(nome_filtro, lambda x: x)\n",
        "            X_cal_f = funcao_filtro(X_cal_scaled)\n",
        "            X_val_f = funcao_filtro(X_val_scaled)\n",
        "            \n",
        "            mlpr = MLPRegressor(random_state=42)\n",
        "            grid = GridSearchCV(mlpr, param_grid_mlpr, cv=5, scoring='r2', n_jobs=-1)\n",
        "            grid.fit(X_cal_f, y_cal)\n",
        "            y_pred = grid.best_estimator_.predict(X_val_f)\n",
        "            \n",
        "            melhor_resultado_do_filtro['Val_R2'] = r2_score(y_val, y_pred)\n",
        "            melhor_resultado_do_filtro['Val_RMSE'] = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "            melhor_resultado_do_filtro['Melhores_Params_Modelo'] = str(grid.best_params_)\n",
        "\n",
        "        # Armazenar o melhor resultado encontrado para o filtro atual\n",
        "        end_time_total = time.time()\n",
        "        lista_resultados_otimizados_mlpr.append({\n",
        "            'Atributo': atributo,\n",
        "            'Modelo': 'MLPR',\n",
        "            'Filtro': nome_filtro,\n",
        "            'Val_R2': melhor_resultado_do_filtro['Val_R2'],\n",
        "            'Val_RMSE': melhor_resultado_do_filtro['Val_RMSE'],\n",
        "            'Melhores_Params_Filtro': melhor_resultado_do_filtro['Melhores_Params_Filtro'],\n",
        "            'Melhores_Params_Modelo': melhor_resultado_do_filtro['Melhores_Params_Modelo'],\n",
        "            'Tempo_s': end_time_total - start_time_total\n",
        "        })\n",
        "        \n",
        "        print(f'--- Melhor resultado para {nome_filtro}: R² = {melhor_resultado_do_filtro[\"Val_R2\"]:.4f} ---')\n",
        "        print(f'    Parâmetros do Filtro: {melhor_resultado_do_filtro[\"Melhores_Params_Filtro\"]}')\n",
        "        print(f'    Parâmetros do Modelo: {melhor_resultado_do_filtro[\"Melhores_Params_Modelo\"]}')\n",
        "        \n",
        "print('\\n\\n✅ Otimização Fina concluída!')\n",
        "\n",
        "# 5. Criar DataFrame e exportar os resultados otimizados\n",
        "df_mlpr_otimizado = pd.DataFrame(lista_resultados_otimizados_mlpr)\n",
        "df_mlpr_otimizado_ordenado = df_mlpr_otimizado.sort_values(by=['Atributo', 'Val_R2'], ascending=[True, False])\n",
        "df_mlpr_otimizado_ordenado.to_excel('resultados_otimizacao_filtros_mlpr.xlsx', index=False)\n",
        "\n",
        "print('✅ Resultados da otimização salvos em \"resultados_otimizacao_filtros_mlpr.xlsx\"')\n",
        "\n",
        "# 6. Exibir melhores resultados da otimização\n",
        "print(\"\\n--- Melhores Resultados por Atributo (Após Otimização de Filtros) ---\")\n",
        "melhores_otimizados_mlpr = df_mlpr_otimizado_ordenado.groupby('Atributo').first().reset_index()\n",
        "print(melhores_otimizados_mlpr[['Atributo', 'Filtro', 'Val_R2', 'Val_RMSE', 'Melhores_Params_Filtro']].round(4))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
